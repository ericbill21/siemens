{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siemens_Approach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbill21/siemens/blob/master/Siemens_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70udFHYSHNI"
      },
      "source": [
        "#@title Imports\n",
        "%tensorflow_version 2.x\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "#time prediciton\n",
        "prev_time = 0\n",
        "\n",
        "#Constants\n",
        "colors = {0 : 'green', 1 : 'red', 'green' : 0, 'red' : 1}\n",
        "sources = {'A' : 'https://drive.google.com/file/d/1hAzAKZNpmSclSI7HnV_cRjpMS4Kh5r1q/view?usp=sharing', 'B' : 'https://drive.google.com/file/d/12VlecL-5iYs-BFpnTOba1x65jWofBX1P/view?usp=sharing', 'C' : 'https://drive.google.com/file/d/1-Z0RuJIi1cZcqrrmV6TqT0O1PwI2OiBY/view?usp=sharing'}\n",
        "source_size = {'A': 1000,'B' : 5000, 'C' : 50000}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E98cwmoUT7w1"
      },
      "source": [
        "#Configuration\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "tf.random.set_seed(31415)\n",
        "\n",
        "dataSetName = 'B'\n",
        "\n",
        "#Examlpe sets\n",
        "subsetA = [47, 847, 993, 55, 102, 572, 430, 115, 842, 72, 770, 107, 78, 834, 593, 43, 234, 709, 210, 378]\n",
        "subsetB = [606, 2663, 1809, 2145, 4539, 3333, 3562, 2262, 512, 2046, 1541, 909, 286, 4815, 3663, 1742, 2822, 2756, 2937, 3080, 3845, 3949, 2506, 3984, 2803, 2067]\n",
        "subsetC = [32088, 33534, 39634, 40177, 25142, 752, 41771, 11793, 16415, 3811, 2096, 35902, 42221, 19594, 25109, 40476, 25162, 41150, 34610, 28329, 46339, 43149, 44441, 25720, 38747, 49497, 12708, 23920, 2280, 17946]\n",
        "\n",
        "sampleSet_indices = subsetB\n",
        "\n",
        "weight = 0.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISkimS6oUTP9"
      },
      "source": [
        "#@title Functions\n",
        "\n",
        "def getDataSet(dataSetName):\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+sources[dataSetName].split('/')[-2]\n",
        "  return pd.read_excel(path)\n",
        "\n",
        "\n",
        "#Plots the given dataSet in the rigth colors\n",
        "def makePlot(t_data = None, marker = [], marker_false = []):\n",
        "\n",
        "  if t_data == None:\n",
        "    t_data = getDataSet(dataSetName) \n",
        "  \n",
        "  data_x1 = np.array(t_data.pop('x_i1'))\n",
        "  data_x2 = np.array(t_data.pop('x_i2'))\n",
        "  data_l = np.array(t_data.pop('l_i'))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  \n",
        "  for i in range(2):\n",
        "    x_1 = []\n",
        "    x_2 = []\n",
        "    for j in range(np.size(data_x1)):\n",
        "      if data_l[j] == i:\n",
        "        x_1.append(data_x1[j])\n",
        "        x_2.append(data_x2[j])\n",
        "    ax.scatter(x_1, x_2, c = colors[i])\n",
        "\n",
        "  for i in marker:\n",
        "    ax.scatter(i[0],i[1],marker = \"x\", c = 'black')\n",
        "\n",
        "  for i in marker_false:\n",
        "    ax.scatter(i[0],i[1],marker = \"*\", c = 'black')\n",
        "\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.title(f'DataSet {dataSetName}')\n",
        "  plt.axis('scaled')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Prepares a subset of the dataSet for validation\n",
        "def popPredictionPoints(dataSet, sampleSet_indices):\n",
        "  sampleSet_x1_x2 = []\n",
        "  sampleSet_li = []\n",
        "\n",
        "  for i in sampleSet_indices:\n",
        "    sampleSet_x1_x2.append([dataSet['x_i1'].loc[i], dataSet['x_i2'].loc[i]])\n",
        "    sampleSet_li.append([dataSet['l_i'].loc[i]])\n",
        "  \n",
        "  # Saving the testing points\n",
        "  prediction_point = tf.constant(sampleSet_x1_x2, tf.float32)\n",
        "  prediciton_label = tf.constant(sampleSet_li, tf.float32)\n",
        "\n",
        "  #Removing the testing point\n",
        "  dataSet.drop(index=sampleSet_indices, inplace=True)\n",
        "  dataSet.reset_index(inplace=True)\n",
        "\n",
        "  return (prediction_point, prediciton_label)\n",
        "\n",
        "\n",
        "\n",
        "# Print iterations progress\n",
        "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% ETA: {round(timeCalc()*(total-iteration), 2)} minutes {suffix}')\n",
        "    sys.stdout.flush()\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "      global prev_time\n",
        "      prev_time = 0\n",
        "      sys.stdout.write('\\r')\n",
        "      sys.stdout.flush()\n",
        "\n",
        "\n",
        "def makeCertaintiyMap(model, accuracy = 100, specific_color = None):\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    tensor = tf.constant([[j/accuracy, i/accuracy] for j in range(accuracy)], tf.float32)\n",
        "    result = model.predict(tensor)\n",
        "\n",
        "    if specific_color != None:\n",
        "      accuracy_map[i] = result[:, specific_color]\n",
        "    else:\n",
        "      result = result.max(axis=1) #Getting each max value\n",
        "      if max(result) != min(result):\n",
        "        normalized = (result-min(result))/(max(result)-min(result))\n",
        "        accuracy_map[i] = normalized\n",
        "      else: \n",
        "        accuracy_map[i] = result\n",
        "      \n",
        "    \n",
        "    printProgressBar(i, accuracy-1)\n",
        "\n",
        "\n",
        "  plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1)\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "  return accuracy_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plotAllWeightCombinations(model, accuracy = 100):\n",
        "  print('Chosen Subset:')\n",
        "  makePlot(marker=prediction_points.numpy())\n",
        "\n",
        "  result_x = np.zeros(accuracy+1)\n",
        "  result_y = np.zeros(accuracy+1)\n",
        "\n",
        "  printProgressBar(0, accuracy,prefix='Progress', suffix='ETA: ')\n",
        "\n",
        "  # Iterating over every weigth\n",
        "  for i in range(accuracy+1):\n",
        "    keras.backend.clear_session()\n",
        "    prevTime = time.time()\n",
        "\n",
        "    weight = i/accuracy #Selecting a value between 0 and 1 with steps of 1/accuracy\n",
        "\n",
        "    model.compile(optimizer='adam',loss=construct_custom_weigthing_loss(weight) ,metrics=['accuracy'])\n",
        "    model.fit(training_points, training_labels, batch_size=32, epochs=10, shuffle=True, verbose=0)\n",
        "    \n",
        "    #Saving results\n",
        "    result_x[i] = weight\n",
        "    result_y[i] = model.evaluate(prediction_points, prediction_labels, verbose=0)[0]\n",
        "\n",
        "    #Printing progress\n",
        "    printProgressBar(i+1, accuracy+1,prefix='Progress')\n",
        "\n",
        "  #Presenting the results\n",
        "  print('\\nThe loss:')\n",
        "  plt.plot(result_x, result_y)\n",
        "\n",
        "  return result_y\n",
        "\n",
        "\n",
        "def timeCalc():\n",
        "  global prev_time\n",
        "  if prev_time == 0:\n",
        "    prev_time = time.time()\n",
        "    return 0\n",
        "  \n",
        "  res = (time.time() - prev_time) / 60\n",
        "  prev_time = time.time()\n",
        "  return res\n",
        "\n",
        "\n",
        "def plotHistory(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def showPredicitons(model, prediction_points, prediction_labels):\n",
        "  prediction = model.predict(prediction_points)\n",
        "\n",
        "  points = prediction_points.numpy()\n",
        "  labels = prediction_labels.numpy()[:, 0].astype(int)\n",
        "\n",
        "  correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "  wrong_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  number_of_points = np.bincount(np.argmax(prediction, axis=1))\n",
        "\n",
        "  print(f'Predictions for green: {number_of_points[0]} / {len(labels)}')\n",
        "  print(f'Predictions for red: {number_of_points[1]} / {len(labels)}')\n",
        "  print(f'The algorithm predicted {np.bincount(labels == np.argmax(prediction, axis=1))[0]} times wrong')\n",
        "  makePlot(marker=points[correct_indices], marker_false=points[wrong_indices])\n",
        "  print(f' \\'*\\' stands for wrong predicted')\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsy_7qMzuBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66099f05-43f7-4d48-9a80-8310afb43fb6"
      },
      "source": [
        "#Preparing data\n",
        "dataSet = getDataSet(dataSetName)\n",
        "dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "prediction_points, prediction_labels = popPredictionPoints(dataSet=dataSet, sampleSet_indices=sampleSet_indices)\n",
        "\n",
        "#Artificially balancing the dataSet\n",
        "number_of_green_points = list(dataSet['l_i']).count(colors['green'])\n",
        "number_of_red_points = list(dataSet['l_i']).count(colors['red'])\n",
        "\n",
        "if number_of_red_points / source_size[dataSetName] <= 0.5:\n",
        "  amount = int((0.5 - number_of_red_points / source_size[dataSetName]) * source_size[dataSetName])\n",
        "  red_points = dataSet.loc[dataSet['l_i'] == 1] #Getting all red points\n",
        "  choosen_points = red_points.sample(amount, replace=True) #Selecting a random subset of red points\n",
        "  dataSet = dataSet.append(choosen_points, ignore_index=True) #appending the subset\n",
        "\n",
        "if number_of_green_points / source_size[dataSetName] <= 0.5:\n",
        "  amount = int((0.5 - number_of_green_points / source_size[dataSetName]) * source_size[dataSetName])\n",
        "  green_points = dataSet.loc[dataSet['l_i'] == 0] #Getting all green points\n",
        "  choosen_points = green_points.sample(amount, replace=True) #Selecting a random subset of green points\n",
        "  dataSet = dataSet.append(choosen_points, ignore_index=True) #appending green subset\n",
        "\n",
        "if 'index' in dataSet.columns:\n",
        "  dataSet.pop('index') #removing old indices\n",
        "  print(f'Artificially exended by {dataSet[\"x_i1\"].size - source_size[dataSetName] + len(sampleSet_indices)} points')\n",
        "  print(f'Relation is now: {dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]} green  : {dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]} red ')\n",
        "\n",
        "#Creating tensors\n",
        "training_labels = tf.constant(dataSet.pop('l_i'), tf.float32)\n",
        "training_points = tf.constant(dataSet, tf.float32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificially exended by 140 points\n",
            "Relation is now: 2500 green  : 2614 red \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N24D-gWpnFmD"
      },
      "source": [
        "def construct_custom_weigthing_loss(weight):\n",
        "\n",
        "  def custom_weigthing_loss(y_true,y_pred):\n",
        "    length = tf.shape(y_true)[0]\n",
        "\n",
        "    #Creating a vector with all values set to the weight: [0.3, 0.3, ... 0.3]\n",
        "    error = tf.multiply(tf.constant(weight, tf.float32), tf.ones(length)) \n",
        "\n",
        "    #Setting every entry to 0 if the corresponding entry in y_true is 1\n",
        "    error = tf.where(tf.equal(y_true[:, 0], tf.zeros(length)), error, tf.zeros(length))\n",
        "\n",
        "    #Setting every entry to 0 if the algorithm predicted 0\n",
        "    error = tf.where(tf.greater(y_pred[:, 0], y_pred[:, 1]), tf.zeros(length), error)\n",
        "\n",
        "    #Transforms the vector from [0, 0, 0.3, ... 0,3] to [[0, -0], [0, -0], [0.3, -0.3], ... [0.3, -0.3]]\n",
        "    error = tf.stack([error, tf.multiply(tf.constant(-1, tf.float32), error)], 1)\n",
        "\n",
        "    #Adding the artificial loss\n",
        "    y_pred = y_pred + error\n",
        "\n",
        "    #Eliminating values > 1 or < 0\n",
        "    y_pred0 = tf.where(tf.greater(y_pred[:, 0], tf.ones(length)), tf.ones(length), y_pred[:, 0])\n",
        "    y_pred1 = tf.where(tf.greater(y_pred[:, 1], tf.zeros(length)), y_pred[:, 1], tf.zeros(length))\n",
        "    y_pred = tf.stack([y_pred0, y_pred1], axis=1)\n",
        "\n",
        "\n",
        "    loss = keras.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y_true)\n",
        "    return loss\n",
        "  \n",
        "  return custom_weigthing_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrxlkLjeccKz"
      },
      "source": [
        "model = keras.Sequential([\n",
        "           keras.layers.Flatten(input_shape=(2,)),      #input layer: 2 neurons\n",
        "           keras.layers.Dense(100,activation='relu'), \n",
        "           keras.layers.Dense(70,activation='relu'), \n",
        "           keras.layers.Dense(50,activation='relu'),       \n",
        "           keras.layers.Dense(10,activation='relu'),\n",
        "           keras.layers.Dense(2,activation='softmax')   #output layer: 2 neurons              \n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss=construct_custom_weigthing_loss(weight) ,metrics=['accuracy'])\n",
        "\n",
        "validation_set = tf.concat([prediction_points, prediction_labels], axis=1)\n",
        "\n",
        "history = model.fit(training_points, training_labels, batch_size=32, epochs=5, shuffle=True, )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}