{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siemens_Approach.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbill21/siemens/blob/master/Siemens_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppIOSk-I8wV2"
      },
      "source": [
        "# Imports, Config & GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeg6hj5kBjI"
      },
      "source": [
        "# Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Arithmetic Operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Data visualization\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Progress calculation\n",
        "import sys\n",
        "import time\n",
        "from datetime import date\n",
        "\n",
        "# Time prediciton\n",
        "PREV_TIME = 0\n",
        "PB_START_TIME = 0\n",
        "\n",
        "# Mounting Google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd5eTUqAkBjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a73393-05e4-456a-9499-ce1e0dc0f150"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# ram_gb = virtual_memory().total / 1e9\n",
        "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "# if ram_gb < 20:\n",
        "#   print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "#   print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "#   print('re-execute this cell.')\n",
        "# else:\n",
        "#   print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 26 10:59:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjZxd7_8MKI"
      },
      "source": [
        "# Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRjXSfIYkBjM"
      },
      "source": [
        "# Dictionaries\n",
        "COLORS = {0 : 'green', 1 : 'red', 'green' : 0, 'red' : 1}\n",
        "SOURCES = {'A' : 'https://drive.google.com/file/d/1hAzAKZNpmSclSI7HnV_cRjpMS4Kh5r1q/view?usp=sharing', 'B' : 'https://drive.google.com/file/d/12VlecL-5iYs-BFpnTOba1x65jWofBX1P/view?usp=sharing', 'C' : 'https://drive.google.com/file/d/1-Z0RuJIi1cZcqrrmV6TqT0O1PwI2OiBY/view?usp=sharing'}\n",
        "SOURCE_SIZE = {'A': 1000,'B' : 5000, 'C' : 50000}\n",
        "\n",
        "CURRENT_SET = 'B'\n",
        "\n",
        "# Balancing dataset to threshold\n",
        "THRESHOLD_DATA = 0.4\n",
        "\n",
        "# Threshold for balanced validation set\n",
        "THRESHOLD_VAL = 0.4\n",
        "\n",
        "# Minimum certainty required to predict green\n",
        "MIN_GREEN_CERT = 0.9\n",
        "\n",
        "# Random number seed\n",
        "random.seed(time.time())\n",
        "\n",
        "subsetA = random.sample(range(1000), 150)\n",
        "subsetB = random.sample(range(5000), 800)\n",
        "subsetC = random.sample(range(50000), 8000)\n",
        "\n",
        "VAL_INDICES = locals()['subset' + CURRENT_SET]\n",
        "\n",
        "# Penalty applied to false green classifications in custom loss function\n",
        "PENALTY = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVhVxxhc5T-a"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC174oUT5utW",
        "cellView": "form"
      },
      "source": [
        "#@title Data Operations\n",
        "\n",
        "def getDataSet(dataset=CURRENT_SET):\n",
        "  \"\"\"Returns pandas.DataFrame of dataset.\n",
        "  \n",
        "  Args:\n",
        "    dataset: char, optional\n",
        "      The dataset to return. 'A', 'B', or 'C'.\n",
        "  \"\"\"\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+SOURCES[dataset].split('/')[-2]\n",
        "  return pd.read_excel(path)\n",
        "\n",
        "def separateValidationSet(dataSet, validationIndices):\n",
        "  \"\"\"Separates a subset of points from dataSet as validation points.\n",
        "\n",
        "  Validation points are extracted and deleted from dataSet to be used for\n",
        "  validation later on.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset which the\n",
        "      validation points are extracted from.\n",
        "    validationIndices: 1-D list of ints\n",
        "      The elements corresponding to these indices are extracted from dataSet.\n",
        "\n",
        "  Returns:\n",
        "    2-tuple of the form (valSet_points, valSet_labels), where valSet_points\n",
        "    is a np.array of shape (x,2) and valSet_labels is a np.array of shape (x,1).\n",
        "  \"\"\"\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be \\\n",
        "      {pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if len(np.array(validationIndices).shape) != 1:\n",
        "    raise TypeError(f'The shape of the parameter validationIndices is: \\\n",
        "      {np.array(validationIndices).shape}, but it should be 1 dimensional')\n",
        "  \n",
        "  valSet_points = dataSet[['x_i1','x_i2']].loc[validationIndices]\n",
        "  valSet_labels = dataSet['l_i'].loc[validationIndices]\n",
        "  \n",
        "  # Saving the validation points\n",
        "  valSet_points = np.array(valSet_points)\n",
        "  valSet_labels = np.array(valSet_labels).astype('float')\n",
        "\n",
        "  # Removing the validation point\n",
        "  dataSet.drop(index=validationIndices, inplace=True)\n",
        "  dataSet.reset_index(inplace=True)\n",
        "\n",
        "  return (valSet_points, valSet_labels)\n",
        "\n",
        "def timeCalc():\n",
        "  \"\"\"Calculates time between previous call and current call.\n",
        "\n",
        "  Returns:\n",
        "    Time difference in minutes as float.\n",
        "  \"\"\"\n",
        "  global PREV_TIME\n",
        "  if PREV_TIME == 0:\n",
        "    PREV_TIME = time.time()\n",
        "    return 0\n",
        "  \n",
        "  res = (time.time() - PREV_TIME) / 60\n",
        "  PREV_TIME = time.time()\n",
        "  return res\n",
        "\n",
        "def balanceDataset(dataSet, threshold, verbose=1):\n",
        "  \"\"\"Artificially balances dataSet by duplicating red or green points.\n",
        "\n",
        "  Args: \n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. The datset to be balanced.\n",
        "    threshold: float between 0 and 0.5\n",
        "      The function duplicates red or green points until the fraction of points\n",
        "      of the less frequent color is at least equal to the threshold.\n",
        "\n",
        "  Returns:\n",
        "    pandas.DataFrame with columns 'x_i1', 'x_i2', 'l_i1'.\n",
        "  \"\"\"\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  amount = 0\n",
        "\n",
        "  if number_of_red_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_red_points) // (1 - threshold) )\n",
        "    red_points = dataSet.loc[dataSet['l_i'] == 1] #Getting all red points\n",
        "    chosen_points = red_points.sample(amount, replace=True) #Selecting a random subset of red points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending the subset\n",
        "\n",
        "  if number_of_green_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_green_points) // (1 - threshold) )\n",
        "    green_points = dataSet.loc[dataSet['l_i'] == 0] #Getting all green points\n",
        "    chosen_points = green_points.sample(amount, replace=True) #Selecting a random subset of green points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending green subset\n",
        "\n",
        "  dataSet = dataSet[['x_i1','x_i2','l_i']]\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  if verbose > 0:\n",
        "    print(f'Artificially exended by {amount} points')\n",
        "    print(f'Relation is now: {round(number_of_green_points / total_number_of_points, 2)}',\n",
        "            f'green : {round(number_of_red_points / total_number_of_points, 2)} red ')\n",
        "  \n",
        "  return dataSet\n",
        "\n",
        "def getBalancedValSetIndices(dataSet, size, threshold):\n",
        "  \"\"\"Get indices of validation points such that neither color represents\n",
        "    less than (threshold*100)% of the validation set.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset from\n",
        "      which the validation points are to be chosen.\n",
        "    size: int\n",
        "      Size of the validation set.\n",
        "    threshold: float between 0 and 1\n",
        "      Fraction of validation points which each color must at least\n",
        "      represent.\n",
        "\n",
        "  Returns:\n",
        "    1-D array of ints (indices).\n",
        "  \"\"\"\n",
        "  random.seed(time.time())\n",
        "\n",
        "  # Amount of points for each color\n",
        "  amount_g = int(random.randint(size*threshold, size*(1-threshold)))\n",
        "  amount_r = size - amount_g\n",
        "\n",
        "  # Indices of each points with the specific color\n",
        "  indices_g = np.where(dataSet['l_i'] == 0)[0]\n",
        "  indices_r = np.where(dataSet['l_i'] == 1)[0]\n",
        "\n",
        "  # Check if possible \n",
        "  if indices_g.shape[0] + indices_r.shape[0] < size:\n",
        "    raise ValueError('The requested size of the validation set is not feasible')\n",
        "\n",
        "  if indices_r.shape[0] < amount_r:\n",
        "    indices_g += amount_r - indces_r.shape[0]\n",
        "\n",
        "  if indices_g.shape[0] < amount_g:\n",
        "    indices_r += amount_g - indces_g.shape[0]\n",
        "  \n",
        "  # Randomly selceting a subset for each color\n",
        "  indices_g = np.random.choice(indices_g, amount_g)\n",
        "  indices_r = np.random.choice(indices_r, amount_r)\n",
        "\n",
        "  # Concatenate and shuffle the chosen subsets\n",
        "  indices = np.concatenate([indices_g, indices_r])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  return indices\n",
        "\n",
        "def getProportionOfMisclassification(model, val_data):\n",
        "\n",
        "  # Creating Numpy arrays from tensors\n",
        "  points = val_data[0]\n",
        "  labels = val_data[1].astype('float')\n",
        "\n",
        "  # Counting number of points for each class\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  prediction = model.predict(val_data[0])\n",
        "\n",
        "  # Determining the incorrect predictions\n",
        "  incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  # Counting the number of misclassifications\n",
        "  total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  return ((total_misclassifications/number_of_points)*100,\n",
        "          (red_misclassifications/red_points)*100,\n",
        "          (green_misclassifications/green_points)*100)\n",
        "\n",
        "\n",
        "def penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=True, verbose=0):\n",
        "\n",
        "  if increasing:\n",
        "    array_penalties = np.linspace(0, penalty, (epochs - epoch_end_of_inc) // increment)\n",
        "  else:\n",
        "    array_penalties = np.linspace(penalty, 0, (epochs - epoch_end_of_inc) // increment)\n",
        "\n",
        "  for i in range((epochs - epoch_end_of_inc) // increment):\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(array_penalties[i]), metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(training_points, training_labels, batch_size=batch_size, epochs=increment,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "\n",
        "  model.fit(training_points, training_labels, batch_size=batch_size, epochs=epochs - epoch_end_of_inc,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "  \n",
        "\n",
        "def thresholdPredict(data, model, threshold):\n",
        "  \"\"\"Generates output predictions for the input samples. Points are only\n",
        "    predicted as green if the model's certainty for green is > threshold. All \n",
        "    other points are predicted red.\n",
        "\n",
        "  Args:\n",
        "    data: array-like, tensors, tf.data dataset...\n",
        "      Input samples.\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    threshold: float between 0.5 and 1\n",
        "      The minimum certainty required for the network to predict a point as green.\n",
        "\n",
        "  Returns:\n",
        "    Numpy array(s) of predictions.\n",
        "  \"\"\"\n",
        "  prediction = model.predict(data)\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i,0] >= 0.5 and prediction[i,0] < threshold:\n",
        "      temp = prediction[i,0]\n",
        "      prediction[i,0] = prediction[i,1]\n",
        "      prediction[i,1] = temp\n",
        "\n",
        "  return prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woke-yOw50gQ",
        "cellView": "form"
      },
      "source": [
        "#@title Visualisation\n",
        "\n",
        "def printProgressBar(iteration, total, prefix = '', suffix = '', decimals = 1,\n",
        "                     length = 100, fill = '█'):\n",
        "  \"\"\"Prints a progress bar.\n",
        "\n",
        "  Args:\n",
        "    iteration: int\n",
        "      Current progress step as. (iteration/total progress).\n",
        "    total: int\n",
        "      Total progress steps until completion.\n",
        "    prefix: str, optional\n",
        "      Printed infront of the progress bar.\n",
        "    suffix: str, optional\n",
        "      Printed behind ETA.\n",
        "    decimals: int, optional\n",
        "      Number of decimal places of percentage progress.\n",
        "    length: int, optional\n",
        "      Length of the progress bar in characters.\n",
        "    fill: char, optional\n",
        "      Filler of the progress bar.\n",
        "  \"\"\"\n",
        "  # Preparing strings\n",
        "  percentage_progress = (100*(iteration/float(total)))\n",
        "  percent = (\"{0:.\" + str(decimals) + \"f}\").format(percentage_progress)\n",
        "  filledLength = int(length * iteration // total)\n",
        "  bar = fill * filledLength + '-' * (length - filledLength)\n",
        "\n",
        "  # Bob's alternative time calculation\n",
        "  if iteration == 0:\n",
        "    global PB_START_TIME\n",
        "    PB_START_TIME = time.time()\n",
        "    time_so_far = 0\n",
        "    time_remaining = 0\n",
        "  else:\n",
        "    time_so_far = time.time() - PB_START_TIME\n",
        "    time_remaining = time_so_far/percentage_progress * (100-percentage_progress)\n",
        "\n",
        "  sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% | ETA: {round((time_remaining/60), 2)} minutes | {suffix}')\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Erease progress bar on complete\n",
        "  if iteration == total:\n",
        "    global PREV_TIME\n",
        "    PREV_TIME = 0\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def makePlot(dataSet=CURRENT_SET, correct_pred_points = np.array([]),\n",
        "             incorrect_pred_points = np.array([]), drawGrid=True,\n",
        "             savePlot=False, path=''):\n",
        "  \"\"\"\"Plots green and red points and markers as scatter graph.\n",
        "  \n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame or char, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "      Dataset to be plotted.\n",
        "    correct_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing correctly predicted points. Marked as black\n",
        "      'x' on scatter graph.\n",
        "    incorrect_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing incorrectly predicted points. Marked as\n",
        "      black '*' on scatter graph.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Raises:\n",
        "    TypeError: If dataSet is not an instance of pd.DataFrame or char or the\n",
        "    other parameters do not have the required shape.\n",
        "  \"\"\"\n",
        "  # Preparing optional parameters\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if isinstance(correct_pred_points, list):\n",
        "    correct_pred_points = np.array(correct_pred_points)\n",
        "  if isinstance(incorrect_pred_points, list):\n",
        "    incorrect_pred_points = np.array(incorrect_pred_points)\n",
        "\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be ' +\n",
        "                    f'{pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if (correct_pred_points.shape != (correct_pred_points.shape[0],2)\n",
        "      and np.array(correct_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter correct_pred_points is: \\\n",
        "      {np.array(correct_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  if (incorrect_pred_points.shape != (incorrect_pred_points.shape[0],2)\n",
        "      and np.array(incorrect_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter incorrect_pred_points is: \\\n",
        "      {np.array(incorrect_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  # Creating a subplot\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Scattering all points\n",
        "  x = dataSet['x_i1']\n",
        "  y = dataSet['x_i2']\n",
        "  c = [COLORS[i] for i in dataSet['l_i']] \n",
        "\n",
        "  ax.scatter(x, y, c=c)\n",
        "\n",
        "  # Adding markers to the specified points\n",
        "  if correct_pred_points.shape[0] > 0:\n",
        "    ax.scatter(correct_pred_points[:, 0], correct_pred_points[:, 1],\n",
        "              marker = \"x\", c = 'black', label='correct')\n",
        "  if incorrect_pred_points.shape[0] > 0:\n",
        "    ax.scatter(incorrect_pred_points[:, 0], incorrect_pred_points[:, 1],\n",
        "            marker = \"*\", c = 'black', label='incorrect')\n",
        "\n",
        "  if correct_pred_points.shape[0] > 0 or incorrect_pred_points.shape[0] > 0:\n",
        "    plt.legend()\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.axis('scaled')\n",
        "  ax.set_title(f'Dataset {dataSet_char}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMap(model, accuracy=100, specific_color=None,\n",
        "                     useThresholdPredict=False, drawGrid=True, verbose=1,\n",
        "                     savePlot=False, path=''):\n",
        "  \"\"\"Plots the prediction certainty of the model for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's preidction certainty is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the certainty map or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy is not\n",
        "      an int.\n",
        "  \"\"\"\n",
        "  # Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    array = np.array([[j/accuracy, i/accuracy] for j in range(accuracy)])\n",
        "    \n",
        "    # Predict points\n",
        "    if useThresholdPredict == True:\n",
        "      result = thresholdPredict(array, model, MIN_GREEN_CERT)\n",
        "    else:\n",
        "      result = model.predict(array)\n",
        "\n",
        "    if specific_color != None:\n",
        "      # Saving the prediction for the specified color\n",
        "      accuracy_map[i] = result[:, specific_color]\n",
        "    \n",
        "    else:\n",
        "      result = result.max(axis=1) # Getting each max value\n",
        "\n",
        "      # Normalize the values which are between 0.5 <-> 1 to 0 <-> 1\n",
        "      accuracy_map[i] = result\n",
        "  \n",
        "    # Print current progress\n",
        "    printProgressBar(i, accuracy-1)\n",
        "\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if specific_color != None:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      ax.set_title(f'Certainty for {COLORS[specific_color]} in {CURRENT_SET}')\n",
        "    else:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      ax.set_title(f'Total certainty in {CURRENT_SET}')\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}CertaintyMap_{CURRENT_SET}.pdf')\n",
        "    fig.savefig(f'{path}CertaintyMap_{CURRENT_SET}.png', dpi=300)\n",
        "\n",
        "  return accuracy_map\n",
        "\n",
        "\n",
        "\n",
        "def plotLoss(history):\n",
        "  \"\"\"Plots training loss and validation loss with respect to training epochs.\n",
        "\n",
        "  Args:\n",
        "    history: keras History\n",
        "      history of keras model.\n",
        "  \"\"\"\n",
        "  if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'])\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def showPredictions(model, history, valSet_points, valSet_labels,\n",
        "                    useThresholdPredict=False, showCorrectPoints=False,\n",
        "                    drawGrid=True):\n",
        "  \"\"\"Plots the predictions for the validation points.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which performs the predictions.\n",
        "    valSet_points: 2-D array of shape (x,2)\n",
        "      Data points used for validation.\n",
        "    valSet_labels: 1-D array of shape (x,)\n",
        "      Ground truth labels of the validation points.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    showCorrectPoints: boolean, optional\n",
        "      Whether correctly classified points should be marked as black 'x' or not.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "\n",
        "  Returns:\n",
        "    2-dimensional numpy array of shape (x,2) with the predictions for the\n",
        "    validation points.\n",
        "  \"\"\"\n",
        "  # Predict the validation points\n",
        "  if useThresholdPredict == True:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identifying correctly and incorrectly classified points\n",
        "  correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == True)\n",
        "  incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  number_of_points = np.bincount(np.argmax(prediction, axis=1))\n",
        "\n",
        "  total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  #Average misclassification certainty\n",
        "  misclass_certainties = []\n",
        "  for i in incorrect_indices[0]:\n",
        "    misclass_certainties.append(np.max(prediction[i]))\n",
        "  avg_misclass_certainty = sum(misclass_certainties)/total_misclassifications\n",
        "  \n",
        "  valAccuracy = 100 - (total_misclassifications/sum(number_of_points))*100\n",
        "\n",
        "  print('Validation accuracy: {:.2f}%'.format(valAccuracy))\n",
        "  print(f'Predictions for green: {number_of_points[0]} / {len(valSet_labels)}')\n",
        "  print(f'Predictions for red: {number_of_points[1]} / {len(valSet_labels)}')\n",
        "  print(f'Points misclassified: {total_misclassifications}')\n",
        "  print(f'Red points misclassified: {red_misclassifications}')\n",
        "  print(f'Green points misclassified: {green_misclassifications}')\n",
        "  print('Average misclassification certainty: {:.2f}'.format(avg_misclass_certainty))\n",
        "\n",
        "  if showCorrectPoints:\n",
        "    makePlot(correct_pred_points=valSet_points[correct_indices],\n",
        "           incorrect_pred_points=valSet_points[incorrect_indices], \n",
        "           drawGrid=drawGrid)\n",
        "  else:\n",
        "    makePlot(incorrect_pred_points=valSet_points[incorrect_indices], \n",
        "             drawGrid=drawGrid)\n",
        "    \n",
        "  # Make bar graph showing red and green misclassifications\n",
        "  bars = ('Red', 'Green')\n",
        "  height = [red_misclassifications, green_misclassifications]\n",
        "  x_pos = np.arange(len(bars))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(x_pos, height, width=0.35, color=['red', 'green'])\n",
        "\n",
        "  ax.set_ylabel('Misclassifications')\n",
        "  ax.set_title('Misclassifications by color')\n",
        "  ax.set_xticks(x_pos)\n",
        "  ax.set_xticklabels(bars)\n",
        "\n",
        "  rects = ax.patches # Array of bars\n",
        "\n",
        "  labels = [red_misclassifications, green_misclassifications]\n",
        "\n",
        "  for rect, label in zip(rects, labels): # Add labels above bars\n",
        "      height = rect.get_height()\n",
        "      ax.text(rect.get_x() + rect.get_width() / 2, height, label,\n",
        "              ha='center', va='bottom')\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "def makeDensityMap(accuracy, dataSet=CURRENT_SET, significance=0.1,\n",
        "                   cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                   drawGrid=True, verbose=1, savePlot=False, path=''):\n",
        "  \"\"\"Creates a heatmap of the density of dataSet.\n",
        "\n",
        "    Args:\n",
        "      accuracy: int, optional\n",
        "        Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "        of data points plotted has the dimension accuracy*accuracy.\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      verbose: 0 or 1, optional\n",
        "        Whether to plot the density map or not.\n",
        "      savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "      path: str, optional\n",
        "        Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "    Returns:\n",
        "      2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  density_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  printProgressBar(0, accuracy**2)\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      count = dataSet.loc[(dataSet['x_i1'] - j/accuracy)**2 + \n",
        "              (dataSet['x_i2'] - i/accuracy)**2 <= significance**2]\n",
        "      \n",
        "      density_map[i,j] = len(count)\n",
        "\n",
        "      printProgressBar(i*accuracy + j + 1, accuracy**2)\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(density_map),vmax=np.max(density_map))\n",
        "\n",
        "  # Plotting \n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if specific_color != None:\n",
        "      ax.set_title(f'Density of {COLORS[specific_color]} in {dataSet_char}')\n",
        "    else:\n",
        "      ax.set_title(f'Total density in {dataSet_char}')\n",
        "\n",
        "    plt.imshow(density_map, origin='lower', cmap='Spectral', norm=norm,\n",
        "               extent=[0, 1, 0, 1])\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}DensityMap_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}DensityMap_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "  return density_map\n",
        "  \n",
        "\n",
        "\n",
        "def plotDensity(dataSet=CURRENT_SET, significance=0.1,\n",
        "                cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Colorises and plots the points of dataSet according to their numbers\n",
        "    of neighbors.\n",
        "\n",
        "    Args:\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "      path: str, optional\n",
        "        Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \"\"\"\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  array = np.zeros((total_number_of_points, 3))\n",
        "\n",
        "  # Counting all neighbours within a radius of significance\n",
        "  for i in range(total_number_of_points):\n",
        "    count = dataSet.loc[(dataSet['x_i1'] - dataSet['x_i1'].loc[i])**2 +\n",
        "     (dataSet['x_i2'] - dataSet['x_i2'].loc[i])**2 <= significance**2]\n",
        "\n",
        "    array[i, 0] = dataSet['x_i1'].loc[i]\n",
        "    array[i, 1] = dataSet['x_i2'].loc[i]\n",
        "    array[i, 2] = len(count)\n",
        "\n",
        "    printProgressBar(i+1, total_number_of_points)\n",
        "\n",
        "  print(f'Max: {np.max(array[:,2])}')\n",
        "  print(f'Min: {np.min(array[:,2])}')\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(array[:,2]),vmax=np.max(array[:,2]))\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.scatter(array[:, 0], array[:, 1], c=array[:, 2], cmap=cmap, norm=norm)\n",
        " \n",
        "  ax.set_title(f'Density of dataset {dataSet_char}')\n",
        "  plt.axis('scaled')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show() \n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Density_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Density_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "def makeWeightedCertaintyMap(model, accuracy, significance=0.1,\n",
        "                          useThresholdPredict=False, referenceMethod='even',\n",
        "                          referenceValue=None, drawGrid=True, savePlot=False,\n",
        "                          path=''):\n",
        "  \"\"\"Plots the model's prediction certainty weighted with the density of points\n",
        "    given in the dataset.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's weighted prediction certainty is to be plotted.\n",
        "    accuracy: int\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False)\n",
        "      when calculating the model's prediction certainty.\n",
        "    referenceMethod: str: 'even', 'maxDensity', or 'customValue', optional\n",
        "      The method used to calculate the reference density used. 'even'\n",
        "      calculates the density if all points in dataset were evenly spaced. \n",
        "      'maxDensity' uses the maximum density from densityMap as the reference\n",
        "      ´density. 'customValue' uses a custom reference density.\n",
        "    referenceValue: float between 0 and 1, optional\n",
        "      Defines the custom reference density when using 'customValue' reference\n",
        "      method. Leave blank otherwise. \n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if invalid referenceMethod was given.\n",
        "\n",
        "  \"\"\"\n",
        "  if not referenceMethod in ['even', 'evenSqrt', 'evenLog', 'maxDensity',\n",
        "                             'customValue']:\n",
        "   raise TypeError(f'Invalid referenceMethod given. referenceMethod should be' +\n",
        "                   f' \"even\", \"evenSqrt\", \"evenLog\", \"maxDensity\", or ' +\n",
        "                   f'\"customValue\", but \"{referenceMethod}\" was given.')\n",
        "    \n",
        "  dataSet = getDataSet()\n",
        "\n",
        "  print(f'Calculating certainty map:')\n",
        "  certaintyMap = makeCertaintyMap(model, accuracy, None, useThresholdPredict,\n",
        "                                  verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  print(f'Calculating density map:')\n",
        "  densityMap = makeDensityMap(accuracy, significance=significance, verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  if referenceMethod == 'even':\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenSqrt':\n",
        "    densityMap = np.sqrt(densityMap)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenLog':\n",
        "    densityMap = np.log(densityMap+1)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'maxDensity':\n",
        "    maxDensity = np.max(densityMap)\n",
        "    densityMap = densityMap/maxDensity\n",
        "\n",
        "  elif referenceMethod == 'customValue':\n",
        "    densityMap = densityMap/referenceValue\n",
        "\n",
        "  weightedCertaintyMap = certaintyMap*densityMap\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  plt.imshow(weightedCertaintyMap, origin='lower', cmap='tab20b', vmin=0,\n",
        "             vmax=np.max(weightedCertaintyMap), extent=[0, 1, 0, 1])\n",
        "  \n",
        "  ax.set_title(f'Weighted certainty of datset {CURRENT_SET}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}WeightedCertaintyMap_{CURRENT_SET}.pdf')\n",
        "    fig.savefig(f'{path}WeightedCertaintyMap_{CURRENT_SET}.png', dpi=300)\n",
        "\n",
        "  return weightedCertaintyMap\n",
        "\n",
        "\n",
        "\n",
        "def makeDistributionMap(dataSet=CURRENT_SET, accuracy=10, colorbarLim=-1,\n",
        "                        drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Plots the distribution of dataSet.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'. Dataset who's distribution is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      The distribution map is split up into accuracy*accuracy many fields.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum\n",
        "      distribution percentage is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = dataSet\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  distribution_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    distribution_map[x_i2,x_i1] = distribution_map[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  distribution_map = distribution_map/len(dataSet)\n",
        "\n",
        "  # Plotting \n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_title(f'Distribution of datset {dataSet_char}')\n",
        "\n",
        "\n",
        "  if colorbarLim == -1:\n",
        "    colorbarLim = np.max(distribution_map)\n",
        "\n",
        "  plt.imshow(distribution_map, origin='lower', cmap='Spectral', vmin=0,\n",
        "             vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "  \n",
        "\n",
        "  plt.colorbar()\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}DistributionMap_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}DistributionMap_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "  return distribution_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htUB_naf6hZ1",
        "cellView": "form"
      },
      "source": [
        "#@title Penalty Effect\n",
        "\n",
        "def calculatePenaltyEffect(model, x, y, validation_data, interval=(0,1), accuracy=10, \n",
        "                      batch_size=32, epochs=200, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to penalty.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int, optional\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  penalties = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    penalty = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(penalty),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "    history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "    prediction = model.predict(validation_data[0])\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    penalties[i] = penalty\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, iterations+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(penalties, total_misclass_percentages, 'b', penalties, \n",
        "              red_misclass_percentages, 'r', penalties, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by penalty')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Penalty')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averagePenaltyEffect(model, n, valSet_size, path='', interval=(0,1),\n",
        "                         accuracy=10, batch_size=32, epochs=200, verbose=1):\n",
        "  \"\"\"Plots average penalty effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the penalty effect is measured and averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    All others:\n",
        "      See calculatePenaltyEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(penalties), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculatePenaltyEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "         green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotPenaltyEffect(model, data=result, interval=interval, accuracy=accuracy, n=n, \n",
        "                    valSet_size=valSet_size, batch_size=batch_size, epochs=epochs,\n",
        "                    path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}Penalty_Data_{CURRENT_SET}_{model.name}_' +\n",
        "                          f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=penalties).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, penalties,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{interval}', f'{accuracy}', f'{batch_size}', f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotPenaltyEffect(model, data, interval, accuracy, n, valSet_size, batch_size, epochs,\n",
        "                      dataset=CURRENT_SET, ylim=[0,10], maj_yt_incr=1,\n",
        "                      min_yt_incr=0.1, figsize=(14,10), showParameters=True,\n",
        "                      resolution=300, path=''):\n",
        "  \"\"\"Plots average penalty effect given by 'data' and saves png and pdf of plot\n",
        "    to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs:\n",
        "      Parameters used for training and calculaing the average penalty effect.\n",
        "      Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the penalty effect was measured on. 'A', 'B' or 'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Penalties to be plotted on the x-axis\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(penalties, total_misclass_percentages_avg, 'b', penalties, \n",
        "            red_misclass_percentages_avg, 'r', penalties,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by penalty',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Penalty', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(penalties)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpYD2sOxkBjP",
        "cellView": "form"
      },
      "source": [
        "#@title k-Nearest-Neighbour\n",
        "\n",
        "def KNN(dataSet, point, k, significance=0.1, increment=0.05, show_plot=True):\n",
        "  \"\"\" K-nearest neighbor classifier.\n",
        "\n",
        "  Statistical classifier. Uses the k nearest neighbors to predict the color of a\n",
        "  given point by comparing the number of neighbours of each color and weigthing\n",
        "  them with their squared distance to the point.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset to be used for\n",
        "      calculation. Defaults to dataset selected by CURRENT_SET.\n",
        "    point: Array in the form of [x_i1, x_i2]\n",
        "    k: Positive int\n",
        "      Number of neighbours taken into account for classification\n",
        "    significance: float between 0 and 1, optional\n",
        "      Start search radius.\n",
        "    increment: float between 0 and 1, optional\n",
        "      Amount of increment of the search radius while gathering k neighbours.\n",
        "    show_plot: boolean, optional\n",
        "      If 'True' the function plots the dataset and the selected neighbours.\n",
        "\n",
        "  Returns:\n",
        "    A 2-tuple with the predictions for each class. \n",
        "    (prediction_green, prediction_red)\n",
        "  \"\"\"\n",
        "  # Gathering points until at least k neighbours are found \n",
        "  neighb = np.array([])\n",
        "  while significance <= 1 and neighb.shape[0] < k:\n",
        "      neighb = dataSet.loc[(dataSet['x_i1'] - point[0])**2 +\n",
        "                           (dataSet['x_i2'] -point[1])**2 <= significance**2]\n",
        "      significance += increment\n",
        "  \n",
        "  # Reindexing\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "  # Removing all overhang neighbours until there are only k\n",
        "  while neighb.shape[0] > k:\n",
        "    neighb = neighb.drop(np.argmax(dist))\n",
        "    dist[np.argmax(dist)] = -1\n",
        "\n",
        "  # Reindexing\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "\n",
        "  pred_g = 0\n",
        "  pred_r = 0\n",
        "\n",
        "  # Sum the neighbours of each color with the weight 1-dist^2 \n",
        "  for i in range(neighb.shape[0]):\n",
        "    if neighb['l_i'].loc[i] == 0:\n",
        "      pred_g += (1 - dist[i])\n",
        "    elif neighb['l_i'].loc[i] == 1:\n",
        "      pred_r += (1 - dist[i])\n",
        "\n",
        "  # Normalize\n",
        "  pred_g = pred_g / neighb.shape[0]\n",
        "  pred_r = pred_r / neighb.shape[0]\n",
        "\n",
        "  # Plot neighbours \n",
        "  if show_plot:\n",
        "    selected_neighb = [[neighb['x_i1'].loc[i], neighb['x_i2'].loc[i]]\n",
        "                       for i in range(neighb.shape[0])]\n",
        "    makePlot(dataSet, [point], selected_neighb)\n",
        "    print(f'Prediction for green: \\t{pred_g}')\n",
        "    print(f'Prediction for red: \\t{pred_r}')\n",
        "\n",
        "  return (pred_g, pred_r)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMapKNN(k, accuracy = 100, specific_color = None):\n",
        "  \"\"\"Visualizes the prediction certainty of K-nearest-neighbour algorithm for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    k: postive int\n",
        "      The number of neighbours specified for the KNN algorithm who's certainty\n",
        "      is to bevisualized.\n",
        "    accuracy: positive int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid \n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red. \n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy and k\n",
        "    is not an int.\n",
        "  \"\"\"\n",
        "  #Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "    \n",
        "  if not isinstance(k, int):\n",
        "    raise TypeError(f'Invalid type for k. Type is {type(k)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  # Init Data\n",
        "  dataSet = getDataSet()\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Main Loop\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      result = KNN(dataSet, [j/accuracy, i/accuracy], k, show_plot=False)\n",
        "\n",
        "      if specific_color != None:\n",
        "        # Saving the prediction for the specified color\n",
        "        accuracy_map[i,j] = result[specific_color]\n",
        "      else:\n",
        "        accuracy_map[i,j] = np.max(result)\n",
        "    \n",
        "      # Print current progress\n",
        "      printProgressBar((j+1) + i*accuracy, accuracy**2)\n",
        "\n",
        "  # Choosing headline\n",
        "  if specific_color != None:\n",
        "    plt.title(f'Certaintiy for {COLORS[specific_color]}')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1)\n",
        "\n",
        "  else:\n",
        "    plt.title(f'General Certainty')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1)\n",
        "\n",
        "  # Plot\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.xticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.yticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.show()\n",
        "  \n",
        "  return accuracy_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tepNATmlkBjy",
        "cellView": "form"
      },
      "source": [
        "#@title Epoch Batch Size\n",
        "\n",
        "def epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    saveAndPlot=True, path='', verbose=1):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number for a random validation set on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which classifies the validation set.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation set.\n",
        "    batchRange: 2-tuple of ints\n",
        "      The range of batch sizes used. (x,y) where x is the smallest and y is the\n",
        "      largest batch size used. \n",
        "    batchIncrements: int\n",
        "      Increment in which the batch size is increased.\n",
        "    epochRange: 2-tuple of ints\n",
        "      The range of epochs used. (x,y) where x is the smallest and y is the\n",
        "      largest epoch number used.\n",
        "    epochIncrements: int\n",
        "      Increment in which the epoch number is increased.\n",
        "    epsilon: float, optional\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "    saveAndPlot: boolean, optional\n",
        "      Whever to save results to Excel and plot graphs or not. Set to false when\n",
        "      using averageEpochsBatchSize.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print a progress bar or not.\n",
        "\n",
        "  Returns:\n",
        "    6-tuple (epochs, batch_sizes, total_misclass_percentage,\n",
        "    red_misclass_percentage, green_misclass_percentage, valSet).\n",
        "    First 5 elements are lists, valSet is pd.DataFrame.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  if verbose > 0:\n",
        "    start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs = []\n",
        "  batch_sizes = []\n",
        "  total_misclass_percentage = []\n",
        "  red_misclass_percentage = []\n",
        "  green_misclass_percentage = []\n",
        "\n",
        "  # Defining iteration lists\n",
        "  batch_size_iter = np.arange(batchRange[0], batchRange[1]+1, batchIncrements)\n",
        "  epoch_iter = np.arange(epochRange[0], epochRange[1]+1, epochIncrements)\n",
        "\n",
        "  if batch_size_iter[0] == 0:\n",
        "    batch_size_iter[0] = 1\n",
        "\n",
        "  # Preparing data\n",
        "  dataSet = getDataSet()\n",
        "  dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "  # Choose random validation set\n",
        "  random.seed(time.time())\n",
        "  val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "  valSet_points, valSet_labels = separateValidationSet(dataSet,val_indices)\n",
        "  \n",
        "  dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "  training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "  training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "  number_of_points = len(valSet_labels)\n",
        "  red_points = len(np.where(valSet_labels==1)[0])\n",
        "  green_points = len(np.where(valSet_labels==0)[0])\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    num_training_points = training_labels.shape[0]\n",
        "    progress = 0\n",
        "    full = 0\n",
        "    # Calculate full progress\n",
        "    for ep in epoch_iter:\n",
        "      for ba in batch_size_iter:\n",
        "        full += ep*math.ceil(num_training_points/ba)\n",
        "    # Print bar\n",
        "    printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Epoch loop\n",
        "  for ep in epoch_iter:\n",
        "    # Batch size loop\n",
        "    for ba in batch_size_iter:\n",
        "      epochs.append(ep)\n",
        "      batch_sizes.append(ba)\n",
        "\n",
        "      # Prepare model for classification\n",
        "      model.set_weights(initialWeights)\n",
        "\n",
        "      history = model.fit(x=training_points, y=training_labels, batch_size=ba, \n",
        "                          epochs=ep, verbose=0)\n",
        "\n",
        "      # Classification and saving results\n",
        "      prediction = model.predict(valSet_points)\n",
        "\n",
        "      correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                 == True)\n",
        "      incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                   == False)\n",
        "\n",
        "      total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "      red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "      green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "      total_misclass_percentage.append((total_misclassifications/number_of_points)*100)\n",
        "      red_misclass_percentage.append((red_misclassifications/red_points)*100)\n",
        "      green_misclass_percentage.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "      # Update progress bar\n",
        "      if verbose > 0:\n",
        "        progress += ep*math.ceil(num_training_points/ba)\n",
        "        printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "  \n",
        "  # Validation set\n",
        "  valSet = pd.DataFrame.from_dict({'x_i1':valSet_points[:,0],'x_i2':valSet_points[:,1],\n",
        "                                  'l_i':valSet_labels})\n",
        "  \n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}']}\n",
        "\n",
        "  index = ['model','dataset','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  if saveAndPlot==True:\n",
        "    today = date.today()\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}EBS_Data_{CURRENT_SET}_' + \n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "    \n",
        "    # Create multiindex for epochs and batch_sizes\n",
        "    arrays = [epochs,batch_sizes]\n",
        "\n",
        "    tuples = list(zip(*arrays))\n",
        "\n",
        "    multiindex = pd.MultiIndex.from_tuples(tuples,\n",
        "                                      names=[\"epoch\", \"batch_size\"])\n",
        "    \n",
        "    # All data\n",
        "    allData = pd.DataFrame({'total':total_misclass_percentage,\n",
        "                            'red':red_misclass_percentage,\n",
        "                            'green':green_misclass_percentage}, index=multiindex)\n",
        "    \n",
        "    allData.to_excel(writer, sheet_name='All Data')\n",
        "\n",
        "    # Optimum points\n",
        "    result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage)\n",
        "    optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "    optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "    # Parameters\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation set\n",
        "    valSet.to_excel(writer, sheet_name='Validation Set')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage, valSet)\n",
        "  \n",
        "  if saveAndPlot==True:\n",
        "    plotEpochsBatchSize(model, result, path=path)\n",
        "\n",
        "  return result\n",
        "  \n",
        "\n",
        "\n",
        "def calculateOptimumPoints(data, epsilon):\n",
        "  \"\"\"Calculates optimum points of epoch and batch size for total, red, and green\n",
        "    misclassification. \n",
        "\n",
        "  Args:\n",
        "    data: 5-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage) or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    epsilon: float\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "\n",
        "  Returns: pd.DataFrame\n",
        "    columns: [min_misclass, epsilon, opt_misclass, opt_epoch, opt_batch,\n",
        "             t_misclass_here, r_misclass here, g_misclass_here]\n",
        "    rows: [total, red, green]\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not in correct form.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The elements of the 5-tuple data must all have the' +\n",
        "                  f' same shape. The {i+1}. element has shape {data[i].shape}' +\n",
        "                  f' and the {i+2}. element has shape {data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The elements of the 5-tuple data must all be' + \n",
        "                      f' 1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass_percentage = data[2]\n",
        "    red_misclass_percentage = data[3]\n",
        "    green_misclass_percentage = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, 'All Data')\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "\n",
        "    epochs = data['epoch']\n",
        "    batch_sizes = data['batch_size']\n",
        "    total_misclass_percentage = data['total']\n",
        "    red_misclass_percentage = data['red']\n",
        "    green_misclass_percentage = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # CALCULATE OPTIMUM POINTS\n",
        "  # For total: Finds the configuration with total misclass within epsilon of\n",
        "  #   minimum total misclass which has the lowest red misclass.\n",
        "  # For green: Finds the configuration with green misclass within epsilon of\n",
        "  #   minimum green misclass which has the lowest red misclass.\n",
        "  # For red: Finds the configuration with red misclass within epsilon of\n",
        "  #   minimum red misclass which has the lowest total misclass.\n",
        "  columns = ['min_misclass', 'epsilon', 'opt_misclass', 'opt_epoch', 'opt_batch',\n",
        "             't_misclass_here', 'r_misclass here', 'g_misclass_here']\n",
        "  rows = ['total', 'red', 'green']\n",
        "  t_considerable_indices = []\n",
        "  r_considerable_indices = []\n",
        "  g_considerable_indices = []\n",
        "\n",
        "  #Total\n",
        "  t_min = np.min(total_misclass_percentage)\n",
        "  t_opt = np.argmin(total_misclass_percentage)  # Index of optimum point for t\n",
        "  for index in range(len(total_misclass_percentage)):\n",
        "    if total_misclass_percentage[index] <= (t_min+epsilon):\n",
        "      t_considerable_indices.append(index)\n",
        "  for index in t_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[t_opt]:\n",
        "      t_opt = index\n",
        "\n",
        "  #Green\n",
        "  g_min = np.min(green_misclass_percentage)\n",
        "  g_opt = np.argmin(green_misclass_percentage)  # Index of optimum point for g\n",
        "  for index in range(len(green_misclass_percentage)):\n",
        "    if green_misclass_percentage[index] <= (g_min+epsilon):\n",
        "      g_considerable_indices.append(index)\n",
        "  for index in g_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[g_opt]:\n",
        "      g_opt = index\n",
        "\n",
        "  #Red\n",
        "  r_min = np.min(red_misclass_percentage)\n",
        "  r_opt = np.argmin(red_misclass_percentage)  # Index of optimum point for r\n",
        "  for index in range(len(red_misclass_percentage)):\n",
        "    if red_misclass_percentage[index] <= (r_min+epsilon):\n",
        "      r_considerable_indices.append(index)\n",
        "  # Find point with lowest total misclass\n",
        "  # Only change r_opt if the improvement in total misclass is greater than the\n",
        "  #   loss in red misclass\n",
        "  for index in r_considerable_indices:  \n",
        "    if (total_misclass_percentage[index] < total_misclass_percentage[r_opt] and \n",
        "        (total_misclass_percentage[index]-total_misclass_percentage[r_opt] <\n",
        "         red_misclass_percentage[r_opt]-red_misclass_percentage[index])):\n",
        "      r_opt = index\n",
        "  \n",
        "  total_row = [t_min, epsilon, total_misclass_percentage[t_opt], epochs[t_opt],\n",
        "               batch_sizes[t_opt], total_misclass_percentage[t_opt],\n",
        "               red_misclass_percentage[t_opt], green_misclass_percentage[t_opt]]\n",
        "  red_row = [r_min, epsilon, red_misclass_percentage[r_opt], epochs[r_opt],\n",
        "               batch_sizes[r_opt], total_misclass_percentage[r_opt],\n",
        "               red_misclass_percentage[r_opt], green_misclass_percentage[r_opt]]\n",
        "  green_row = [g_min, epsilon, green_misclass_percentage[g_opt], epochs[g_opt],\n",
        "               batch_sizes[g_opt], total_misclass_percentage[g_opt],\n",
        "               red_misclass_percentage[g_opt], green_misclass_percentage[g_opt]]\n",
        "\n",
        "  return pd.DataFrame([total_row, red_row, green_row], index=rows, \n",
        "                      columns=columns)\n",
        "  \n",
        "\n",
        "def averageEpochsBatchSize(model, n, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    path='', verbose=1):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number averaged over n validation sets on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    n: int\n",
        "      Number of iterations.\n",
        "    All others:\n",
        "      See epochsBatchSize.\n",
        "\n",
        "  Returns:\n",
        "    5 tuple of lists (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "    green_misclass_avg).\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs_collected = []\n",
        "  batch_sizes_collected = []\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "\n",
        "  # For saving in excel\n",
        "  validationSets = {}\n",
        "  misclassCollected = {}\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # Collecting misclassification percentages\n",
        "    data = epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                           batchIncrements, epochRange, epochIncrements,\n",
        "                           epsilon=0, saveAndPlot=False, verbose=0)\n",
        "\n",
        "    epochs_collected.append(data[0])\n",
        "    batch_sizes_collected.append(data[1])\n",
        "    total_misclass_collected.append(data[2])\n",
        "    red_misclass_collected.append(data[3])\n",
        "    green_misclass_collected.append(data[4])\n",
        "\n",
        "    # Adding validation set to dictionary for dataframe\n",
        "    validationSets[f'x_i1:{i}'] = data[5]['x_i1']\n",
        "    validationSets[f'x_i2:{i}'] = data[5]['x_i2']\n",
        "    validationSets[f'l_i:{i}'] = data[5]['l_i']\n",
        "\n",
        "    # Adding misclassification data to dictionary for dataframe\n",
        "    misclassCollected[f'total:{i}'] = data[2]\n",
        "    misclassCollected[f'red:{i}'] = data[3]\n",
        "    misclassCollected[f'green{i}'] = data[4]\n",
        "\n",
        "    # Update progress bar\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  epochs = np.average(epochs_collected, axis=0)\n",
        "  batch_sizes = np.average(batch_sizes_collected, axis=0)\n",
        "  total_misclass_avg = np.average(total_misclass_collected, axis=0)\n",
        "  red_misclass_avg = np.average(red_misclass_collected, axis=0)\n",
        "  green_misclass_avg = np.average(green_misclass_collected, axis=0)\n",
        "\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  today = date.today()\n",
        "\n",
        "  # Create multiindex for epochs and batch_sizes\n",
        "  arrays = [epochs,batch_sizes]\n",
        "\n",
        "  tuples = list(zip(*arrays))\n",
        "\n",
        "  multiindex = pd.MultiIndex.from_tuples(tuples, names=[\"epoch\", \"batch_size\"])\n",
        "\n",
        "  # Initialize writer\n",
        "  writer = pd.ExcelWriter(f'{path}Avg_EBS_Data_{CURRENT_SET}_' + \n",
        "                        f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "  # Average \n",
        "  average = pd.DataFrame({'total':total_misclass_avg,\n",
        "                          'red':red_misclass_avg,\n",
        "                          'green':green_misclass_avg}, index=multiindex)\n",
        "\n",
        "  average.to_excel(writer, sheet_name='Average')\n",
        "\n",
        "  # Collected\n",
        "  misclassCollected = pd.DataFrame(misclassCollected, index=multiindex)\n",
        "  misclassCollected.to_excel(writer, sheet_name='Collected')\n",
        "\n",
        "  # Optimum points\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "  optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "  parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  validationSets = pd.DataFrame.from_dict(validationSets)\n",
        "  validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  \n",
        "  plotEpochsBatchSize(model, result, path=path, prefix='Avg_')\n",
        "  \n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotEpochsBatchSize(model, data, dataset=CURRENT_SET,\n",
        "                        misclass_range=(0,15), figsize=(14,10), resolution=300,\n",
        "                        cmap='viridis', path='', prefix=''):\n",
        "  \"\"\"Plots a 3D graph showing the relation between epoch number, batch size,\n",
        "    and percentage misclassification.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model which was used for training and classification.\n",
        "    data: 6-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage, valSet) or the name of an Excel\n",
        "      sheet present in the directory as a String (e.g. 'data.xlsx').\n",
        "    dataset: char, optional\n",
        "      The dataset used. 'A', 'B' or 'C'.\n",
        "    misclass_range: 2-tuple, optional\n",
        "      The range of misclassification percentages plotted (limits of the z-axis).\n",
        "    figsize: 2-tuple, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    cmap: Colormap, optional\n",
        "      A colormap for the surface patches.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    prefix: str, optional\n",
        "      appended to the front of the pnd and pdf file names\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is of invalid type or shape.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The first 5 elements of the tuple data must all ' +\n",
        "                  f'have the same shape. The {i+1}. element has shape ' +\n",
        "                  f'{data[i].shape} and the {i+2}. element has shape ' +\n",
        "                  f'{data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The first 5 elements of the tuple data must all be ' +\n",
        "                      f'1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass = data[2]\n",
        "    red_misclass = data[3]\n",
        "    green_misclass = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, sheet_name=0, index_col=[0,1])\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "    \n",
        "    index_list = list(data.index)\n",
        "    index_len = len(index_list)\n",
        "    \n",
        "    epochs = []\n",
        "    batch_sizes = []\n",
        "    for i in range(index_len):\n",
        "      epochs.append(index_list[i][0])\n",
        "      batch_sizes.append(index_list[i][1])\n",
        "\n",
        "    total_misclass = data['total']\n",
        "    red_misclass = data['red']\n",
        "    green_misclass = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # Plotting\n",
        "  fig = plt.figure(figsize=(figsize[0], figsize[1]*3))\n",
        "  # Total misclassification\n",
        "  ax_t = fig.add_subplot(3, 1, 1, projection='3d')\n",
        "  ax_t.plot_trisurf(epochs, batch_sizes, total_misclass, cmap=cmap)\n",
        "  ax_t.set_title(f'Dataset {dataset}: Total misclassification by epoch and batch size')\n",
        "  ax_t.set_xlabel('Epochs')\n",
        "  ax_t.set_ylabel('Batch size')\n",
        "  ax_t.set_zlabel('% misclassification')\n",
        "  ax_t.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Red misclassification\n",
        "  ax_r = fig.add_subplot(3, 1, 2, projection='3d')\n",
        "  ax_r.plot_trisurf(epochs, batch_sizes, red_misclass, cmap=cmap)\n",
        "  ax_r.set_title(f'Dataset {dataset}: Red misclassification by epoch and batch size')\n",
        "  ax_r.set_xlabel('Epochs')\n",
        "  ax_r.set_ylabel('Batch size')\n",
        "  ax_r.set_zlabel('% misclassification')\n",
        "  ax_r.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Green misclassification\n",
        "  ax_g = fig.add_subplot(3, 1, 3, projection='3d')\n",
        "  ax_g.plot_trisurf(epochs, batch_sizes, green_misclass, cmap=cmap)\n",
        "  ax_g.set_title(f'Dataset {dataset}: Green misclassification by epoch and batch size')\n",
        "  ax_g.set_xlabel('Epochs')\n",
        "  ax_g.set_ylabel('Batch size')\n",
        "  ax_g.set_zlabel('% misclassification')\n",
        "  ax_g.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdlLeaiYaqo"
      },
      "source": [
        "#@title Different Training Approaches\n",
        "def diffPenaltyAproach(n, val_size, model, penalty, epochs, batch_size, increment, epoch_end_of_inc, verbose=0, figsize=(14,10), path=''):\n",
        "  '''Plots the average misclassification of each class for penalty increasing, \n",
        "  consistent penalty and penalty decreasing fitting in a bar graph.\n",
        "\n",
        "  Args:\n",
        "    n: int\n",
        "      Number of cyclces for averaging\n",
        "    model: keras model\n",
        "      Model which classifies the validation set.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    penalty: float between 0 and 1\n",
        "      Penalty to be added to the loss of misclassified red points during fitting\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation set.\n",
        "    epochs: int\n",
        "      Number of epochs for each training cycle\n",
        "    batch_size: int\n",
        "      Batch size to be used for training\n",
        "    epoch_increment: int\n",
        "      Amount of epochs after the penalty should be incremented\n",
        "    epoch_end_of_inc: int\n",
        "      Defines the point at which the increasing or decreasing stops and the modell\n",
        "      will train with a consistent penalty value for the rest of the epochs\n",
        "    verbose: boolean, optional\n",
        "      Whether to print a progress bar or not.\n",
        "    figsize: 2-tuple of int, optional\n",
        "      Determines the size of the figure\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  '''\n",
        "  dataSet_original = getDataSet()\n",
        "  valSets = [getBalancedValSetIndices(dataSet_original, val_size, THRESHOLD_VAL) for i in range(n)]\n",
        "\n",
        "  history_1 = np.zeros((n,3))\n",
        "  history_2 = np.zeros((n,3))\n",
        "  history_3 = np.zeros((n,3))\n",
        "\n",
        "  printProgressBar(0, 3*n)\n",
        "\n",
        "  model.set_weights(initialWeights)\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels)\n",
        "    \n",
        "    history_1[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i+1, 3*n)\n",
        "\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    model.fit(training_points, training_labels, epochs=epochs,\n",
        "                                batch_size=batch_size, shuffle=True, verbose=0)\n",
        "\n",
        "    history_2[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + n+1, 3*n)\n",
        "  \n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=False)\n",
        "    \n",
        "    history_3[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + 2*n+1, 3*n)\n",
        "  \n",
        "  clear_output()\n",
        "\n",
        "  labels = ['total', 'red', 'green']\n",
        "  y_1 = [i/n for i in np.sum(history_1, axis=0)]\n",
        "  y_2 = [i/n for i in np.sum(history_2, axis=0)]\n",
        "  y_3 = [i/n for i in np.sum(history_3, axis=0)]\n",
        "\n",
        "  x = np.arange(len(labels))  # the label locations\n",
        "  width = 0.2  # the width of the bars\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  rects1 = ax.bar(x - width, y_1, width, label='With Increment')\n",
        "  rects2 = ax.bar(x, y_2, width, label='Normal')\n",
        "  rects3 = ax.bar(x + width, y_3, width, label='With Decrement')\n",
        "\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_ylabel('Misclassification in %')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "\n",
        "  fig.text(0,0, f'Dataset: {CURRENT_SET}, Epochs: {epochs}, Batch Size: {batch_size}, Epoch Increment: {increment}, Epoch end of Increment: {epoch_end_of_inc}')\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Comparison{CURRENT_SET}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bljczC5kBj7",
        "cellView": "form"
      },
      "source": [
        "#@title Custom Loss Function\n",
        "\n",
        "def construct_custom_penalty_loss(penalty,\n",
        "                                  lossFunction=keras.losses.sparse_categorical_crossentropy):\n",
        "  \"\"\"Constructs a loss function which penalizes 'red as green' misclassifications. \n",
        "\n",
        "  Args:\n",
        "    penalty: float between 0 and 1\n",
        "      Value added to the loss if a red point is misclassified as green. \n",
        "    lossFunction: loss function, optional\n",
        "      The loss function used after adapting the loss values.\n",
        "\n",
        "  Returns:\n",
        "    custom_penalty_loss function with specified penalty and loss function. Can be\n",
        "    used like a regular loss function. \n",
        "  \"\"\"\n",
        "\n",
        "  def custom_penalty_loss(y_true, y_pred):\n",
        "    length = tf.shape(y_true)[0]\n",
        "\n",
        "    #Creating a vector with all values set to the penalty: [0.3, 0.3, ... 0.3]\n",
        "    error = tf.multiply(tf.constant(penalty, tf.float32), tf.ones(length)) \n",
        "\n",
        "    #Setting every entry to 0 if the corresponding entry in y_true is 1\n",
        "    error = tf.where(tf.equal(y_true[:, 0], tf.zeros(length)), error, tf.zeros(length))\n",
        "\n",
        "    #Setting every entry to 0 if the algorithm predicted 0\n",
        "    error = tf.where(tf.greater(y_pred[:, 0], y_pred[:, 1]), tf.zeros(length), error)\n",
        "\n",
        "    #Transforms the vector from [0, 0, 0.3, ... 0,3] to [[0, -0], [0, -0], [0.3, -0.3], ... [0.3, -0.3]]\n",
        "    error = tf.stack([error, tf.multiply(tf.constant(-1, tf.float32), error)], 1)\n",
        "\n",
        "    #Adding the artificial loss\n",
        "    y_pred = y_pred + error\n",
        "\n",
        "    #Eliminating values > 1 or < 0\n",
        "    y_pred0 = tf.where(tf.greater(y_pred[:, 0], tf.ones(length)), tf.ones(length), y_pred[:, 0])\n",
        "    y_pred1 = tf.where(tf.greater(y_pred[:, 1], tf.zeros(length)), y_pred[:, 1], tf.zeros(length))\n",
        "    y_pred = tf.stack([y_pred0, y_pred1], axis=1)\n",
        "\n",
        "\n",
        "    loss = lossFunction(y_pred=y_pred, y_true=y_true)\n",
        "    return loss\n",
        "  \n",
        "  return custom_penalty_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_01aKYUEy82",
        "cellView": "form"
      },
      "source": [
        "#@title Certainty Threshold Effect\n",
        "\n",
        "def calculateThresholdEffect(model, x, y, validation_data, interval=(0.8,1),\n",
        "                             accuracy=10, batch_size=64, epochs=500, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to the\n",
        "    certainty threshold for predicting points as green.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the threshold interval plotted. x is the lowest\n",
        "      threshold, y the highest.\n",
        "    accuracy: int, optional\n",
        "      Threshold interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  thresholds = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  # Initialize and fit model\n",
        "  model.set_weights(initialWeights)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "  history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    threshold = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "\n",
        "    prediction = thresholdPredict(validation_data[0], model, threshold)\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction,axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    thresholds[i] = threshold\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, accuracy+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(thresholds, total_misclass_percentages, 'b', thresholds, \n",
        "              red_misclass_percentages, 'r', thresholds, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by certainty threshold')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Certainty threshold')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averageThresholdEffect(model, n, valSet_size, path='', interval=(0.8,1),\n",
        "                         accuracy=10, batch_size=64, epochs=500, verbose=1):\n",
        "  \"\"\"Plots average certainty threshold effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the certainty threshold effect is measured and\n",
        "      averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    All others:\n",
        "      See calculateThresholdEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(thresholds), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = random.sample(range(SOURCE_SIZE[CURRENT_SET]), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculateThresholdEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "            green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotThresholdEffect(model, data=result, interval=interval, accuracy=accuracy,\n",
        "                      n=n, valSet_size=valSet_size, batch_size=batch_size,\n",
        "                      epochs=epochs, path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}CertaintyThreshold_Data_{CURRENT_SET}_' +\n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=thresholds).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, thresholds,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{interval}', f'{accuracy}', f'{batch_size}',\n",
        "                    f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotThresholdEffect(model, data, interval, accuracy, n, valSet_size,\n",
        "                      batch_size, epochs, penalty=PENALTY, dataset=CURRENT_SET,\n",
        "                      ylim=[0,10], maj_yt_incr=1, min_yt_incr=0.1,\n",
        "                      figsize=(14,10), showParameters=True, resolution=300,\n",
        "                      path=''):\n",
        "  \"\"\"Plots average certainty threshold effect given by 'data' and saves png and\n",
        "    pdf of plot to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the certainty threshold interval plotted. x is the\n",
        "      lowest penalty, y the highest.\n",
        "    accuracy: int\n",
        "      Certainty threshold interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs, penalty:\n",
        "      Parameters used for training and calculaing the average certainty\n",
        "      threshold effect. Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the certainty penalty effect was measured on. 'A', 'B' or\n",
        "      'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Thresholds to be plotted on the x-axis\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(thresholds, total_misclass_percentages_avg, 'b', thresholds, \n",
        "            red_misclass_percentages_avg, 'r', thresholds,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by certainty threshold',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Certainty threshold', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(thresholds)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}\\n' +\n",
        "                   f'Penalty: {penalty}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVX3rT2aStCu",
        "cellView": "form"
      },
      "source": [
        "#@title Points Per Square\n",
        "def pointsPerSquare(dataSet=CURRENT_SET, accuracy=100):\n",
        "  \"\"\"Calculates the number of points from dataSet present in each square of an\n",
        "    accuracy x accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'.\n",
        "    accuracy: int, optional\n",
        "      The grid consists of accuracy x accuracy many squares.\n",
        "\n",
        "  Returns:\n",
        "    Three 2-D np.array of the shape (accuracy,accuracy): squares, red and green.\n",
        "    squares contains the number of total points present in each square, red\n",
        "    contains the number of red points present in each square, and green contains\n",
        "    the number of green points present in each square.\n",
        "  \"\"\"\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  squares = np.zeros((accuracy, accuracy))\n",
        "  red = np.zeros((accuracy, accuracy))\n",
        "  green = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    squares[x_i2,x_i1] = squares[x_i2,x_i1]+1\n",
        "\n",
        "    if (dataSet.loc[i]['l_i']) == 0:\n",
        "      green[x_i2,x_i1] = green[x_i2,x_i1]+1\n",
        "    else:\n",
        "      red[x_i2,x_i1] = red[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  return squares, red, green"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkIU-4-J-jw",
        "cellView": "form"
      },
      "source": [
        "#@title Misclassifications per square\n",
        "def misclassPerSquare(model, validationSet, accuracy=10, useThresholdPredict=False,\n",
        "                      drawGrid=True, verbose=0, savePlot=False, path='',\n",
        "                      colorbarLim=-1):\n",
        "  \"\"\"Calculates proportion of red, green, and total validaion points\n",
        "    misclassified per square in an accuracy*accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    validationSet: 2-tuple of np.arrays\n",
        "      2-tuple of the form (valSet_points, valSet_labels), where valSet_points is\n",
        "      a np.array of shape (x,2) and valSet_labels is a np.array of shape (x,1).\n",
        "    accuracy: int, optional\n",
        "      The dataset is split up into accuracy*accuracy many fields.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the misclassifications per square or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plot will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum misclass\n",
        "      proportion is used as the upper limit.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple 2-D np.arrays (total, red, green) of shape (accuracy,accuracy)\n",
        "    containing the proportions of total, red, and green misclassifications per\n",
        "    square as floats between 0 and 1.\n",
        "  \"\"\"\n",
        "  # Predicting the validation points\n",
        "  valSet_points = validationSet[0]\n",
        "  valSet_labels = validationSet[1]\n",
        "\n",
        "  # Preparing arrays\n",
        "  totalPoints = np.zeros((accuracy,accuracy))\n",
        "  totalMisclass = np.zeros((accuracy,accuracy))\n",
        "  redPoints = np.zeros((accuracy,accuracy))\n",
        "  redMisclass = np.zeros((accuracy,accuracy))\n",
        "  greenPoints = np.zeros((accuracy,accuracy))\n",
        "  greenMisclass = np.zeros((accuracy,accuracy))\n",
        "\n",
        "  # Predicting points\n",
        "  if useThresholdPredict:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identifying incorrectly classified points\n",
        "  incorrect_indices = np.where((valSet_labels != np.argmax(prediction, axis=1)))\n",
        "\n",
        "  # Multiplying all entries with accuracy to calculate which square each\n",
        "  # validation point falls into\n",
        "  valSet_points = valSet_points*accuracy\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, len(valSet_points))\n",
        "\n",
        "  for i in range(len(valSet_points)):\n",
        "    x_i1 = math.floor(valSet_points[i,0])\n",
        "    x_i2 = math.floor(valSet_points[i,1])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    # Total\n",
        "    totalPoints[x_i2,x_i1] += 1\n",
        "    if i in incorrect_indices[0]:\n",
        "      totalMisclass[x_i2,x_i1] += 1\n",
        "    \n",
        "    # Red\n",
        "    if valSet_labels[i] == 1:\n",
        "      redPoints[x_i2,x_i1] += 1\n",
        "\n",
        "      if i in incorrect_indices[0]:\n",
        "        redMisclass[x_i2,x_i1] += 1\n",
        "    \n",
        "    # Green\n",
        "    if valSet_labels[i] == 0:\n",
        "      greenPoints[x_i2,x_i1] += 1\n",
        "\n",
        "      if i in incorrect_indices[0]:\n",
        "        greenMisclass[x_i2,x_i1] += 1\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, len(valSet_points))\n",
        "\n",
        "  # Setting all 0 entries in xPoints to 1 to prevent div by zero\n",
        "  totalPoints[totalPoints == 0] = 1\n",
        "  redPoints[redPoints == 0] = 1\n",
        "  greenPoints[greenPoints == 0] = 1\n",
        "\n",
        "  # Calculating proportion of validation points misclassified\n",
        "  totalMisclassPerSquare = totalMisclass/totalPoints\n",
        "  redMisclassPerSquare = redMisclass/redPoints\n",
        "  greenMisclassPerSquare = greenMisclass/greenPoints\n",
        "\n",
        "  # PLOTTING\n",
        "  today = date.today()\n",
        "  # Total\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of val points misclassified in {CURRENT_SET}')\n",
        "   \n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(totalMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(totalMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving total plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Red\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of r val points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(redMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(redMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving red plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Green\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of g val points misclassified in {CURRENT_SET}')\n",
        "    \n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(greenMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(greenMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving green plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  return (totalMisclassPerSquare, redMisclassPerSquare, greenMisclassPerSquare)\n",
        "\n",
        "\n",
        "\n",
        "def avgMisclassPerSquare(model, initialWeights, n, valSet_size, batch_size,\n",
        "                         epochs, accuracy=10, useThresholdPredict=False,\n",
        "                         drawGrid=True, verbose=1, savePlot=False,\n",
        "                         saveToExcel=False, path='', colorbarLim=-1):\n",
        "  \"\"\"CURRENTLY WITHOUT BALANCING DATASET BEFORE TRAINING.\n",
        "    Calculates average proportions of red, green, and total validaion points\n",
        "    misclassified per square in an accuracy*accuracy grid over n training rounds\n",
        "    and validation sets.\n",
        "    \n",
        "  Args:\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    n: int\n",
        "      The number of training/predition rounds to average over.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation sets.\n",
        "    batch_size: int\n",
        "      Batch size used for training the model.\n",
        "    epochs: int\n",
        "      Number of epochs used for training the model.\n",
        "    accuracy: int, optional\n",
        "      The dataset is split up into accuracy*accuracy many fields.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot results or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    saveToExcel: boolean, optional\n",
        "      Whether to save the results as an Excel document or not.\n",
        "    path: str, optional\n",
        "      Path to which the results will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum avg\n",
        "      misclass proportion is used as the upper limit.\n",
        "\n",
        "  Returns: 6-tuple (at, ar, ag, ct, cr, cg)\n",
        "    - (at, ar, ag): 2-D np.arrays (avgTotal, avgRed, avgGreen) of shape\n",
        "    (accuracy,accuracy) containing the avg proportions of total, red, and green\n",
        "    misclassifications per square as floats between 0 and 1.\n",
        "    - (ct, cr, cg): 1-D lists of shape (n,) of 2-D np.arrays of shape\n",
        "    (accuracy,accuracy) containing the proportions of total, red, and green\n",
        "    misclassifications per square of each run as floats between 0 and 1.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "  avgTotalMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  avgRedMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  avgGreenMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  validationSets = {}\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  dataSet_original = getDataSet()\n",
        "\n",
        "  for i in range(n):\n",
        "    # Preparing data\n",
        "    dataSet = dataSet_original.copy()\n",
        "    dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]\n",
        "\n",
        "    # Choose random validation set\n",
        "    random.seed(time.time())\n",
        "    val_indices = random.sample(range(len(dataSet)), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet, val_indices)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Classifying validation set\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    history = model.fit(x=training_points, y=training_labels,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "    \n",
        "    # Calculating misclassification per square\n",
        "    result = misclassPerSquare(model, (valSet_points,valSet_labels), accuracy,\n",
        "                               useThresholdPredict)\n",
        "    \n",
        "    total_misclass_collected.append(result[0])\n",
        "    red_misclass_collected.append(result[1])\n",
        "    green_misclass_collected.append(result[2])\n",
        "\n",
        "    # Saving validation set for Excel\n",
        "    if saveToExcel == True:\n",
        "      valSet_points = pd.DataFrame(valSet_points)\n",
        "      valSet_labels = pd.DataFrame(valSet_labels)\n",
        "\n",
        "      validationSets[f'x_i1:{i}'] = valSet_points[0]\n",
        "      validationSets[f'x_i2:{i}'] = valSet_points[1]\n",
        "      validationSets[f'l_i:{i}'] = valSet_labels[0]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  avgTotalMisclassPerSquare = np.average(total_misclass_collected, axis=0)\n",
        "  avgRedMisclassPerSquare = np.average(red_misclass_collected, axis=0)\n",
        "  avgGreenMisclassPerSquare = np.average(green_misclass_collected, axis=0)\n",
        "\n",
        "  # PLOTTING  \n",
        "  today = date.today()\n",
        "  # Total\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgTotalMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgTotalMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving total plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Red\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of r points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgRedMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgRedMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving red plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Green\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of g points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgGreenMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgGreenMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving green plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "    \n",
        "\n",
        "  # Saving to Excel \n",
        "  if saveToExcel == True:\n",
        "    # Averages\n",
        "    index = [i/accuracy for i in range(accuracy)]\n",
        "    index.reverse()  # Reverse index for correct orientation in Excel\n",
        "    columns = [i/accuracy for i in range(accuracy)]  \n",
        "    # Flip for correct orientation in Excel\n",
        "    totalFlipped = np.flipud(avgTotalMisclassPerSquare)\n",
        "    redFlipped = np.flipud(avgRedMisclassPerSquare)\n",
        "    greenFlipped = np.flipud(avgGreenMisclassPerSquare)\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}Avg_Misclass_Per_Square_' +\n",
        "                            f'{CURRENT_SET}_{model.name}_' +\n",
        "                            f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "    # Total \n",
        "    total = pd.DataFrame(totalFlipped, columns=columns,\n",
        "                         index=index)\n",
        "    \n",
        "    total.to_excel(writer, sheet_name='Total')\n",
        "\n",
        "    # Red\n",
        "    red = pd.DataFrame(redFlipped, columns=columns,\n",
        "                       index=index)\n",
        "    \n",
        "    red.to_excel(writer, sheet_name='Red')\n",
        "\n",
        "    # Green\n",
        "    green = pd.DataFrame(greenFlipped, columns=columns,\n",
        "                       index=index)\n",
        "    \n",
        "    green.to_excel(writer, sheet_name='Green')\n",
        "\n",
        "    # Parameters\n",
        "    data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{accuracy}',\n",
        "                      f'{valSet_size}', f'{batch_size}', f'{epochs}',\n",
        "                      f'{PENALTY}', f'{useThresholdPredict}',\n",
        "                      f'{MIN_GREEN_CERT}']}\n",
        "\n",
        "    index = ['model','dataset','n','accuracy','valSet_size','batch_size',\n",
        "             'epochs','penalty','useThresholdPredict','min_green_cert']\n",
        "\n",
        "    parameters = pd.DataFrame(data, index=index)\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation sets\n",
        "    validationSets = pd.DataFrame(validationSets)\n",
        "    validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "    # Collected\n",
        "    index = [i/accuracy for i in range(accuracy)]\n",
        "    index.reverse()  # Reverse index for correct orientation in Excel\n",
        "    columns = [i/accuracy for i in range(accuracy)]  \n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}Collected_Misclass_Per_Square_' +\n",
        "                            f'{CURRENT_SET}_{model.name}_' +\n",
        "                            f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "    \n",
        "    # Iterate over all training/validation runs\n",
        "    for i in range(n):\n",
        "      # Flip for correct orientation in Excel\n",
        "      totalCollectedFlipped = np.flipud(total_misclass_collected[i])\n",
        "      redCollectedFlipped = np.flipud(red_misclass_collected[i])\n",
        "      greenCollectedFlipped = np.flipud(green_misclass_collected[i])\n",
        "      \n",
        "      # Total \n",
        "      total = pd.DataFrame(totalCollectedFlipped, columns=columns,\n",
        "                          index=index)\n",
        "      \n",
        "      total.to_excel(writer, sheet_name=f'total_{i}')\n",
        "\n",
        "      # Red\n",
        "      red = pd.DataFrame(redCollectedFlipped, columns=columns,\n",
        "                        index=index)\n",
        "      \n",
        "      red.to_excel(writer, sheet_name=f'red_{i}')\n",
        "\n",
        "      # Green\n",
        "      green = pd.DataFrame(greenCollectedFlipped, columns=columns,\n",
        "                        index=index)\n",
        "      \n",
        "      green.to_excel(writer, sheet_name=f'green_{i}')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return (avgTotalMisclassPerSquare, avgRedMisclassPerSquare,\n",
        "          avgGreenMisclassPerSquare, total_misclass_collected,\n",
        "          red_misclass_collected, green_misclass_collected)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy2HWZscTuIm",
        "cellView": "form"
      },
      "source": [
        "#@title Weighted Misclassification Probability\n",
        "def weightedMisclassProbability(distributionMap, misclassPerSquare, model,\n",
        "                                specific_color=None, verbose=1, colorbarLim=-1,\n",
        "                                drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Calculates the probability that the next point will be misclassified\n",
        "    by weighting the misclassification probaility of each square with the\n",
        "    probability distribution of the datset.\n",
        "  \n",
        "  Args:\n",
        "    distributionMap: 2-D np.array of shape (x,x)\n",
        "      The distribution map of the dataset.\n",
        "    misclassPerSquare: 6-tuple of 2-D np.array of shape (x,x) or str\n",
        "      (at, ar, ag, ct, cr, cg) misclassification probabilities per square as\n",
        "      floats between 0 and 1 or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    model: keras.model\n",
        "      The model used for calculating misclassPerSquare (only used for naming\n",
        "      files here).\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, calculates weighted green misclassification probability. If 1,\n",
        "      analogously for red. If none given, total misclassification probability is\n",
        "      calculated.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to show the plot or not.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum\n",
        "      misclassification probability is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    Float between 0 and 1. The total probability that the next point will be\n",
        "    misclassified.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if distributionMap.shape != misclassPerSquare.shape.\n",
        "    TypeError: if specific_color is not 0, 1, or None.\n",
        "  \"\"\"\n",
        "  if specific_color != None and specific_color != 0 and specific_color != 1:\n",
        "    raise TypeError(f'specific_color should be 0, 1, or None, but is ' +\n",
        "                    f'{specific_color}.')\n",
        "    \n",
        "  # Getting data from Excel sheet\n",
        "  if type(misclassPerSquare) == str:\n",
        "    data = pd.ExcelFile(misclassPerSquare)\n",
        "    if specific_color == None:\n",
        "      data = pd.read_excel(data, sheet_name='Total', index_col=0)\n",
        "    elif specific_color == 1:\n",
        "      data = pd.read_excel(data, sheet_name='Red', index_col=0)\n",
        "    elif specific_color == 0:\n",
        "      data = pd.read_excel(data, sheet_name='Green', index_col=0)\n",
        "    \n",
        "    misclassPerSquare = np.array(data)\n",
        "    misclassPerSquare = np.flipud(misclassPerSquare)\n",
        "  \n",
        "  # Getting selected color from tuple\n",
        "  if type(misclassPerSquare) == tuple:\n",
        "    if specific_color == None:\n",
        "      misclassPerSquare = misclassPerSquare[0]\n",
        "    elif specific_color == 1:\n",
        "      misclassPerSquare = misclassPerSquare[1]\n",
        "    elif specific_color == 0:\n",
        "      misclassPerSquare = misclassPerSquare[2]\n",
        "    \n",
        "  # Checking shapes\n",
        "  if distributionMap.shape != misclassPerSquare.shape:\n",
        "    raise TypeError(f'distributionMap.shape and misclassPerSquare.shape must ' + \n",
        "                    f'be equal. distributionMap.shape is ' +\n",
        "                    f'{distributionMap.shape} and misclassPerSquare.shape is ' +\n",
        "                    f'{misclassPerSquare.shape}.')\n",
        "\n",
        "  weightedMap = distributionMap*misclassPerSquare\n",
        "\n",
        "  result = np.sum(weightedMap)\n",
        "\n",
        "  if specific_color == None:\n",
        "    char = ''\n",
        "    color = 'total'\n",
        "    prefix = 'Total_'\n",
        "  elif specific_color == 0:\n",
        "    char = 'g '\n",
        "    color = 'green'\n",
        "    prefix = 'Green_'\n",
        "  elif specific_color == 1:\n",
        "    char = 'r '\n",
        "    color = 'red'\n",
        "    prefix = 'Red_'\n",
        "\n",
        "  # Plotting\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Weighted {char}misclass probability in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(weightedMap)\n",
        "\n",
        "    plt.imshow(weightedMap, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    config_info = (f'Total: {round(result*100,2)}%')\n",
        "    ax.text(1.05, 1.03, config_info, weight='bold')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  today = date.today()\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Weighted_{prefix}Misclass_Prob_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Weighted_{prefix}Misclass_Prob_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "  \n",
        "  print(f'Weighted {color} misclassification probability: {round(result*100,2)}%')\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjTBuhEJ65e5"
      },
      "source": [
        "#@title Calculate Upper Bounds\n",
        "def calculateUpperBounds(distributionMap, misclassPerSquare, n, proportionOfRuns,\n",
        "                         model, verbose=1, saveToExcel=False, path=''):\n",
        "  \"\"\"Calculates the upper bounds for total, red, and green misclassification\n",
        "    probability fulfilled by a given proportion of runs. \n",
        "\n",
        "  Args:\n",
        "    distributionMap: 2-D np.array of shape (x,x)\n",
        "      The distribution map of the dataset.\n",
        "    misclassPerSquare: 6-tuple of 2-D np.array of shape (x,x) or str\n",
        "      (at, ar, ag, ct, cr, cg) misclassification probabilities per square as\n",
        "      floats between 0 and 1 or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    n: int\n",
        "      The number of training/prediction rounds. (The number of misclassPerSquare\n",
        "      maps collected in misclassPerSquare).\n",
        "    proportionOfRuns: float between 0 and 1\n",
        "      The proportion (as a float) of runs which must fulfill the upper bound.\n",
        "    model: keras.model\n",
        "      The model used for calculating misclassPerSquare (only used for naming\n",
        "      files here).\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to print the results or not.\n",
        "    saveToExcel: boolean, optional\n",
        "      Whether to save the calculated upper bounds to Excel or not.\n",
        "    path: str, optional\n",
        "      Path to which the sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of floats (t,r,g) containing the upper bounds for total, red, and\n",
        "    green misclassification probability fulfilled by the given proportion of\n",
        "    runs.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if distributionMap.shape is not equal to shape of each\n",
        "    misclassPerSquare array/sheet.\n",
        "  \"\"\"\n",
        "  # Preparing data arrays\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "  total_misclass_probs = []\n",
        "  red_misclass_probs = []\n",
        "  green_misclass_probs = []\n",
        "\n",
        "  # Getting data from Excel sheet\n",
        "  if type(misclassPerSquare) == str:\n",
        "    data = pd.ExcelFile(misclassPerSquare)\n",
        "    for i in range(n):\n",
        "      data_t = pd.read_excel(data, sheet_name=f'total_{i}', index_col=0)\n",
        "      data_t = np.array(data_t)\n",
        "      total_misclass_collected.append(np.flipud(data_t))\n",
        "      \n",
        "      data_r = pd.read_excel(data, sheet_name=f'red_{i}', index_col=0)\n",
        "      data_r = np.array(data_r)\n",
        "      red_misclass_collected.append(np.flipud(data_r))\n",
        "\n",
        "      data_g = pd.read_excel(data, sheet_name=f'green_{i}', index_col=0)\n",
        "      data_g = np.array(data_g)\n",
        "      green_misclass_collected.append(np.flipud(data_g))\n",
        "    \n",
        "  # Getting selected color from tuple\n",
        "  if type(misclassPerSquare) == tuple:\n",
        "      total_misclass_collected = misclassPerSquare[3]\n",
        "      red_misclass_collected = misclassPerSquare[4]\n",
        "      green_misclass_collected = misclassPerSquare[5]\n",
        "      \n",
        "  # Checking for correct shapes\n",
        "  if distributionMap.shape != total_misclass_collected[0].shape:\n",
        "    raise TypeError(f'distributionMap.shape and shape of misclassPerSquare ' +  \n",
        "                    f'arrays must be equal. distributionMap.shape is ' +\n",
        "                    f'{distributionMap.shape} and shape of misclassPerSquare ' +\n",
        "                    f'is {total_misclass_collected[0].shape}.')\n",
        "    \n",
        "  # Calculating misclassification probabilities for each run and color\n",
        "  for i in range(n):\n",
        "    t_prob = (np.sum(distributionMap*total_misclass_collected[i]))\n",
        "    total_misclass_probs.append(t_prob)\n",
        "\n",
        "    r_prob = (np.sum(distributionMap*red_misclass_collected[i]))\n",
        "    red_misclass_probs.append(r_prob)\n",
        "\n",
        "    g_prob = (np.sum(distributionMap*green_misclass_collected[i]))\n",
        "    green_misclass_probs.append(g_prob)\n",
        "\n",
        "  #################################################################\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3YU__k-8CfW"
      },
      "source": [
        "# Magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR2BPwsfkBj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd25acfa-8c02-4ab4-ce13-ef81aac7aae1"
      },
      "source": [
        "# Data Preparation\n",
        "dataSet = getDataSet()\n",
        "dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet, validationIndices=VAL_INDICES)\n",
        "\n",
        "dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA)\n",
        "\n",
        "#Creating tensors\n",
        "training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "training_points = np.array(dataSet[['x_i1','x_i2']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificially exended by 0 points\n",
            "Relation is now: 0.47 green : 0.53 red \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuey_TJrkBj8"
      },
      "source": [
        "# Configure and compile model\n",
        "initalizer = keras.initializers.GlorotNormal()\n",
        "\n",
        "model_0 = keras.Sequential([\n",
        "          keras.layers.Flatten(input_shape=(2,)),      #input layer: 2 neurons\n",
        "          keras.layers.Dense(100,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(70,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(50,activation='relu', kernel_initializer=initalizer),       \n",
        "          keras.layers.Dense(10,activation='relu', kernel_initializer=initalizer),\n",
        "          keras.layers.Dense(2,activation='softmax', kernel_initializer=initalizer)   #output layer: 2 neurons              \n",
        "          ], name=\"model_0\")\n",
        "\n",
        "model_0.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Save initial weights\n",
        "initialWeights = model_0.get_weights()\n",
        "\n",
        "# Fit model\n",
        "history = model_0.fit(training_points, training_labels, batch_size=912, epochs=100,\n",
        "                    shuffle=True, validation_data=(valSet_points, valSet_labels))\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQXEUuYZxzG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "f7afbd7a-5704-4d62-90e6-57c029d6451c"
      },
      "source": [
        "dist = makeDistributionMap(accuracy=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxZMRbIreja1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "da65dcab-2c9c-49a1-ae86-b7a7fa5cfae8"
      },
      "source": [
        "misclass_thresh = avgMisclassPerSquare(model_0, initialWeights, 3, 800, 912, 1800,\n",
        "                             20, savePlot=True, saveToExcel=True, useThresholdPredict=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken: 0.93 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqvZNI7nespc"
      },
      "source": [
        "# res_t_thresh = weightedMisclassProbability(dist, misclass_thresh, model_0, specific_color=None,\n",
        "#                                            savePlot=True)\n",
        "# res_r_thresh = weightedMisclassProbability(dist, misclass_thresh, model_0, specific_color=1,\n",
        "#                                            savePlot=True)\n",
        "# res_g_thresh = weightedMisclassProbability(dist, misclass_thresh, model_0, specific_color=0,\n",
        "#                                            savePlot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPkoNJCgDW5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43107e6f-306a-43c0-aa74-e05dff19b4f0"
      },
      "source": [
        "len(dist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}