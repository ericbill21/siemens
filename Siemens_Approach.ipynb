{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siemens_Approach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbill21/siemens/blob/cleanup/Siemens_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70udFHYSHNI"
      },
      "source": [
        "#@title Imports and coinfig\n",
        "%tensorflow_version 2.x\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "#time prediciton\n",
        "prev_time = 0\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "tf.random.set_seed(31415)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E98cwmoUT7w1"
      },
      "source": [
        "#@title Global constants\n",
        "\n",
        "#Dictionaries\n",
        "colors = {0 : 'green', 1 : 'red', 'green' : 0, 'red' : 1}\n",
        "sources = {'A' : 'https://drive.google.com/file/d/1hAzAKZNpmSclSI7HnV_cRjpMS4Kh5r1q/view?usp=sharing', 'B' : 'https://drive.google.com/file/d/12VlecL-5iYs-BFpnTOba1x65jWofBX1P/view?usp=sharing', 'C' : 'https://drive.google.com/file/d/1-Z0RuJIi1cZcqrrmV6TqT0O1PwI2OiBY/view?usp=sharing'}\n",
        "SOURCE_SIZE = {'A': 1000,'B' : 5000, 'C' : 50000}\n",
        "\n",
        "CURRENT_SET = 'B'\n",
        "\n",
        "#Examlpe subsets of each dataset\n",
        "subsetA = [47, 847, 993, 55, 102, 572, 430, 115, 842, 72, 770, 107, 78, 834, 593, 43, 234, 709, 210, 378]\n",
        "subsetB = [606, 2663, 1809, 2145, 4539, 3333, 3562, 2262, 512, 2046, 1541, 909, 286, 4815, 3663, 1742, 2822, 2756, 2937, 3080, 3845, 3949, 2506, 3984, 2803, 2067]\n",
        "subsetC = [32088, 33534, 39634, 40177, 25142, 752, 41771, 11793, 16415, 3811, 2096, 35902, 42221, 19594, 25109, 40476, 25162, 41150, 34610, 28329, 46339, 43149, 44441, 25720, 38747, 49497, 12708, 23920, 2280, 17946]\n",
        "\n",
        "VAL_INDICES = subsetB\n",
        "\n",
        "#Penalty applied to false green classifications in custom loss function\n",
        "PENALTY = 0.3"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISkimS6oUTP9"
      },
      "source": [
        "#@title Functions\n",
        "\n",
        "def getDataSet():\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+sources[CURRENT_SET].split('/')[-2]\n",
        "  return pd.read_excel(path)\n",
        "\n",
        "\n",
        "#Plots the given dataSet in the rigth colors\n",
        "def makePlot(t_data = None, marker = [], marker_false = []):\n",
        "\n",
        "  if t_data == None:\n",
        "    t_data = getDataSet() \n",
        "  \n",
        "  data_x1 = np.array(t_data.pop('x_i1'))\n",
        "  data_x2 = np.array(t_data.pop('x_i2'))\n",
        "  data_l = np.array(t_data.pop('l_i'))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  \n",
        "  for i in range(2):\n",
        "    x_1 = []\n",
        "    x_2 = []\n",
        "    for j in range(np.size(data_x1)):\n",
        "      if data_l[j] == i:\n",
        "        x_1.append(data_x1[j])\n",
        "        x_2.append(data_x2[j])\n",
        "    ax.scatter(x_1, x_2, c = colors[i])\n",
        "\n",
        "  for i in marker:\n",
        "    ax.scatter(i[0],i[1],marker = \"x\", c = 'black')\n",
        "\n",
        "  for i in marker_false:\n",
        "    ax.scatter(i[0],i[1],marker = \"*\", c = 'black')\n",
        "\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.title(f'DataSet {CURRENT_SET}')\n",
        "  plt.axis('scaled')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Prepares a subset of the dataSet for validation\n",
        "def popPredictionPoints(dataSet, validationIndices):\n",
        "  sampleSet_x1_x2 = []\n",
        "  sampleSet_li = []\n",
        "\n",
        "  for i in validationIndices:\n",
        "    sampleSet_x1_x2.append([dataSet['x_i1'].loc[i], dataSet['x_i2'].loc[i]])\n",
        "    sampleSet_li.append([dataSet['l_i'].loc[i]])\n",
        "  \n",
        "  #Saving the testing points\n",
        "  prediction_point = tf.constant(sampleSet_x1_x2, tf.float32)\n",
        "  prediciton_label = tf.constant(sampleSet_li, tf.float32)\n",
        "\n",
        "  #Removing the testing point\n",
        "  dataSet.drop(index=validationIndices, inplace=True)\n",
        "  dataSet.reset_index(inplace=True)\n",
        "\n",
        "  return (prediction_point, prediciton_label)\n",
        "\n",
        "\n",
        "\n",
        "# Print iterations progress\n",
        "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% ETA: {round(timeCalc()*(total-iteration), 2)} minutes {suffix}')\n",
        "    sys.stdout.flush()\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "      global prev_time\n",
        "      prev_time = 0\n",
        "      sys.stdout.write('\\r')\n",
        "      sys.stdout.flush()\n",
        "\n",
        "\n",
        "def makeCertaintiyMap(model, accuracy = 100, specific_color = None):\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    tensor = tf.constant([[j/accuracy, i/accuracy] for j in range(accuracy)], tf.float32)\n",
        "    result = model.predict(tensor)\n",
        "\n",
        "    if specific_color != None:\n",
        "      accuracy_map[i] = result[:, specific_color]\n",
        "    else:\n",
        "      result = result.max(axis=1) #Getting each max value\n",
        "      if max(result) != min(result):\n",
        "        normalized = (result-min(result))/(max(result)-min(result))\n",
        "        accuracy_map[i] = normalized\n",
        "      else: \n",
        "        accuracy_map[i] = result\n",
        "      \n",
        "    \n",
        "    printProgressBar(i, accuracy-1)\n",
        "\n",
        "\n",
        "  plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1)\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "  return accuracy_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plotAllPenaltyCombinations(model, accuracy = 100):\n",
        "  print('Chosen Subset:')\n",
        "  makePlot(marker=prediction_points.numpy())\n",
        "\n",
        "  result_x = np.zeros(accuracy+1)\n",
        "  result_y = np.zeros(accuracy+1)\n",
        "\n",
        "  printProgressBar(0, accuracy,prefix='Progress', suffix='ETA: ')\n",
        "\n",
        "  #Iterating over every penalty\n",
        "  for i in range(accuracy+1):\n",
        "    keras.backend.clear_session()\n",
        "    prevTime = time.time()\n",
        "\n",
        "    penalty = i/accuracy #Selecting a value between 0 and 1 with steps of 1/accuracy\n",
        "\n",
        "    model.compile(optimizer='adam',loss=construct_custom_penalty_loss(penalty),metrics=['accuracy'])\n",
        "    model.fit(training_points, training_labels, batch_size=32, epochs=10, shuffle=True, verbose=0)\n",
        "    \n",
        "    #Saving results\n",
        "    result_x[i] = penalty\n",
        "    result_y[i] = model.evaluate(prediction_points, prediction_labels, verbose=0)[0]\n",
        "\n",
        "    #Printing progress\n",
        "    printProgressBar(i+1, accuracy+1,prefix='Progress')\n",
        "\n",
        "  #Presenting the results\n",
        "  print('\\nThe loss:')\n",
        "  plt.plot(result_x, result_y)\n",
        "\n",
        "  return result_y\n",
        "\n",
        "\n",
        "def timeCalc():\n",
        "  global prev_time\n",
        "  if prev_time == 0:\n",
        "    prev_time = time.time()\n",
        "    return 0\n",
        "  \n",
        "  res = (time.time() - prev_time) / 60\n",
        "  prev_time = time.time()\n",
        "  return res\n",
        "\n",
        "\n",
        "def plotHistory(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def showPredicitons(model, prediction_points, prediction_labels):\n",
        "  prediction = model.predict(prediction_points)\n",
        "\n",
        "  points = prediction_points.numpy()\n",
        "  labels = prediction_labels.numpy()[:, 0].astype(int)\n",
        "\n",
        "  correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "  wrong_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  number_of_points = np.bincount(np.argmax(prediction, axis=1))\n",
        "\n",
        "  print(f'Predictions for green: {number_of_points[0]} / {len(labels)}')\n",
        "  print(f'Predictions for red: {number_of_points[1]} / {len(labels)}')\n",
        "  print(f'The algorithm predicted {np.bincount(labels == np.argmax(prediction, axis=1))[0]} times wrong')\n",
        "  makePlot(marker=points[correct_indices], marker_false=points[wrong_indices])\n",
        "  print(f' \\'*\\' stands for wrong predicted')\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsy_7qMzuBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aef3781-c5ed-4390-9537-a2ba834f425a"
      },
      "source": [
        "#Preparing data\n",
        "dataSet = getDataSet()\n",
        "dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "prediction_points, prediction_labels = popPredictionPoints(dataSet=dataSet, validationIndices=VAL_INDICES)\n",
        "\n",
        "#Artificially balancing the dataSet\n",
        "number_of_green_points = list(dataSet['l_i']).count(colors['green'])\n",
        "number_of_red_points = list(dataSet['l_i']).count(colors['red'])\n",
        "\n",
        "if number_of_red_points / SOURCE_SIZE[CURRENT_SET] <= 0.5:\n",
        "  amount = int((0.5 - number_of_red_points / SOURCE_SIZE[CURRENT_SET]) * SOURCE_SIZE[CURRENT_SET])\n",
        "  red_points = dataSet.loc[dataSet['l_i'] == 1] #Getting all red points\n",
        "  choosen_points = red_points.sample(amount, replace=True) #Selecting a random subset of red points\n",
        "  dataSet = dataSet.append(choosen_points, ignore_index=True) #appending the subset\n",
        "\n",
        "if number_of_green_points / SOURCE_SIZE[CURRENT_SET] <= 0.5:\n",
        "  amount = int((0.5 - number_of_green_points / SOURCE_SIZE[CURRENT_SET]) * SOURCE_SIZE[CURRENT_SET])\n",
        "  green_points = dataSet.loc[dataSet['l_i'] == 0] #Getting all green points\n",
        "  choosen_points = green_points.sample(amount, replace=True) #Selecting a random subset of green points\n",
        "  dataSet = dataSet.append(choosen_points, ignore_index=True) #appending green subset\n",
        "\n",
        "if 'index' in dataSet.columns:\n",
        "  dataSet.pop('index') #removing old indices\n",
        "  print(f'Artificially exended by {dataSet[\"x_i1\"].size - SOURCE_SIZE[CURRENT_SET] + len(VAL_INDICES)} points')\n",
        "  print(f'Relation is now: {dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]} green  : {dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]} red ')\n",
        "\n",
        "#Creating tensors\n",
        "training_labels = tf.constant(dataSet.pop('l_i'), tf.float32)\n",
        "training_points = tf.constant(dataSet, tf.float32)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificially exended by 140 points\n",
            "Relation is now: 2500 green  : 2614 red \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N24D-gWpnFmD"
      },
      "source": [
        "def construct_custom_penalty_loss(penalty=PENALTY):\n",
        "\n",
        "  def custom_penalty_loss(y_true,y_pred):\n",
        "    length = tf.shape(y_true)[0]\n",
        "\n",
        "    #Creating a vector with all values set to the penalty: [0.3, 0.3, ... 0.3]\n",
        "    error = tf.multiply(tf.constant(penalty, tf.float32), tf.ones(length)) \n",
        "\n",
        "    #Setting every entry to 0 if the corresponding entry in y_true is 1\n",
        "    error = tf.where(tf.equal(y_true[:, 0], tf.zeros(length)), error, tf.zeros(length))\n",
        "\n",
        "    #Setting every entry to 0 if the algorithm predicted 0\n",
        "    error = tf.where(tf.greater(y_pred[:, 0], y_pred[:, 1]), tf.zeros(length), error)\n",
        "\n",
        "    #Transforms the vector from [0, 0, 0.3, ... 0,3] to [[0, -0], [0, -0], [0.3, -0.3], ... [0.3, -0.3]]\n",
        "    error = tf.stack([error, tf.multiply(tf.constant(-1, tf.float32), error)], 1)\n",
        "\n",
        "    #Adding the artificial loss\n",
        "    y_pred = y_pred + error\n",
        "\n",
        "    #Eliminating values > 1 or < 0\n",
        "    y_pred0 = tf.where(tf.greater(y_pred[:, 0], tf.ones(length)), tf.ones(length), y_pred[:, 0])\n",
        "    y_pred1 = tf.where(tf.greater(y_pred[:, 1], tf.zeros(length)), y_pred[:, 1], tf.zeros(length))\n",
        "    y_pred = tf.stack([y_pred0, y_pred1], axis=1)\n",
        "\n",
        "\n",
        "    loss = keras.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y_true)\n",
        "    return loss\n",
        "  \n",
        "  return custom_penalty_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrxlkLjeccKz",
        "outputId": "a01526f6-2053-4fec-87c9-ad69385ea91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "           keras.layers.Flatten(input_shape=(2,)),      #input layer: 2 neurons\n",
        "           keras.layers.Dense(100,activation='relu'), \n",
        "           keras.layers.Dense(70,activation='relu'), \n",
        "           keras.layers.Dense(50,activation='relu'),       \n",
        "           keras.layers.Dense(10,activation='relu'),\n",
        "           keras.layers.Dense(2,activation='softmax')   #output layer: 2 neurons              \n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss=construct_custom_penalty_loss(PENALTY) ,metrics=['accuracy'])\n",
        "\n",
        "validation_set = tf.concat([prediction_points, prediction_labels], axis=1)\n",
        "\n",
        "history = model.fit(training_points, training_labels, batch_size=32, epochs=5, shuffle=True, )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "160/160 [==============================] - 4s 2ms/step - loss: 0.3996 - accuracy: 0.6183\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.8348\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.8463\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.8460\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 0s 2ms/step - loss: 0.2148 - accuracy: 0.8451\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}