{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siemens_Approach.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbill21/siemens/blob/master/Siemens_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppIOSk-I8wV2"
      },
      "source": [
        "# Imports, Config & GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeg6hj5kBjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be804acc-f6ef-4637-fef9-38a850c14939"
      },
      "source": [
        "# Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Arithmetic Operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Data visualization\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Progress calculation\n",
        "import sys\n",
        "import time\n",
        "from datetime import date\n",
        "\n",
        "# Time prediciton\n",
        "PREV_TIME = 0\n",
        "PB_START_TIME = 0\n",
        "\n",
        "# Mounting Google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd5eTUqAkBjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892e815b-f087-47f2-c414-9f6eef22d171"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# ram_gb = virtual_memory().total / 1e9\n",
        "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "# if ram_gb < 20:\n",
        "#   print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "#   print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "#   print('re-execute this cell.')\n",
        "# else:\n",
        "#   print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 21 17:08:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjZxd7_8MKI"
      },
      "source": [
        "# Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRjXSfIYkBjM"
      },
      "source": [
        "# Dictionaries\n",
        "COLORS = {0 : 'green', 1 : 'red', 'green' : 0, 'red' : 1}\n",
        "SOURCES = {'A' : 'https://drive.google.com/file/d/1hAzAKZNpmSclSI7HnV_cRjpMS4Kh5r1q/view?usp=sharing', 'B' : 'https://drive.google.com/file/d/12VlecL-5iYs-BFpnTOba1x65jWofBX1P/view?usp=sharing', 'C' : 'https://drive.google.com/file/d/1-Z0RuJIi1cZcqrrmV6TqT0O1PwI2OiBY/view?usp=sharing'}\n",
        "SOURCE_SIZE = {'A': 1000,'B' : 5000, 'C' : 50000}\n",
        "\n",
        "CURRENT_SET = 'A'\n",
        "\n",
        "# Balancing dataset to threshold\n",
        "THRESHOLD_DATA = 0.4\n",
        "\n",
        "# Threshold for balanced validation set\n",
        "THRESHOLD_VAL = 0.4\n",
        "\n",
        "# Minimum certainty required to predict green\n",
        "MIN_GREEN_CERT = 0.9\n",
        "\n",
        "# Random number seed\n",
        "random.seed(time.time())\n",
        "\n",
        "subsetA = random.sample(range(1000), 150)\n",
        "subsetB = random.sample(range(5000), 800)\n",
        "subsetC = random.sample(range(50000), 8000)\n",
        "\n",
        "VAL_INDICES = locals()['subset' + CURRENT_SET]\n",
        "\n",
        "# Penalty applied to false green classifications in custom loss function\n",
        "PENALTY = 0.2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVhVxxhc5T-a"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC174oUT5utW",
        "cellView": "form"
      },
      "source": [
        "#@title Data Operations\n",
        "\n",
        "def getDataSet(dataset=CURRENT_SET):\n",
        "  \"\"\"Returns pandas.DataFrame of dataset.\n",
        "  \n",
        "  Args:\n",
        "    dataset: char, optional\n",
        "      The dataset to return. 'A', 'B', or 'C'.\n",
        "  \"\"\"\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+SOURCES[dataset].split('/')[-2]\n",
        "  return pd.read_excel(path)\n",
        "\n",
        "def seperateValidationSet(dataSet, validationIndices):\n",
        "  \"\"\"Formats a subset of points from a dataset as validation points.\n",
        "\n",
        "  Validation points are extracted and deleted from dataSet to be used for\n",
        "  validation later on.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset which the\n",
        "      validation points are extracted from.\n",
        "    validationIndices: 1-D list of ints\n",
        "      The elements corresponding to these indices are extracted from dataSet.\n",
        "\n",
        "  Returns: 2-tuple of the form (valSet_points, valSet_labels), where valSet_points\n",
        "    is a tensor of shape (x,2) and valSet_labels is a tensor of shape (x,1).\n",
        "  \"\"\"\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be \\\n",
        "      {pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if len(np.array(validationIndices).shape) != 1:\n",
        "    raise TypeError(f'The shape of the parameter validationIndices is: \\\n",
        "      {np.array(validationIndices).shape}, but it should be 1 dimensional')\n",
        "  \n",
        "  valSet_points = dataSet[['x_i1','x_i2']].loc[validationIndices]\n",
        "  valSet_labels = dataSet['l_i'].loc[validationIndices]\n",
        "  \n",
        "  # Saving the validation points\n",
        "  valSet_points = np.array(valSet_points)\n",
        "  valSet_labels = np.array(valSet_labels).astype('float')\n",
        "\n",
        "  # Removing the validation point\n",
        "  dataSet.drop(index=validationIndices, inplace=True)\n",
        "  dataSet.reset_index(inplace=True)\n",
        "\n",
        "  return (valSet_points, valSet_labels)\n",
        "\n",
        "def timeCalc():\n",
        "  \"\"\"Calculates time between previous call and current call.\n",
        "\n",
        "  Returns:\n",
        "    Time difference in minutes as float.\n",
        "  \"\"\"\n",
        "  global PREV_TIME\n",
        "  if PREV_TIME == 0:\n",
        "    PREV_TIME = time.time()\n",
        "    return 0\n",
        "  \n",
        "  res = (time.time() - PREV_TIME) / 60\n",
        "  PREV_TIME = time.time()\n",
        "  return res\n",
        "\n",
        "def balanceDataset(dataSet, threshold, verbose=1):\n",
        "  \"\"\"Artificially balances dataSet by duplicating red or green points.\n",
        "\n",
        "  Args: \n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. The datset to be balanced.\n",
        "    threshold: float between 0 and 0.5\n",
        "      The function duplicates red or green points until the fraction of points\n",
        "      of the less frequent color is at least equal to the threshold.\n",
        "\n",
        "  Returns:\n",
        "    pandas.DataFrame with columns 'x_i1', 'x_i2', 'l_i1'.\n",
        "  \"\"\"\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  amount = 0\n",
        "\n",
        "  if number_of_red_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_red_points) // (1 - threshold) )\n",
        "    red_points = dataSet.loc[dataSet['l_i'] == 1] #Getting all red points\n",
        "    chosen_points = red_points.sample(amount, replace=True) #Selecting a random subset of red points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending the subset\n",
        "\n",
        "  if number_of_green_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_green_points) // (1 - threshold) )\n",
        "    green_points = dataSet.loc[dataSet['l_i'] == 0] #Getting all green points\n",
        "    chosen_points = green_points.sample(amount, replace=True) #Selecting a random subset of green points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending green subset\n",
        "\n",
        "  dataSet = dataSet[['x_i1','x_i2','l_i']]\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  if verbose > 0:\n",
        "    print(f'Artificially exended by {amount} points')\n",
        "    print(f'Relation is now: {round(number_of_green_points / total_number_of_points, 2)}',\n",
        "            f'green : {round(number_of_red_points / total_number_of_points, 2)} red ')\n",
        "  \n",
        "  return dataSet\n",
        "\n",
        "def getBalancedValSetIndices(dataSet, size, threshold):\n",
        "  \"\"\"Get indices of validation points such that neither color represents\n",
        "    less than (threshold*100)% of the validation set.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset from\n",
        "      which the validation points are to be chosen.\n",
        "    size: int\n",
        "      Size of the validation set.\n",
        "    threshold: float between 0 and 1\n",
        "      Fraction of validation points which each color must at least\n",
        "      represent.\n",
        "\n",
        "  Returns:\n",
        "    1-D array of ints (indices).\n",
        "  \"\"\"\n",
        "  random.seed(time.time())\n",
        "\n",
        "  # Amount of points for each color\n",
        "  amount_g = int(random.randint(size*threshold, size*(1-threshold)))\n",
        "  amount_r = size - amount_g\n",
        "\n",
        "  # Indices of each points with the specific color\n",
        "  indices_g = np.where(dataSet['l_i'] == 0)[0]\n",
        "  indices_r = np.where(dataSet['l_i'] == 1)[0]\n",
        "\n",
        "  # Check if possible \n",
        "  if indices_g.shape[0] + indices_r.shape[0] < size:\n",
        "    raise ValueError('The requested size of the validation set is not feasible')\n",
        "\n",
        "  if indices_r.shape[0] < amount_r:\n",
        "    indices_g += amount_r - indces_r.shape[0]\n",
        "\n",
        "  if indices_g.shape[0] < amount_g:\n",
        "    indices_r += amount_g - indces_g.shape[0]\n",
        "  \n",
        "  # Randomly selceting a subset for each color\n",
        "  indices_g = np.random.choice(indices_g, amount_g)\n",
        "  indices_r = np.random.choice(indices_r, amount_r)\n",
        "\n",
        "  # Concatenate and shuffle the chosen subsets\n",
        "  indices = np.concatenate([indices_g, indices_r])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  return indices\n",
        "\n",
        "def getProportionOfMisclassification(model, val_data):\n",
        "\n",
        "  # Creating Numpy arrays from tensors\n",
        "  points = val_data[0]\n",
        "  labels = val_data[1].astype('float')\n",
        "\n",
        "  # Counting number of points for each class\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  prediction = model.predict(val_data[0])\n",
        "\n",
        "  # Determining the incorrect predictions\n",
        "  incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  # Counting the number of misclassifications\n",
        "  total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  return ((total_misclassifications/number_of_points)*100,\n",
        "          (red_misclassifications/red_points)*100,\n",
        "          (green_misclassifications/green_points)*100)\n",
        "\n",
        "\n",
        "def penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=True, verbose=0):\n",
        "\n",
        "  if increasing:\n",
        "    array_penalties = np.linspace(0, penalty, (epochs - epoch_end_of_inc) // increment)\n",
        "  else:\n",
        "    array_penalties = np.linspace(penalty, 0, (epochs - epoch_end_of_inc) // increment)\n",
        "\n",
        "  for i in range((epochs - epoch_end_of_inc) // increment):\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(array_penalties[i]), metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(training_points, training_labels, batch_size=batch_size, epochs=increment,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "\n",
        "  model.fit(training_points, training_labels, batch_size=batch_size, epochs=epochs - epoch_end_of_inc,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "  \n",
        "\n",
        "def thresholdPredict(data, model, threshold):\n",
        "  \"\"\"Generates output predictions for the input samples. Points are only\n",
        "    predicted as green if the model's certainty for green is > threshold. All \n",
        "    other points are predicted red.\n",
        "\n",
        "  Args:\n",
        "    data: array-like, tensors, tf.data dataset...\n",
        "      Input samples.\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    threshold: float between 0.5 and 1\n",
        "      The minimum certainty required for the network to predict a point as green.\n",
        "\n",
        "  Returns:\n",
        "    Numpy array(s) of predictions.\n",
        "  \"\"\"\n",
        "  prediction = model.predict(data)\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i,0] >= 0.5 and prediction[i,0] < threshold:\n",
        "      temp = prediction[i,0]\n",
        "      prediction[i,0] = prediction[i,1]\n",
        "      prediction[i,1] = temp\n",
        "\n",
        "  return prediction\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woke-yOw50gQ"
      },
      "source": [
        "#@title Visualisation\n",
        "\n",
        "def printProgressBar(iteration, total, prefix = '', suffix = '', decimals = 1,\n",
        "                     length = 100, fill = '█'):\n",
        "  \"\"\"Prints a progress bar.\n",
        "\n",
        "  Args:\n",
        "    iteration: int\n",
        "      Current progress step as. (iteration/total progress).\n",
        "    total: int\n",
        "      Total progress steps until completion.\n",
        "    prefix: str, optional\n",
        "      Printed infront of the progress bar.\n",
        "    suffix: str, optional\n",
        "      Printed behind ETA.\n",
        "    decimals: int, optional\n",
        "      Number of decimal places of percentage progress.\n",
        "    length: int, optional\n",
        "      Length of the progress bar in characters.\n",
        "    fill: char, optional\n",
        "      Filler of the progress bar.\n",
        "  \"\"\"\n",
        "  # Preparing strings\n",
        "  percentage_progress = (100*(iteration/float(total)))\n",
        "  percent = (\"{0:.\" + str(decimals) + \"f}\").format(percentage_progress)\n",
        "  filledLength = int(length * iteration // total)\n",
        "  bar = fill * filledLength + '-' * (length - filledLength)\n",
        "\n",
        "  # Bob's alternative time calculation\n",
        "  if iteration == 0:\n",
        "    global PB_START_TIME\n",
        "    PB_START_TIME = time.time()\n",
        "    time_so_far = 0\n",
        "    time_remaining = 0\n",
        "  else:\n",
        "    time_so_far = time.time() - PB_START_TIME\n",
        "    time_remaining = time_so_far/percentage_progress * (100-percentage_progress)\n",
        "\n",
        "  sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% | ETA: {round((time_remaining/60), 2)} minutes | {suffix}')\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Erease progress bar on complete\n",
        "  if iteration == total:\n",
        "    global PREV_TIME\n",
        "    PREV_TIME = 0\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def makePlot(dataSet=CURRENT_SET, correct_pred_points = np.array([]),\n",
        "             incorrect_pred_points = np.array([]), drawGrid=False,\n",
        "             savePlot=False, path=''):\n",
        "  \"\"\"\"Plots green and red points and markers as scatter graph.\n",
        "  \n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame or char, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "      Dataset to be plotted.\n",
        "    correct_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing correctly predicted points. Marked as black\n",
        "      'x' on scatter graph.\n",
        "    incorrect_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing incorrectly predicted points. Marked as\n",
        "      black '*' on scatter graph.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot to the current directory or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Raises:\n",
        "    TypeError: If dataSet is not an instance of pd.DataFrame or char or the\n",
        "    other parameters do not have the required shape.\n",
        "  \"\"\"\n",
        "  # Preparing optional parameters\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "  if isinstance(correct_pred_points, list):\n",
        "    correct_pred_points = np.array(correct_pred_points)\n",
        "  if isinstance(incorrect_pred_points, list):\n",
        "    incorrect_pred_points = np.array(incorrect_pred_points)\n",
        "\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be ' +\n",
        "                    f'{pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if (correct_pred_points.shape != (correct_pred_points.shape[0],2)\n",
        "      and np.array(correct_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter correct_pred_points is: \\\n",
        "      {np.array(correct_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  if (incorrect_pred_points.shape != (incorrect_pred_points.shape[0],2)\n",
        "      and np.array(incorrect_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter incorrect_pred_points is: \\\n",
        "      {np.array(incorrect_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  # Creating a subplot\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Scattering all points\n",
        "  x = dataSet['x_i1']\n",
        "  y = dataSet['x_i2']\n",
        "  c = [COLORS[i] for i in dataSet['l_i']] \n",
        "\n",
        "  ax.scatter(x, y, c=c)\n",
        "\n",
        "  # Adding markers to the specified points\n",
        "  if correct_pred_points.shape[0] > 0:\n",
        "    ax.scatter(correct_pred_points[:, 0], correct_pred_points[:, 1],\n",
        "              marker = \"x\", c = 'black', label='correct')\n",
        "  if incorrect_pred_points.shape[0] > 0:\n",
        "    ax.scatter(incorrect_pred_points[:, 0], incorrect_pred_points[:, 1],\n",
        "            marker = \"*\", c = 'black', label='incorrect')\n",
        "\n",
        "  if correct_pred_points.shape[0] > 0 or incorrect_pred_points.shape[0] > 0:\n",
        "    plt.legend()\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.axis('scaled')\n",
        "  if dataSet_char == None:\n",
        "    ax.set_title(f'DataSet {CURRENT_SET}')\n",
        "  else:\n",
        "    ax.set_title(f'DataSet {dataSet_char}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMap(model, accuracy=100, specific_color=None,\n",
        "                     useThresholdPredict=False, drawGrid=False, verbose=1):\n",
        "  \"\"\"Plots the prediction certainty of the model for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's preidction certainty is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the certainty map or not.\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy is not\n",
        "      an int.\n",
        "  \"\"\"\n",
        "  # Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    array = np.array([[j/accuracy, i/accuracy] for j in range(accuracy)])\n",
        "    \n",
        "    # Predict points\n",
        "    if useThresholdPredict == True:\n",
        "      result = thresholdPredict(array, model, MIN_GREEN_CERT)\n",
        "    else:\n",
        "      result = model.predict(array)\n",
        "\n",
        "    if specific_color != None:\n",
        "      # Saving the prediction for the specified color\n",
        "      accuracy_map[i] = result[:, specific_color]\n",
        "    \n",
        "    else:\n",
        "      result = result.max(axis=1) # Getting each max value\n",
        "\n",
        "      # Normalize the values which are between 0.5 <-> 1 to 0 <-> 1\n",
        "      accuracy_map[i] = result\n",
        "  \n",
        "    # Print current progress\n",
        "    printProgressBar(i, accuracy-1)\n",
        "\n",
        "  if verbose > 0:\n",
        "    if specific_color != None:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      plt.title(f'Certaintiy for {COLORS[specific_color]}')\n",
        "    else:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      plt.title(f'General Certainty')\n",
        "    \n",
        "    plt.colorbar()\n",
        "    plt.xlabel('x_i1')\n",
        "    plt.ylabel('x_i2')\n",
        "    plt.xlim((0,1))\n",
        "    plt.ylim((0,1))\n",
        "    plt.xticks([i/10 for i in range(11)])\n",
        "    plt.yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      plt.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  return accuracy_map\n",
        "\n",
        "\n",
        "\n",
        "def plotLoss(history):\n",
        "  \"\"\"Plots training loss and validation loss with respect to training epochs.\n",
        "\n",
        "  Args:\n",
        "    history: keras History\n",
        "      history of keras model.\n",
        "  \"\"\"\n",
        "  if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'])\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def showPredictions(model, history, valSet_points, valSet_labels,\n",
        "                    useThresholdPredict=False, showCorrectPoints=False):\n",
        "  \"\"\"Plots the predictions for the validation points.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which performs the predictions.\n",
        "    history: keras History\n",
        "      history of keras model.\n",
        "    valSet_points: 2-D array of shape (x,2)\n",
        "      Data points used for validation.\n",
        "    valSet_labels: 1-D array of shape (x,)\n",
        "      Ground truth labels of the validation points.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    showCorrectPoints: boolean, optional\n",
        "      Whether correctly classified points should be marked as black 'x' or not.\n",
        "\n",
        "  Returns:\n",
        "    2-dimensional numpy array of shape (x,2) with the predictions for the\n",
        "    validation points.\n",
        "  \"\"\"\n",
        "  # Predict the validation points\n",
        "  if useThresholdPredict == True:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identifying correctly and incorrectly classified points\n",
        "  correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == True)\n",
        "  incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  number_of_points = np.bincount(np.argmax(prediction, axis=1))\n",
        "\n",
        "  total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  #Average misclassification certainty\n",
        "  misclass_certainties = []\n",
        "  for i in incorrect_indices[0]:\n",
        "    misclass_certainties.append(np.max(prediction[i]))\n",
        "  avg_misclass_certainty = sum(misclass_certainties)/total_misclassifications\n",
        "  \n",
        "  valAccuracy = 100 - (total_misclassifications/sum(number_of_points))*100\n",
        "\n",
        "  print('Validation accuracy: {:.2f}%'.format(valAccuracy))\n",
        "  print(f'Predictions for green: {number_of_points[0]} / {len(valSet_labels)}')\n",
        "  print(f'Predictions for red: {number_of_points[1]} / {len(valSet_labels)}')\n",
        "  print(f'Points misclassified: {total_misclassifications}')\n",
        "  print(f'Red points misclassified: {red_misclassifications}')\n",
        "  print(f'Green points misclassified: {green_misclassifications}')\n",
        "  print('Average misclassification certainty: {:.2f}'.format(avg_misclass_certainty))\n",
        "\n",
        "  if showCorrectPoints:\n",
        "    makePlot(correct_pred_points=valSet_points[correct_indices],\n",
        "           incorrect_pred_points=valSet_points[incorrect_indices])\n",
        "  else:\n",
        "    makePlot(incorrect_pred_points=valSet_points[incorrect_indices])\n",
        "    \n",
        "  # Make bar graph showing red and green misclassifications\n",
        "  bars = ('Red', 'Green')\n",
        "  height = [red_misclassifications, green_misclassifications]\n",
        "  x_pos = np.arange(len(bars))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(x_pos, height, width=0.35, color=['red', 'green'])\n",
        "\n",
        "  ax.set_ylabel('Misclassifications')\n",
        "  ax.set_title('Misclassifications by color')\n",
        "  ax.set_xticks(x_pos)\n",
        "  ax.set_xticklabels(bars)\n",
        "\n",
        "  rects = ax.patches # Array of bars\n",
        "\n",
        "  labels = [red_misclassifications, green_misclassifications]\n",
        "\n",
        "  for rect, label in zip(rects, labels): # Add labels above bars\n",
        "      height = rect.get_height()\n",
        "      ax.text(rect.get_x() + rect.get_width() / 2, height, label,\n",
        "              ha='center', va='bottom')\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "def makeDensityMap(accuracy, dataSet=CURRENT_SET, significance=0.1,\n",
        "                   cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                   drawGrid=False, verbose=1):\n",
        "  \"\"\"Creates a heatmap of the density of dataSet.\n",
        "\n",
        "    Args:\n",
        "      accuracy: int, optional\n",
        "        Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "        of data points plotted has the dimension accuracy*accuracy.\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      verbose: 0 or 1, optional\n",
        "        Whether to plot the density map or not.\n",
        "\n",
        "    Returns:\n",
        "      2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  if type(dataSet) == str:\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  density_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  printProgressBar(0, accuracy**2)\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      count = dataSet.loc[(dataSet['x_i1'] - j/accuracy)**2 + \n",
        "              (dataSet['x_i2'] - i/accuracy)**2 <= significance**2]\n",
        "      \n",
        "      density_map[i,j] = len(count)\n",
        "\n",
        "      printProgressBar(i*accuracy + j + 1, accuracy**2)\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(density_map),vmax=np.max(density_map))\n",
        "\n",
        "  # Plotting \n",
        "  if verbose > 0:\n",
        "    if specific_color != None:\n",
        "      plt.title(f'Density for {COLORS[specific_color]}')\n",
        "    else:\n",
        "      plt.title(f'General Density')\n",
        "\n",
        "    plt.imshow(density_map, origin='lower', cmap='Spectral', norm=norm,\n",
        "               extent=[0, 1, 0, 1])\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('x_i1')\n",
        "    plt.ylabel('x_i2')\n",
        "    plt.xlim((0,1))\n",
        "    plt.ylim((0,1))\n",
        "    plt.xticks([i/10 for i in range(11)])\n",
        "    plt.yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      plt.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  return density_map\n",
        "  \n",
        "\n",
        "\n",
        "def plotDensity(dataSet=CURRENT_SET, significance=0.1,\n",
        "                cmap=plt.cm.get_cmap('Spectral'), specific_color = None):\n",
        "  \"\"\"Colorises and plots the points of dataSet according to their numbers\n",
        "    of neighbors.\n",
        "\n",
        "    Args:\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "  \"\"\"\n",
        "  if type(dataSet) == str:\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  array = np.zeros((total_number_of_points, 3))\n",
        "\n",
        "  # Counting all neighbours within a radius of significance\n",
        "  for i in range(total_number_of_points):\n",
        "    count = dataSet.loc[(dataSet['x_i1'] - dataSet['x_i1'].loc[i])**2 +\n",
        "     (dataSet['x_i2'] - dataSet['x_i2'].loc[i])**2 <= significance**2]\n",
        "\n",
        "    array[i, 0] = dataSet['x_i1'].loc[i]\n",
        "    array[i, 1] = dataSet['x_i2'].loc[i]\n",
        "    array[i, 2] = len(count)\n",
        "\n",
        "    printProgressBar(i+1, total_number_of_points)\n",
        "\n",
        "  print(f'Max: {np.max(array[:,2])}')\n",
        "  print(f'Min: {np.min(array[:,2])}')\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(array[:,2]),vmax=np.max(array[:,2]))\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.scatter(array[:, 0], array[:, 1], c=array[:, 2], cmap=cmap, norm=norm)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.title(f'Density of DataSet {CURRENT_SET}')\n",
        "  plt.axis('scaled')\n",
        "  plt.show() \n",
        "\n",
        "\n",
        "\n",
        "def makeWeightedCertaintyMap(model, accuracy, significance=0.1,\n",
        "                          useThresholdPredict=False, referenceMethod='even',\n",
        "                          referenceValue=None, drawGrid=False):\n",
        "  \"\"\"Plots the model's prediction certainty weighted with the density of points\n",
        "    given in the dataset.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's weighted prediction certainty is to be plotted.\n",
        "    accuracy: int\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False)\n",
        "      when calculating the model's prediction certainty.\n",
        "    referenceMethod: str: 'even', 'maxDensity', or 'customValue', optional\n",
        "      The method used to calculate the reference density used. 'even'\n",
        "      calculates the density if all points in dataset were evenly spaced. \n",
        "      'maxDensity' uses the maximum density from densityMap as the reference\n",
        "      ´density. 'customValue' uses a custom reference density.\n",
        "    referenceValue: float between 0 and 1, optional\n",
        "      Defines the custom reference density when using 'customValue' reference\n",
        "      method. Leave blank otherwise. \n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if invalid referenceMethod was given.\n",
        "\n",
        "  \"\"\"\n",
        "  if not referenceMethod in ['even', 'evenSqrt', 'evenLog', 'maxDensity',\n",
        "                             'customValue']:\n",
        "   raise TypeError(f'Invalid referenceMethod given. referenceMethod should be' +\n",
        "                   f' \"even\", \"evenSqrt\", \"evenLog\", \"maxDensity\", or ' +\n",
        "                   f'\"customValue\", but \"{referenceMethod}\" was given.')\n",
        "    \n",
        "  dataSet = getDataSet()\n",
        "\n",
        "  print(f'Calculating certainty map:')\n",
        "  certaintyMap = makeCertaintyMap(model, accuracy, None, useThresholdPredict,\n",
        "                                  verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  print(f'Calculating density map:')\n",
        "  densityMap = makeDensityMap(accuracy, significance=significance, verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  if referenceMethod == 'even':\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenSqrt':\n",
        "    densityMap = np.sqrt(densityMap)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenLog':\n",
        "    densityMap = np.log(densityMap+1)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'maxDensity':\n",
        "    maxDensity = np.max(densityMap)\n",
        "    densityMap = densityMap/maxDensity\n",
        "\n",
        "  elif referenceMethod == 'customValue':\n",
        "    densityMap = densityMap/referenceValue\n",
        "\n",
        "  weightedCertaintyMap = certaintyMap*densityMap\n",
        "\n",
        "  plt.imshow(weightedCertaintyMap, origin='lower', cmap='tab20b', vmin=0,\n",
        "             vmax=np.max(weightedCertaintyMap), extent=[0, 1, 0, 1])\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.xticks([i/10 for i in range(11)])\n",
        "  plt.yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    plt.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "  return weightedCertaintyMap\n",
        "\n",
        "\n",
        "\n",
        "def makeDistributionMap(dataSet=CURRENT_SET, accuracy=10, colorbarLim=-1,\n",
        "                        drawGrid=False):\n",
        "  \"\"\"Plots the distribution of dataSet.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'. Dataset who's distribution is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      The distribution map is split up into accuracy*accuracy many fields.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the max. distribution\n",
        "      percentage is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "  \n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = dataSet\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  distribution_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    distribution_map[x_i2,x_i1] = distribution_map[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  distribution_map = distribution_map/len(dataSet)\n",
        "\n",
        "  # Plotting \n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_title(f'Distribution of datset {dataSet_char}')\n",
        "\n",
        "\n",
        "  if colorbarLim == -1:\n",
        "    colorbarLim = np.max(distribution_map)\n",
        "\n",
        "  plt.imshow(distribution_map, origin='lower', cmap='Spectral', vmin=0,\n",
        "             vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "  \n",
        "\n",
        "  plt.colorbar()\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  return distribution_map\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htUB_naf6hZ1",
        "cellView": "form"
      },
      "source": [
        "#@title Penalty Effect\n",
        "\n",
        "def calculatePenaltyEffect(model, x, y, validation_data, interval=(0,1), accuracy=10, \n",
        "                      batch_size=32, epochs=200, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to penalty.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int, optional\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  penalties = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    penalty = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(penalty),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "    history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "    prediction = model.predict(validation_data[0])\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    penalties[i] = penalty\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, iterations+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(penalties, total_misclass_percentages, 'b', penalties, \n",
        "              red_misclass_percentages, 'r', penalties, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by penalty')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Penalty')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averagePenaltyEffect(model, n, valSet_size, path='', interval=(0,1),\n",
        "                         accuracy=10, batch_size=32, epochs=200, verbose=1):\n",
        "  \"\"\"Plots average penalty effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the penalty effect is measured and averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    All others:\n",
        "      See calculatePenaltyEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(penalties), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "    valSet_points, valSet_labels = seperateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculatePenaltyEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating seperate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "         green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotPenaltyEffect(model, data=result, interval=interval, accuracy=accuracy, n=n, \n",
        "                    valSet_size=valSet_size, batch_size=batch_size, epochs=epochs,\n",
        "                    path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}Penalty_Data_{CURRENT_SET}_{model.name}_' +\n",
        "                          f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=penalties).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, penalties,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{interval}', f'{accuracy}', f'{batch_size}', f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotPenaltyEffect(model, data, interval, accuracy, n, valSet_size, batch_size, epochs,\n",
        "                      dataset=CURRENT_SET, ylim=[0,10], maj_yt_incr=1,\n",
        "                      min_yt_incr=0.1, figsize=(14,10), showParameters=True,\n",
        "                      resolution=300, path=''):\n",
        "  \"\"\"Plots average penalty effect given by 'data' and saves png and pdf of plot\n",
        "    to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs:\n",
        "      Parameters used for training and calculaing the average penalty effect.\n",
        "      Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the penalty effect was measured on. 'A', 'B' or 'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Penalties to be plotted on the x-axis\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(penalties, total_misclass_percentages_avg, 'b', penalties, \n",
        "            red_misclass_percentages_avg, 'r', penalties,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by penalty',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Penalty', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(penalties)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpYD2sOxkBjP",
        "cellView": "form"
      },
      "source": [
        "#@title k-Nearest-Neighbour\n",
        "\n",
        "def KNN(dataSet, point, k, significance=0.1, increment=0.05, show_plot=True):\n",
        "  \"\"\" K-nearest neighbor classifier.\n",
        "\n",
        "  Statistical classifier. Uses the k nearest neighbors to predict the color of a\n",
        "  given point by comparing the number of neighbours of each color and weigthing\n",
        "  them with their squared distance to the point.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset to be used for\n",
        "      calculation. Defaults to dataset selected by CURRENT_SET.\n",
        "    point: Array in the form of [x_i1, x_i2]\n",
        "    k: Positive int\n",
        "      Number of neighbours taken into account for classification\n",
        "    significance: float between 0 and 1, optional\n",
        "      Start search radius.\n",
        "    increment: float between 0 and 1, optional\n",
        "      Amount of increment of the search radius while gathering k neighbours.\n",
        "    show_plot: boolean, optional\n",
        "      If 'True' the function plots the dataset and the selected neighbours.\n",
        "\n",
        "  Returns:\n",
        "    A 2-tuple with the predictions for each class. \n",
        "    (prediction_green, prediction_red)\n",
        "  \"\"\"\n",
        "  # Gathering points until at least k neighbours are found \n",
        "  neighb = np.array([])\n",
        "  while significance <= 1 and neighb.shape[0] < k:\n",
        "      neighb = dataSet.loc[(dataSet['x_i1'] - point[0])**2 +\n",
        "                           (dataSet['x_i2'] -point[1])**2 <= significance**2]\n",
        "      significance += increment\n",
        "  \n",
        "  # Reindexing\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "  # Removing all overhang neighbours until there are only k\n",
        "  while neighb.shape[0] > k:\n",
        "    neighb = neighb.drop(np.argmax(dist))\n",
        "    dist[np.argmax(dist)] = -1\n",
        "\n",
        "  # Reindexing\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "\n",
        "  pred_g = 0\n",
        "  pred_r = 0\n",
        "\n",
        "  # Sum the neighbours of each color with the weight 1-dist^2 \n",
        "  for i in range(neighb.shape[0]):\n",
        "    if neighb['l_i'].loc[i] == 0:\n",
        "      pred_g += (1 - dist[i])\n",
        "    elif neighb['l_i'].loc[i] == 1:\n",
        "      pred_r += (1 - dist[i])\n",
        "\n",
        "  # Normalize\n",
        "  pred_g = pred_g / neighb.shape[0]\n",
        "  pred_r = pred_r / neighb.shape[0]\n",
        "\n",
        "  # Plot neighbours \n",
        "  if show_plot:\n",
        "    selected_neighb = [[neighb['x_i1'].loc[i], neighb['x_i2'].loc[i]]\n",
        "                       for i in range(neighb.shape[0])]\n",
        "    makePlot(dataSet, [point], selected_neighb)\n",
        "    print(f'Prediction for green: \\t{pred_g}')\n",
        "    print(f'Prediction for red: \\t{pred_r}')\n",
        "\n",
        "  return (pred_g, pred_r)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMapKNN(k, accuracy = 100, specific_color = None):\n",
        "  \"\"\"Visualizes the prediction certainty of K-nearest-neighbour algorithm for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    k: postive int\n",
        "      The number of neighbours specified for the KNN algorithm who's certainty\n",
        "      is to bevisualized.\n",
        "    accuracy: positive int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid \n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red. \n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy and k\n",
        "    is not an int.\n",
        "  \"\"\"\n",
        "  #Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "    \n",
        "  if not isinstance(k, int):\n",
        "    raise TypeError(f'Invalid type for k. Type is {type(k)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  # Init Data\n",
        "  dataSet = getDataSet()\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Main Loop\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      result = KNN(dataSet, [j/accuracy, i/accuracy], k, show_plot=False)\n",
        "\n",
        "      if specific_color != None:\n",
        "        # Saving the prediction for the specified color\n",
        "        accuracy_map[i,j] = result[specific_color]\n",
        "      else:\n",
        "        accuracy_map[i,j] = np.max(result)\n",
        "    \n",
        "      # Print current progress\n",
        "      printProgressBar((j+1) + i*accuracy, accuracy**2)\n",
        "\n",
        "  # Choosing headline\n",
        "  if specific_color != None:\n",
        "    plt.title(f'Certaintiy for {COLORS[specific_color]}')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1)\n",
        "\n",
        "  else:\n",
        "    plt.title(f'General Certainty')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1)\n",
        "\n",
        "  # Plot\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.xticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.yticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.show()\n",
        "  \n",
        "  return accuracy_map\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tepNATmlkBjy",
        "cellView": "form"
      },
      "source": [
        "#@title Epoch Batch Size\n",
        "\n",
        "def epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    saveAndPlot=True, path='', verbose=1):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number for a random validation set on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which classifies the validation set.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation set.\n",
        "    batchRange: 2-tuple of ints\n",
        "      The range of batch sizes used. (x,y) where x is the smallest and y is the\n",
        "      largest batch size used. \n",
        "    batchIncrements: int\n",
        "      Increment in which the batch size is increased.\n",
        "    epochRange: 2-tuple of ints\n",
        "      The range of epochs used. (x,y) where x is the smallest and y is the\n",
        "      largest epoch number used.\n",
        "    epochIncrements: int\n",
        "      Increment in which the epoch number is increased.\n",
        "    epsilon: float, optional\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "    saveAndPlot: boolean, optional\n",
        "      Whever to save results to Excel and plot graphs or not. Set to false when\n",
        "      using averageEpochsBatchSize.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print a progress bar or not.\n",
        "\n",
        "  Returns:\n",
        "    6-tuple (epochs, batch_sizes, total_misclass_percentage,\n",
        "    red_misclass_percentage, green_misclass_percentage, valSet).\n",
        "    First 5 elements are lists, valSet is pd.DataFrame.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  if verbose > 0:\n",
        "    start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs = []\n",
        "  batch_sizes = []\n",
        "  total_misclass_percentage = []\n",
        "  red_misclass_percentage = []\n",
        "  green_misclass_percentage = []\n",
        "\n",
        "  # Defining iteration lists\n",
        "  batch_size_iter = np.arange(batchRange[0], batchRange[1]+1, batchIncrements)\n",
        "  epoch_iter = np.arange(epochRange[0], epochRange[1]+1, epochIncrements)\n",
        "\n",
        "  if batch_size_iter[0] == 0:\n",
        "    batch_size_iter[0] = 1\n",
        "\n",
        "  # Preparing data\n",
        "  dataSet = getDataSet()\n",
        "  dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "  # Choose random validation set\n",
        "  random.seed(time.time())\n",
        "  val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "  valSet_points, valSet_labels = seperateValidationSet(dataSet,val_indices)\n",
        "  \n",
        "  dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "  training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "  training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "  number_of_points = len(valSet_labels)\n",
        "  red_points = len(np.where(valSet_labels==1)[0])\n",
        "  green_points = len(np.where(valSet_labels==0)[0])\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    num_training_points = training_labels.shape[0]\n",
        "    progress = 0\n",
        "    full = 0\n",
        "    # Calculate full progress\n",
        "    for ep in epoch_iter:\n",
        "      for ba in batch_size_iter:\n",
        "        full += ep*math.ceil(num_training_points/ba)\n",
        "    # Print bar\n",
        "    printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Epoch loop\n",
        "  for ep in epoch_iter:\n",
        "    # Batch size loop\n",
        "    for ba in batch_size_iter:\n",
        "      epochs.append(ep)\n",
        "      batch_sizes.append(ba)\n",
        "\n",
        "      # Prepare model for classification\n",
        "      model.set_weights(initialWeights)\n",
        "\n",
        "      history = model.fit(x=training_points, y=training_labels, batch_size=ba, \n",
        "                          epochs=ep, verbose=0)\n",
        "\n",
        "      # Classification and saving results\n",
        "      prediction = model.predict(valSet_points)\n",
        "\n",
        "      correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                 == True)\n",
        "      incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                   == False)\n",
        "\n",
        "      total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "      red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "      green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "      total_misclass_percentage.append((total_misclassifications/number_of_points)*100)\n",
        "      red_misclass_percentage.append((red_misclassifications/red_points)*100)\n",
        "      green_misclass_percentage.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "      # Update progress bar\n",
        "      if verbose > 0:\n",
        "        progress += ep*math.ceil(num_training_points/ba)\n",
        "        printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "  \n",
        "  # Validation set\n",
        "  valSet = pd.DataFrame.from_dict({'x_i1':valSet_points[:,0],'x_i2':valSet_points[:,1],\n",
        "                                  'l_i':valSet_labels})\n",
        "  \n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}']}\n",
        "\n",
        "  index = ['model','dataset','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  if saveAndPlot==True:\n",
        "    today = date.today()\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}EBS_Data_{CURRENT_SET}_' + \n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "    \n",
        "    # Create multiindex for epochs and batch_sizes\n",
        "    arrays = [epochs,batch_sizes]\n",
        "\n",
        "    tuples = list(zip(*arrays))\n",
        "\n",
        "    multiindex = pd.MultiIndex.from_tuples(tuples,\n",
        "                                      names=[\"epoch\", \"batch_size\"])\n",
        "    \n",
        "    # All data\n",
        "    allData = pd.DataFrame({'total':total_misclass_percentage,\n",
        "                            'red':red_misclass_percentage,\n",
        "                            'green':green_misclass_percentage}, index=multiindex)\n",
        "    \n",
        "    allData.to_excel(writer, sheet_name='All Data')\n",
        "\n",
        "    # Optimum points\n",
        "    result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage)\n",
        "    optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "    optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "    # Parameters\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation set\n",
        "    valSet.to_excel(writer, sheet_name='Validation Set')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage, valSet)\n",
        "  \n",
        "  if saveAndPlot==True:\n",
        "    plotEpochsBatchSize(model, result, path=path)\n",
        "\n",
        "  return result\n",
        "  \n",
        "\n",
        "\n",
        "def calculateOptimumPoints(data, epsilon):\n",
        "  \"\"\"Calculates optimum points of epoch and batch size for total, red, and green\n",
        "    misclassification. \n",
        "\n",
        "  Args:\n",
        "    data: 5-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage) or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    epsilon: float\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "\n",
        "  Returns: pd.DataFrame\n",
        "    columns: [min_misclass, epsilon, opt_misclass, opt_epoch, opt_batch,\n",
        "             t_misclass_here, r_misclass here, g_misclass_here]\n",
        "    rows: [total, red, green]\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not in correct form.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The elements of the 5-tuple data must all have the' +\n",
        "                  f' same shape. The {i+1}. element has shape {data[i].shape}' +\n",
        "                  f' and the {i+2}. element has shape {data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The elements of the 5-tuple data must all be' + \n",
        "                      f' 1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass_percentage = data[2]\n",
        "    red_misclass_percentage = data[3]\n",
        "    green_misclass_percentage = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, 'All Data')\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "\n",
        "    epochs = data['epoch']\n",
        "    batch_sizes = data['batch_size']\n",
        "    total_misclass_percentage = data['total']\n",
        "    red_misclass_percentage = data['red']\n",
        "    green_misclass_percentage = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # CALCULATE OPTIMUM POINTS\n",
        "  # For total: Finds the configuration with total misclass within epsilon of\n",
        "  #   minimum total misclass which has the lowest red misclass.\n",
        "  # For green: Finds the configuration with green misclass within epsilon of\n",
        "  #   minimum green misclass which has the lowest red misclass.\n",
        "  # For red: Finds the configuration with red misclass within epsilon of\n",
        "  #   minimum red misclass which has the lowest total misclass.\n",
        "  columns = ['min_misclass', 'epsilon', 'opt_misclass', 'opt_epoch', 'opt_batch',\n",
        "             't_misclass_here', 'r_misclass here', 'g_misclass_here']\n",
        "  rows = ['total', 'red', 'green']\n",
        "  t_considerable_indices = []\n",
        "  r_considerable_indices = []\n",
        "  g_considerable_indices = []\n",
        "\n",
        "  #Total\n",
        "  t_min = np.min(total_misclass_percentage)\n",
        "  t_opt = np.argmin(total_misclass_percentage)  # Index of optimum point for t\n",
        "  for index in range(len(total_misclass_percentage)):\n",
        "    if total_misclass_percentage[index] <= (t_min+epsilon):\n",
        "      t_considerable_indices.append(index)\n",
        "  for index in t_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[t_opt]:\n",
        "      t_opt = index\n",
        "\n",
        "  #Green\n",
        "  g_min = np.min(green_misclass_percentage)\n",
        "  g_opt = np.argmin(green_misclass_percentage)  # Index of optimum point for g\n",
        "  for index in range(len(green_misclass_percentage)):\n",
        "    if green_misclass_percentage[index] <= (g_min+epsilon):\n",
        "      g_considerable_indices.append(index)\n",
        "  for index in g_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[g_opt]:\n",
        "      g_opt = index\n",
        "\n",
        "  #Red\n",
        "  r_min = np.min(red_misclass_percentage)\n",
        "  r_opt = np.argmin(red_misclass_percentage)  # Index of optimum point for r\n",
        "  for index in range(len(red_misclass_percentage)):\n",
        "    if red_misclass_percentage[index] <= (r_min+epsilon):\n",
        "      r_considerable_indices.append(index)\n",
        "  # Find point with lowest total misclass\n",
        "  # Only change r_opt if the improvement in total misclass is greater than the\n",
        "  #   loss in red misclass\n",
        "  for index in r_considerable_indices:  \n",
        "    if (total_misclass_percentage[index] < total_misclass_percentage[r_opt] and \n",
        "        (total_misclass_percentage[index]-total_misclass_percentage[r_opt] <\n",
        "         red_misclass_percentage[r_opt]-red_misclass_percentage[index])):\n",
        "      r_opt = index\n",
        "  \n",
        "  total_row = [t_min, epsilon, total_misclass_percentage[t_opt], epochs[t_opt],\n",
        "               batch_sizes[t_opt], total_misclass_percentage[t_opt],\n",
        "               red_misclass_percentage[t_opt], green_misclass_percentage[t_opt]]\n",
        "  red_row = [r_min, epsilon, red_misclass_percentage[r_opt], epochs[r_opt],\n",
        "               batch_sizes[r_opt], total_misclass_percentage[r_opt],\n",
        "               red_misclass_percentage[r_opt], green_misclass_percentage[r_opt]]\n",
        "  green_row = [g_min, epsilon, green_misclass_percentage[g_opt], epochs[g_opt],\n",
        "               batch_sizes[g_opt], total_misclass_percentage[g_opt],\n",
        "               red_misclass_percentage[g_opt], green_misclass_percentage[g_opt]]\n",
        "\n",
        "  return pd.DataFrame([total_row, red_row, green_row], index=rows, \n",
        "                      columns=columns)\n",
        "  \n",
        "\n",
        "def averageEpochsBatchSize(model, n, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    path='', verbose=1):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number averaged over n validation sets on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    n: int\n",
        "      Number of iterations.\n",
        "    All others:\n",
        "      See epochsBatchSize.\n",
        "\n",
        "  Returns:\n",
        "    5 tuple of lists (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "    green_misclass_avg).\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs_collected = []\n",
        "  batch_sizes_collected = []\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "\n",
        "  # For saving in excel\n",
        "  validationSets = {}\n",
        "  misclassCollected = {}\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # Collecting misclassification percentages\n",
        "    data = epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                           batchIncrements, epochRange, epochIncrements,\n",
        "                           epsilon=0, saveAndPlot=False, verbose=0)\n",
        "\n",
        "    epochs_collected.append(data[0])\n",
        "    batch_sizes_collected.append(data[1])\n",
        "    total_misclass_collected.append(data[2])\n",
        "    red_misclass_collected.append(data[3])\n",
        "    green_misclass_collected.append(data[4])\n",
        "\n",
        "    # Adding validation set to dictionary for dataframe\n",
        "    validationSets[f'x_i1:{i}'] = data[5]['x_i1']\n",
        "    validationSets[f'x_i2:{i}'] = data[5]['x_i2']\n",
        "    validationSets[f'l_i:{i}'] = data[5]['l_i']\n",
        "\n",
        "    # Adding misclassification data to dictionary for dataframe\n",
        "    misclassCollected[f'total:{i}'] = data[2]\n",
        "    misclassCollected[f'red:{i}'] = data[3]\n",
        "    misclassCollected[f'green{i}'] = data[4]\n",
        "\n",
        "    # Update progress bar\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  epochs = np.average(epochs_collected, axis=0)\n",
        "  batch_sizes = np.average(batch_sizes_collected, axis=0)\n",
        "  total_misclass_avg = np.average(total_misclass_collected, axis=0)\n",
        "  red_misclass_avg = np.average(red_misclass_collected, axis=0)\n",
        "  green_misclass_avg = np.average(green_misclass_collected, axis=0)\n",
        "\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  today = date.today()\n",
        "\n",
        "  # Create multiindex for epochs and batch_sizes\n",
        "  arrays = [epochs,batch_sizes]\n",
        "\n",
        "  tuples = list(zip(*arrays))\n",
        "\n",
        "  multiindex = pd.MultiIndex.from_tuples(tuples, names=[\"epoch\", \"batch_size\"])\n",
        "\n",
        "  # Initialize writer\n",
        "  writer = pd.ExcelWriter(f'{path}Avg_EBS_Data_{CURRENT_SET}_' + \n",
        "                        f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "  # Average \n",
        "  average = pd.DataFrame({'total':total_misclass_avg,\n",
        "                          'red':red_misclass_avg,\n",
        "                          'green':green_misclass_avg}, index=multiindex)\n",
        "\n",
        "  average.to_excel(writer, sheet_name='Average')\n",
        "\n",
        "  # Collected\n",
        "  misclassCollected = pd.DataFrame(misclassCollected, index=multiindex)\n",
        "  misclassCollected.to_excel(writer, sheet_name='Collected')\n",
        "\n",
        "  # Optimum points\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "  optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "  parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation set\n",
        "  validationSets = pd.DataFrame.from_dict(validationSets)\n",
        "  validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  \n",
        "  plotEpochsBatchSize(model, result, path=path, prefix='Avg_')\n",
        "  \n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotEpochsBatchSize(model, data, dataset=CURRENT_SET,\n",
        "                        misclass_range=(0,15), figsize=(14,10), resolution=300,\n",
        "                        cmap='viridis', path='', prefix=''):\n",
        "  \"\"\"Plots a 3D graph showing the relation between epoch number, batch size,\n",
        "    and percentage misclassification.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model which was used for training and classification.\n",
        "    data: 6-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage, valSet) or the name of an Excel\n",
        "      sheet present in the directory as a String (e.g. 'data.xlsx').\n",
        "    dataset: char, optional\n",
        "      The dataset used. 'A', 'B' or 'C'.\n",
        "    misclass_range: 2-tuple, optional\n",
        "      The range of misclassification percentages plotted (limits of the z-axis).\n",
        "    figsize: 2-tuple, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    cmap: Colormap, optional\n",
        "      A colormap for the surface patches.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    prefix: str, optional\n",
        "      appended to the front of the pnd and pdf file names\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is of invalid type or shape.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The first 5 elements of the tuple data must all ' +\n",
        "                  f'have the same shape. The {i+1}. element has shape ' +\n",
        "                  f'{data[i].shape} and the {i+2}. element has shape ' +\n",
        "                  f'{data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The first 5 elements of the tuple data must all be ' +\n",
        "                      f'1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass = data[2]\n",
        "    red_misclass = data[3]\n",
        "    green_misclass = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, sheet_name=0, index_col=[0,1])\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "    \n",
        "    index_list = list(data.index)\n",
        "    index_len = len(index_list)\n",
        "    \n",
        "    epochs = []\n",
        "    batch_sizes = []\n",
        "    for i in range(index_len):\n",
        "      epochs.append(index_list[i][0])\n",
        "      batch_sizes.append(index_list[i][1])\n",
        "\n",
        "    total_misclass = data['total']\n",
        "    red_misclass = data['red']\n",
        "    green_misclass = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # Plotting\n",
        "  fig = plt.figure(figsize=(figsize[0], figsize[1]*3))\n",
        "  # Total misclassification\n",
        "  ax_t = fig.add_subplot(3, 1, 1, projection='3d')\n",
        "  ax_t.plot_trisurf(epochs, batch_sizes, total_misclass, cmap=cmap)\n",
        "  ax_t.set_title(f'Dataset {dataset}: Total misclassification by epoch and batch size')\n",
        "  ax_t.set_xlabel('Epochs')\n",
        "  ax_t.set_ylabel('Batch size')\n",
        "  ax_t.set_zlabel('% misclassification')\n",
        "  ax_t.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Red misclassification\n",
        "  ax_r = fig.add_subplot(3, 1, 2, projection='3d')\n",
        "  ax_r.plot_trisurf(epochs, batch_sizes, red_misclass, cmap=cmap)\n",
        "  ax_r.set_title(f'Dataset {dataset}: Red misclassification by epoch and batch size')\n",
        "  ax_r.set_xlabel('Epochs')\n",
        "  ax_r.set_ylabel('Batch size')\n",
        "  ax_r.set_zlabel('% misclassification')\n",
        "  ax_r.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Green misclassification\n",
        "  ax_g = fig.add_subplot(3, 1, 3, projection='3d')\n",
        "  ax_g.plot_trisurf(epochs, batch_sizes, green_misclass, cmap=cmap)\n",
        "  ax_g.set_title(f'Dataset {dataset}: Green misclassification by epoch and batch size')\n",
        "  ax_g.set_xlabel('Epochs')\n",
        "  ax_g.set_ylabel('Batch size')\n",
        "  ax_g.set_zlabel('% misclassification')\n",
        "  ax_g.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdlLeaiYaqo",
        "cellView": "form"
      },
      "source": [
        "#@title Different Training Approaches\n",
        "def diffPenaltyAproach(n, val_size, model, penalty, epochs, batch_size, increment, epoch_end_of_inc, verbose=0, figsize=(14,10), path=''):\n",
        "\n",
        "  dataSet_original = getDataSet()\n",
        "  valSets = [getBalancedValSetIndices(dataSet_original, val_size, THRESHOLD_VAL) for i in range(n)]\n",
        "\n",
        "  history_1 = np.zeros((n,3))\n",
        "  history_2 = np.zeros((n,3))\n",
        "  history_3 = np.zeros((n,3))\n",
        "\n",
        "  printProgressBar(0, 3*n)\n",
        "\n",
        "  model.set_weights(initialWeights)\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = seperateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels)\n",
        "    \n",
        "    history_1[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i+1, 3*n)\n",
        "\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    val_data = seperateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    model.fit(training_points, training_labels, epochs=epochs,\n",
        "                                batch_size=batch_size, shuffle=True, verbose=0)\n",
        "\n",
        "    history_2[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + n+1, 3*n)\n",
        "  \n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = seperateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=False)\n",
        "    \n",
        "    history_3[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + 2*n+1, 3*n)\n",
        "  \n",
        "  clear_output()\n",
        "\n",
        "  labels = ['total', 'red', 'green']\n",
        "  y_1 = [i/n for i in np.sum(history_1, axis=0)]\n",
        "  y_2 = [i/n for i in np.sum(history_2, axis=0)]\n",
        "  y_3 = [i/n for i in np.sum(history_3, axis=0)]\n",
        "\n",
        "  x = np.arange(len(labels))  # the label locations\n",
        "  width = 0.2  # the width of the bars\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  rects1 = ax.bar(x - width, y_1, width, label='With Increment')\n",
        "  rects2 = ax.bar(x, y_2, width, label='Normal')\n",
        "  rects3 = ax.bar(x + width, y_3, width, label='With Decrement')\n",
        "\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_ylabel('Misclassification in %')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "\n",
        "  fig.text(0,0, f'Dataset: {CURRENT_SET}, Epochs: {epochs}, Batch Size: {batch_size}, Epoch Increment: {increment}, Epoch end of Increment: {epoch_end_of_inc}')\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Comparison{CURRENT_SET}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bljczC5kBj7",
        "cellView": "form"
      },
      "source": [
        "#@title Custom Loss Function\n",
        "\n",
        "def construct_custom_penalty_loss(penalty,\n",
        "                                  lossFunction=keras.losses.sparse_categorical_crossentropy):\n",
        "  \"\"\"Constructs a loss function which penalizes 'red as green' misclassifications. \n",
        "\n",
        "  Args:\n",
        "    penalty: float between 0 and 1\n",
        "      Value added to the loss if a red point is misclassified as green. \n",
        "    lossFunction: loss function, optional\n",
        "      The loss function used after adapting the loss values.\n",
        "\n",
        "  Returns:\n",
        "    custom_penalty_loss function with specified penalty and loss function. Can be\n",
        "    used like a regular loss function. \n",
        "  \"\"\"\n",
        "\n",
        "  def custom_penalty_loss(y_true, y_pred):\n",
        "    length = tf.shape(y_true)[0]\n",
        "\n",
        "    #Creating a vector with all values set to the penalty: [0.3, 0.3, ... 0.3]\n",
        "    error = tf.multiply(tf.constant(penalty, tf.float32), tf.ones(length)) \n",
        "\n",
        "    #Setting every entry to 0 if the corresponding entry in y_true is 1\n",
        "    error = tf.where(tf.equal(y_true[:, 0], tf.zeros(length)), error, tf.zeros(length))\n",
        "\n",
        "    #Setting every entry to 0 if the algorithm predicted 0\n",
        "    error = tf.where(tf.greater(y_pred[:, 0], y_pred[:, 1]), tf.zeros(length), error)\n",
        "\n",
        "    #Transforms the vector from [0, 0, 0.3, ... 0,3] to [[0, -0], [0, -0], [0.3, -0.3], ... [0.3, -0.3]]\n",
        "    error = tf.stack([error, tf.multiply(tf.constant(-1, tf.float32), error)], 1)\n",
        "\n",
        "    #Adding the artificial loss\n",
        "    y_pred = y_pred + error\n",
        "\n",
        "    #Eliminating values > 1 or < 0\n",
        "    y_pred0 = tf.where(tf.greater(y_pred[:, 0], tf.ones(length)), tf.ones(length), y_pred[:, 0])\n",
        "    y_pred1 = tf.where(tf.greater(y_pred[:, 1], tf.zeros(length)), y_pred[:, 1], tf.zeros(length))\n",
        "    y_pred = tf.stack([y_pred0, y_pred1], axis=1)\n",
        "\n",
        "\n",
        "    loss = lossFunction(y_pred=y_pred, y_true=y_true)\n",
        "    return loss\n",
        "  \n",
        "  return custom_penalty_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_01aKYUEy82",
        "cellView": "form"
      },
      "source": [
        "#@title Certainty Threshold Effect\n",
        "\n",
        "def calculateThresholdEffect(model, x, y, validation_data, interval=(0.8,1),\n",
        "                             accuracy=10, batch_size=64, epochs=500, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to the\n",
        "    certainty threshold for predicting points as green.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the threshold interval plotted. x is the lowest\n",
        "      threshold, y the highest.\n",
        "    accuracy: int, optional\n",
        "      Threshold interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  thresholds = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  # Initialize and fit model\n",
        "  model.set_weights(initialWeights)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "  history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    threshold = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "\n",
        "    prediction = thresholdPredict(validation_data[0], model, threshold)\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction,axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    thresholds[i] = threshold\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, accuracy+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(thresholds, total_misclass_percentages, 'b', thresholds, \n",
        "              red_misclass_percentages, 'r', thresholds, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by certainty threshold')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Certainty threshold')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averageThresholdEffect(model, n, valSet_size, path='', interval=(0.8,1),\n",
        "                         accuracy=10, batch_size=64, epochs=500, verbose=1):\n",
        "  \"\"\"Plots average certainty threshold effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the certainty threshold effect is measured and\n",
        "      averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    All others:\n",
        "      See calculateThresholdEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(thresholds), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = random.sample(range(SOURCE_SIZE[CURRENT_SET]), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = seperateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculateThresholdEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating seperate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "            green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotThresholdEffect(model, data=result, interval=interval, accuracy=accuracy,\n",
        "                      n=n, valSet_size=valSet_size, batch_size=batch_size,\n",
        "                      epochs=epochs, path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}CertaintyThreshold_Data_{CURRENT_SET}_' +\n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=thresholds).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, thresholds,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{interval}', f'{accuracy}', f'{batch_size}',\n",
        "                    f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotThresholdEffect(model, data, interval, accuracy, n, valSet_size,\n",
        "                      batch_size, epochs, penalty=PENALTY, dataset=CURRENT_SET,\n",
        "                      ylim=[0,10], maj_yt_incr=1, min_yt_incr=0.1,\n",
        "                      figsize=(14,10), showParameters=True, resolution=300,\n",
        "                      path=''):\n",
        "  \"\"\"Plots average certainty threshold effect given by 'data' and saves png and\n",
        "    pdf of plot to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the certainty threshold interval plotted. x is the\n",
        "      lowest penalty, y the highest.\n",
        "    accuracy: int\n",
        "      Certainty threshold interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs, penalty:\n",
        "      Parameters used for training and calculaing the average certainty\n",
        "      threshold effect. Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the certainty penalty effect was measured on. 'A', 'B' or\n",
        "      'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Thresholds to be plotted on the x-axis\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(thresholds, total_misclass_percentages_avg, 'b', thresholds, \n",
        "            red_misclass_percentages_avg, 'r', thresholds,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by certainty threshold',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Certainty threshold', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(thresholds)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}\\n' +\n",
        "                   f'Penalty: {penalty}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVX3rT2aStCu",
        "cellView": "form"
      },
      "source": [
        "#@title Points Per Square\n",
        "def pointsPerSquare(dataSet=CURRENT_SET, accuracy=100):\n",
        "  \"\"\"Calculates the number of points from dataSet present in each square of an\n",
        "    accuracy x accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'.\n",
        "    accuracy: int, optional\n",
        "      The grid consists of accuracy x accuracy many squares.\n",
        "\n",
        "  Returns:\n",
        "    Three 2-D np.array of the shape (accuracy,accuracy): squares, red and green.\n",
        "    squares contains the number of total points present in each square, red\n",
        "    contains the number of red points present in each square, and green contains\n",
        "    the number of green points present in each square.\n",
        "  \"\"\"\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  squares = np.zeros((accuracy, accuracy))\n",
        "  red = np.zeros((accuracy, accuracy))\n",
        "  green = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    squares[x_i2,x_i1] = squares[x_i2,x_i1]+1\n",
        "\n",
        "    if (dataSet.loc[i]['l_i']) == 0:\n",
        "      green[x_i2,x_i1] = green[x_i2,x_i1]+1\n",
        "    else:\n",
        "      red[x_i2,x_i1] = red[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  return squares, red, green"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3YU__k-8CfW"
      },
      "source": [
        "# Magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR2BPwsfkBj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef684df9-2b6d-4c0d-af08-388735171483"
      },
      "source": [
        "# Data Preparation\n",
        "dataSet = getDataSet()\n",
        "dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "valSet_points, valSet_labels = seperateValidationSet(dataSet=dataSet, validationIndices=VAL_INDICES)\n",
        "\n",
        "dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA)\n",
        "\n",
        "#Creating tensors\n",
        "training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "training_points = np.array(dataSet[['x_i1','x_i2']])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificially exended by 106 points\n",
            "Relation is now: 0.6 green : 0.4 red \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuey_TJrkBj8"
      },
      "source": [
        "# Configure and compile model\n",
        "initalizer = keras.initializers.GlorotNormal()\n",
        "\n",
        "model_0 = keras.Sequential([\n",
        "          keras.layers.Flatten(input_shape=(2,)),      #input layer: 2 neurons\n",
        "          keras.layers.Dense(100,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(70,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(50,activation='relu', kernel_initializer=initalizer),       \n",
        "          keras.layers.Dense(10,activation='relu', kernel_initializer=initalizer),\n",
        "          keras.layers.Dense(2,activation='softmax', kernel_initializer=initalizer)   #output layer: 2 neurons              \n",
        "          ], name=\"model_0\")\n",
        "\n",
        "model_0.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Save initial weights\n",
        "initialWeights = model_0.get_weights()\n",
        "\n",
        "# Fit model\n",
        "history = model_0.fit(training_points, training_labels, batch_size=32, epochs=1500,\n",
        "                    shuffle=True, validation_data=(valSet_points, valSet_labels))\n",
        "clear_output()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXag1CSehWAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "2efbf53a-3433-4711-cdb6-5c2d55627cff"
      },
      "source": [
        "x = makeDistributionMap('A', 10, -1, drawGrid=True)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MucCAvLupbau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3d822e-e4b2-4de0-ff52-7abfee1ba126"
      },
      "source": [
        "t,r,g = pointsPerSquare('A', 10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-7Ts-mRHVEE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "0eec7495-c0a8-43c6-d017-1c65a812afed"
      },
      "source": [
        "res = makeDensityMap(20, drawGrid=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}