{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siemens_Approach.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbill21/siemens/blob/master/Siemens_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppIOSk-I8wV2"
      },
      "source": [
        "# Imports, Config & GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeg6hj5kBjI",
        "outputId": "c8b2be3e-4cb8-47f7-f3f4-5e123594fbfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Arithmetic Operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Data visualization\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Progress calculation\n",
        "import sys\n",
        "import time\n",
        "from datetime import date\n",
        "\n",
        "# Time prediciton\n",
        "PREV_TIME = 0\n",
        "PB_START_TIME = 0\n",
        "\n",
        "# Mounting Google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd5eTUqAkBjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1b3281-c7ac-4af5-978c-02375b1832c8"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# ram_gb = virtual_memory().total / 1e9\n",
        "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "# if ram_gb < 20:\n",
        "#   print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "#   print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "#   print('re-execute this cell.')\n",
        "# else:\n",
        "#   print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 28 19:10:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    42W / 300W |    553MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjZxd7_8MKI"
      },
      "source": [
        "# Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRjXSfIYkBjM"
      },
      "source": [
        "# Dictionaries\n",
        "COLORS = {0 : 'green', 1 : 'red', 'green' : 0, 'red' : 1}\n",
        "SOURCES = {'A' : 'https://drive.google.com/file/d/1hAzAKZNpmSclSI7HnV_cRjpMS4Kh5r1q/view?usp=sharing', 'B' : 'https://drive.google.com/file/d/12VlecL-5iYs-BFpnTOba1x65jWofBX1P/view?usp=sharing', 'C' : 'https://drive.google.com/file/d/1-Z0RuJIi1cZcqrrmV6TqT0O1PwI2OiBY/view?usp=sharing'}\n",
        "SOURCE_SIZE = {'A': 1000,'B' : 5000, 'C' : 50000}\n",
        "\n",
        "CURRENT_SET = 'C'\n",
        "# Initialize current dataset as empty dataframe\n",
        "DATASET = pd.DataFrame()\n",
        "# Dataset previously used\n",
        "PREV_SET = None\n",
        "\n",
        "# Balancing dataset to threshold\n",
        "THRESHOLD_DATA = 0.3\n",
        "\n",
        "# Threshold for balanced validation set\n",
        "THRESHOLD_VAL = 0.4\n",
        "\n",
        "# Minimum certainty required to predict green\n",
        "MIN_GREEN_CERT = 0.9\n",
        "\n",
        "# Random number seed\n",
        "random.seed(time.time())\n",
        "\n",
        "subsetA = random.sample(range(1000), 150)\n",
        "subsetB = random.sample(range(5000), 800)\n",
        "subsetC = random.sample(range(50000), 8000)\n",
        "\n",
        "VAL_INDICES = locals()['subset' + CURRENT_SET]\n",
        "\n",
        "# Penalties applied to false green classifications in custom loss function\n",
        "PENALTIES = {'A' : 0.2, 'B' : 0.25, 'C' : 0.2}\n",
        "PENALTY = PENALTIES[CURRENT_SET]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVhVxxhc5T-a"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC174oUT5utW",
        "cellView": "form"
      },
      "source": [
        "#@title Data Operations\n",
        "\n",
        "def getDataSet(dataset=None):\n",
        "  \"\"\"Returns pandas.DataFrame of dataset.\n",
        "  \n",
        "  Args:\n",
        "    dataset: char, optional\n",
        "      The dataset to return. 'A', 'B', or 'C'.\n",
        "  \"\"\"\n",
        "  global DATASET\n",
        "  global PREV_SET\n",
        "\n",
        "  if dataset == None:\n",
        "    dataset = CURRENT_SET\n",
        "\n",
        "  try:\n",
        "    if DATASET.empty:\n",
        "      path = 'https://drive.google.com/uc?export=download&id='+SOURCES[dataset].split('/')[-2]\n",
        "      DATASET = pd.read_excel(path)\n",
        "      PREV_SET = dataset\n",
        "\n",
        "    if dataset != PREV_SET:\n",
        "      path = 'https://drive.google.com/uc?export=download&id='+SOURCES[dataset].split('/')[-2]\n",
        "      DATASET = pd.read_excel(path)\n",
        "      PREV_SET = dataset\n",
        "  \n",
        "  except:\n",
        "    print('Exception in getDataSet occured')\n",
        "    print('Going to sleep for 2 minutes and trying again')\n",
        "    time.sleep(120)\n",
        "    DATASET = getDataSet(dataSet)\n",
        "  \n",
        "  return DATASET.copy()\n",
        "\n",
        "def separateValidationSet(dataSet, validationIndices):\n",
        "  \"\"\"Separates a subset of points from dataSet as validation points.\n",
        "\n",
        "  Validation points are extracted and deleted from dataSet to be used for\n",
        "  validation later on.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset which the\n",
        "      validation points are extracted from.\n",
        "    validationIndices: 1-D list of ints\n",
        "      The elements corresponding to these indices are extracted from dataSet.\n",
        "\n",
        "  Returns:\n",
        "    2-tuple of the form (valSet_points, valSet_labels), where valSet_points\n",
        "    is a np.array of shape (x,2) and valSet_labels is a np.array of shape (x,1).\n",
        "  \"\"\"\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be \\\n",
        "      {pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if len(np.array(validationIndices).shape) != 1:\n",
        "    raise TypeError(f'The shape of the parameter validationIndices is: \\\n",
        "      {np.array(validationIndices).shape}, but it should be 1 dimensional')\n",
        "  \n",
        "  valSet_points = dataSet[['x_i1','x_i2']].loc[validationIndices]\n",
        "  valSet_labels = dataSet['l_i'].loc[validationIndices]\n",
        "  \n",
        "  # Saving the validation points\n",
        "  valSet_points = np.array(valSet_points)\n",
        "  valSet_labels = np.array(valSet_labels).astype('float')\n",
        "\n",
        "  # Removing the validation point\n",
        "  dataSet.drop(index=validationIndices, inplace=True)\n",
        "  dataSet.reset_index(inplace=True)\n",
        "\n",
        "  return (valSet_points, valSet_labels)\n",
        "\n",
        "def timeCalc():\n",
        "  \"\"\"Calculates time between previous call and current call.\n",
        "\n",
        "  Returns:\n",
        "    Time difference in minutes as float.\n",
        "  \"\"\"\n",
        "  global PREV_TIME\n",
        "  if PREV_TIME == 0:\n",
        "    PREV_TIME = time.time()\n",
        "    return 0\n",
        "  \n",
        "  res = (time.time() - PREV_TIME) / 60\n",
        "  PREV_TIME = time.time()\n",
        "  return res\n",
        "\n",
        "def balanceDataset(dataSet, threshold, verbose=1):\n",
        "  \"\"\"Artificially balances dataSet by duplicating red or green points.\n",
        "\n",
        "  Args: \n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. The datset to be balanced.\n",
        "    threshold: float between 0 and 0.5\n",
        "      The function duplicates red or green points until the fraction of points\n",
        "      of the less frequent color is at least equal to the threshold.\n",
        "\n",
        "  Returns:\n",
        "    pandas.DataFrame with columns 'x_i1', 'x_i2', 'l_i1'.\n",
        "  \"\"\"\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  amount = 0\n",
        "\n",
        "  if number_of_red_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_red_points) // (1 - threshold) )\n",
        "    red_points = dataSet.loc[dataSet['l_i'] == 1] #Getting all red points\n",
        "    chosen_points = red_points.sample(amount, replace=True) #Selecting a random subset of red points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending the subset\n",
        "\n",
        "  if number_of_green_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_green_points) // (1 - threshold) )\n",
        "    green_points = dataSet.loc[dataSet['l_i'] == 0] #Getting all green points\n",
        "    chosen_points = green_points.sample(amount, replace=True) #Selecting a random subset of green points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending green subset\n",
        "\n",
        "  dataSet = dataSet[['x_i1','x_i2','l_i']]\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  if verbose > 0:\n",
        "    print(f'Artificially exended by {amount} points')\n",
        "    print(f'Relation is now: {round(number_of_green_points / total_number_of_points, 2)}',\n",
        "            f'green : {round(number_of_red_points / total_number_of_points, 2)} red ')\n",
        "  \n",
        "  return dataSet\n",
        "\n",
        "def getBalancedValSetIndices(dataSet, size, threshold):\n",
        "  \"\"\"Get indices of validation points such that neither color represents\n",
        "    less than (threshold*100)% of the validation set.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset from\n",
        "      which the validation points are to be chosen.\n",
        "    size: int\n",
        "      Size of the validation set.\n",
        "    threshold: float between 0 and 1\n",
        "      Fraction of validation points which each color must at least\n",
        "      represent.\n",
        "\n",
        "  Returns:\n",
        "    1-D array of ints (indices).\n",
        "  \"\"\"\n",
        "  random.seed(time.time())\n",
        "\n",
        "  # Amount of points for each color\n",
        "  amount_g = int(random.randint(size*threshold, size*(1-threshold)))\n",
        "  amount_r = size - amount_g\n",
        "\n",
        "  # Indices of each points with the specific color\n",
        "  indices_g = np.where(dataSet['l_i'] == 0)[0]\n",
        "  indices_r = np.where(dataSet['l_i'] == 1)[0]\n",
        "\n",
        "  # Check if possible \n",
        "  if indices_g.shape[0] + indices_r.shape[0] < size:\n",
        "    raise ValueError('The requested size of the validation set is not feasible')\n",
        "\n",
        "  if indices_r.shape[0] < amount_r:\n",
        "    indices_g += amount_r - indices_r.shape[0]\n",
        "\n",
        "  if indices_g.shape[0] < amount_g:\n",
        "    indices_r += amount_g - indices_g.shape[0]\n",
        "  \n",
        "  # Randomly selceting a subset for each color\n",
        "  indices_g = np.random.choice(indices_g, amount_g)\n",
        "  indices_r = np.random.choice(indices_r, amount_r)\n",
        "\n",
        "  # Concatenate and shuffle the chosen subsets\n",
        "  indices = np.concatenate([indices_g, indices_r])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  return indices\n",
        "\n",
        "def getProportionOfMisclassification(model, val_data):\n",
        "\n",
        "  # Creating Numpy arrays from tensors\n",
        "  points = val_data[0]\n",
        "  labels = val_data[1].astype('float')\n",
        "\n",
        "  # Counting number of points for each class\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  prediction = model.predict(val_data[0])\n",
        "\n",
        "  # Determining the incorrect predictions\n",
        "  incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  # Counting the number of misclassifications\n",
        "  total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  return ((total_misclassifications/number_of_points)*100,\n",
        "          (red_misclassifications/red_points)*100,\n",
        "          (green_misclassifications/green_points)*100)\n",
        "\n",
        "\n",
        "def penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=True, verbose=0):\n",
        "\n",
        "  if increasing:\n",
        "    array_penalties = np.linspace(0, penalty, (epochs - epoch_end_of_inc) // increment)\n",
        "  else:\n",
        "    array_penalties = np.linspace(penalty, 0, (epochs - epoch_end_of_inc) // increment)\n",
        "\n",
        "  for i in range((epochs - epoch_end_of_inc) // increment):\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(array_penalties[i]), metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(training_points, training_labels, batch_size=batch_size, epochs=increment,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "\n",
        "  model.fit(training_points, training_labels, batch_size=batch_size, epochs=epochs - epoch_end_of_inc,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "  \n",
        "\n",
        "def thresholdPredict(data, model, threshold):\n",
        "  \"\"\"Generates output predictions for the input samples. Points are only\n",
        "    predicted as green if the model's certainty for green is > threshold. All \n",
        "    other points are predicted red.\n",
        "\n",
        "  Args:\n",
        "    data: array-like, tensors, tf.data dataset...\n",
        "      Input samples.\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    threshold: float between 0.5 and 1\n",
        "      The minimum certainty required for the network to predict a point as green.\n",
        "\n",
        "  Returns:\n",
        "    Numpy array(s) of predictions.\n",
        "  \"\"\"\n",
        "  prediction = model.predict(data)\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i,0] >= 0.5 and prediction[i,0] < threshold:\n",
        "      temp = prediction[i,0]\n",
        "      prediction[i,0] = prediction[i,1]\n",
        "      prediction[i,1] = temp\n",
        "\n",
        "  return prediction\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woke-yOw50gQ",
        "cellView": "form"
      },
      "source": [
        "#@title Visualisation\n",
        "\n",
        "def printProgressBar(iteration, total, prefix = '', suffix = '', decimals = 1,\n",
        "                     length = 100, fill = '█'):\n",
        "  \"\"\"Prints a progress bar.\n",
        "\n",
        "  Args:\n",
        "    iteration: int\n",
        "      Current progress step as. (iteration/total progress).\n",
        "    total: int\n",
        "      Total progress steps until completion.\n",
        "    prefix: str, optional\n",
        "      Printed infront of the progress bar.\n",
        "    suffix: str, optional\n",
        "      Printed behind ETA.\n",
        "    decimals: int, optional\n",
        "      Number of decimal places of percentage progress.\n",
        "    length: int, optional\n",
        "      Length of the progress bar in characters.\n",
        "    fill: char, optional\n",
        "      Filler of the progress bar.\n",
        "  \"\"\"\n",
        "  # Preparing strings\n",
        "  percentage_progress = (100*(iteration/float(total)))\n",
        "  percent = (\"{0:.\" + str(decimals) + \"f}\").format(percentage_progress)\n",
        "  filledLength = int(length * iteration // total)\n",
        "  bar = fill * filledLength + '-' * (length - filledLength)\n",
        "\n",
        "  # Bob's alternative time calculation\n",
        "  if iteration == 0:\n",
        "    global PB_START_TIME\n",
        "    PB_START_TIME = time.time()\n",
        "    time_so_far = 0\n",
        "    time_remaining = 0\n",
        "  else:\n",
        "    time_so_far = time.time() - PB_START_TIME\n",
        "    time_remaining = time_so_far/percentage_progress * (100-percentage_progress)\n",
        "\n",
        "  sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% | ETA: {round((time_remaining/60), 2)} minutes | {suffix}')\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Erease progress bar on complete\n",
        "  if iteration == total:\n",
        "    global PREV_TIME\n",
        "    PREV_TIME = 0\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def makePlot(dataSet=CURRENT_SET, correct_pred_points = np.array([]),\n",
        "             incorrect_pred_points = np.array([]), drawGrid=True,\n",
        "             savePlot=False, path=''):\n",
        "  \"\"\"\"Plots green and red points and markers as scatter graph.\n",
        "  \n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame or char, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "      Dataset to be plotted.\n",
        "    correct_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing correctly predicted points. Marked as black\n",
        "      'x' on scatter graph.\n",
        "    incorrect_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing incorrectly predicted points. Marked as\n",
        "      black '*' on scatter graph.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Raises:\n",
        "    TypeError: If dataSet is not an instance of pd.DataFrame or char or the\n",
        "    other parameters do not have the required shape.\n",
        "  \"\"\"\n",
        "  # Preparing optional parameters\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if isinstance(correct_pred_points, list):\n",
        "    correct_pred_points = np.array(correct_pred_points)\n",
        "  if isinstance(incorrect_pred_points, list):\n",
        "    incorrect_pred_points = np.array(incorrect_pred_points)\n",
        "\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be ' +\n",
        "                    f'{pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if (correct_pred_points.shape != (correct_pred_points.shape[0],2)\n",
        "      and np.array(correct_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter correct_pred_points is: \\\n",
        "      {np.array(correct_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  if (incorrect_pred_points.shape != (incorrect_pred_points.shape[0],2)\n",
        "      and np.array(incorrect_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter incorrect_pred_points is: \\\n",
        "      {np.array(incorrect_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  # Creating a subplot\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Scattering all points\n",
        "  x = dataSet['x_i1']\n",
        "  y = dataSet['x_i2']\n",
        "  c = [COLORS[i] for i in dataSet['l_i']] \n",
        "\n",
        "  ax.scatter(x, y, c=c)\n",
        "\n",
        "  # Adding markers to the specified points\n",
        "  if correct_pred_points.shape[0] > 0:\n",
        "    ax.scatter(correct_pred_points[:, 0], correct_pred_points[:, 1],\n",
        "              marker = \"x\", c = 'black', label='correct')\n",
        "  if incorrect_pred_points.shape[0] > 0:\n",
        "    ax.scatter(incorrect_pred_points[:, 0], incorrect_pred_points[:, 1],\n",
        "            marker = \"*\", c = 'black', label='incorrect')\n",
        "\n",
        "  if correct_pred_points.shape[0] > 0 or incorrect_pred_points.shape[0] > 0:\n",
        "    plt.legend()\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.axis('scaled')\n",
        "  ax.set_title(f'Dataset {dataSet_char}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMap(model, accuracy=100, specific_color=None,\n",
        "                     useThresholdPredict=False, drawGrid=True, verbose=1,\n",
        "                     savePlot=False, path=''):\n",
        "  \"\"\"Plots the prediction certainty of the model for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's preidction certainty is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the certainty map or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy is not\n",
        "      an int.\n",
        "  \"\"\"\n",
        "  # Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    array = np.array([[j/accuracy, i/accuracy] for j in range(accuracy)])\n",
        "    \n",
        "    # Predict points\n",
        "    if useThresholdPredict == True:\n",
        "      result = thresholdPredict(array, model, MIN_GREEN_CERT)\n",
        "    else:\n",
        "      result = model.predict(array)\n",
        "\n",
        "    if specific_color != None:\n",
        "      # Saving the prediction for the specified color\n",
        "      accuracy_map[i] = result[:, specific_color]\n",
        "    \n",
        "    else:\n",
        "      result = result.max(axis=1) # Getting each max value\n",
        "\n",
        "      # Normalize the values which are between 0.5 <-> 1 to 0 <-> 1\n",
        "      accuracy_map[i] = result\n",
        "  \n",
        "    # Print current progress\n",
        "    printProgressBar(i, accuracy-1)\n",
        "\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if specific_color != None:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      ax.set_title(f'Certainty for {COLORS[specific_color]} in {CURRENT_SET}')\n",
        "    else:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      ax.set_title(f'Total certainty in {CURRENT_SET}')\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}CertaintyMap_{CURRENT_SET}.pdf')\n",
        "    fig.savefig(f'{path}CertaintyMap_{CURRENT_SET}.png', dpi=300)\n",
        "\n",
        "  return accuracy_map\n",
        "\n",
        "\n",
        "\n",
        "def plotLoss(history):\n",
        "  \"\"\"Plots training loss and validation loss with respect to training epochs.\n",
        "\n",
        "  Args:\n",
        "    history: keras History\n",
        "      history of keras model.\n",
        "  \"\"\"\n",
        "  if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'])\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def showPredictions(model, history, valSet_points, valSet_labels,\n",
        "                    useThresholdPredict=False, showCorrectPoints=False,\n",
        "                    drawGrid=True):\n",
        "  \"\"\"Plots the predictions for the validation points.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which performs the predictions.\n",
        "    valSet_points: 2-D array of shape (x,2)\n",
        "      Data points used for validation.\n",
        "    valSet_labels: 1-D array of shape (x,)\n",
        "      Ground truth labels of the validation points.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    showCorrectPoints: boolean, optional\n",
        "      Whether correctly classified points should be marked as black 'x' or not.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "\n",
        "  Returns:\n",
        "    2-dimensional numpy array of shape (x,2) with the predictions for the\n",
        "    validation points.\n",
        "  \"\"\"\n",
        "  # Predict the validation points\n",
        "  if useThresholdPredict == True:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identifying correctly and incorrectly classified points\n",
        "  correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == True)\n",
        "  incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  number_of_points = np.bincount(np.argmax(prediction, axis=1))\n",
        "\n",
        "  total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  #Average misclassification certainty\n",
        "  misclass_certainties = []\n",
        "  for i in incorrect_indices[0]:\n",
        "    misclass_certainties.append(np.max(prediction[i]))\n",
        "  avg_misclass_certainty = sum(misclass_certainties)/total_misclassifications\n",
        "  \n",
        "  valAccuracy = 100 - (total_misclassifications/sum(number_of_points))*100\n",
        "\n",
        "  print('Validation accuracy: {:.2f}%'.format(valAccuracy))\n",
        "  print(f'Predictions for green: {number_of_points[0]} / {len(valSet_labels)}')\n",
        "  print(f'Predictions for red: {number_of_points[1]} / {len(valSet_labels)}')\n",
        "  print(f'Points misclassified: {total_misclassifications}')\n",
        "  print(f'Red points misclassified: {red_misclassifications}')\n",
        "  print(f'Green points misclassified: {green_misclassifications}')\n",
        "  print('Average misclassification certainty: {:.2f}'.format(avg_misclass_certainty))\n",
        "\n",
        "  if showCorrectPoints:\n",
        "    makePlot(correct_pred_points=valSet_points[correct_indices],\n",
        "           incorrect_pred_points=valSet_points[incorrect_indices], \n",
        "           drawGrid=drawGrid)\n",
        "  else:\n",
        "    makePlot(incorrect_pred_points=valSet_points[incorrect_indices], \n",
        "             drawGrid=drawGrid)\n",
        "    \n",
        "  # Make bar graph showing red and green misclassifications\n",
        "  bars = ('Red', 'Green')\n",
        "  height = [red_misclassifications, green_misclassifications]\n",
        "  x_pos = np.arange(len(bars))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(x_pos, height, width=0.35, color=['red', 'green'])\n",
        "\n",
        "  ax.set_ylabel('Misclassifications')\n",
        "  ax.set_title('Misclassifications by color')\n",
        "  ax.set_xticks(x_pos)\n",
        "  ax.set_xticklabels(bars)\n",
        "\n",
        "  rects = ax.patches # Array of bars\n",
        "\n",
        "  labels = [red_misclassifications, green_misclassifications]\n",
        "\n",
        "  for rect, label in zip(rects, labels): # Add labels above bars\n",
        "      height = rect.get_height()\n",
        "      ax.text(rect.get_x() + rect.get_width() / 2, height, label,\n",
        "              ha='center', va='bottom')\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "def makeDensityMap(accuracy, dataSet=CURRENT_SET, significance=0.1,\n",
        "                   cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                   drawGrid=True, verbose=1, savePlot=False, path=''):\n",
        "  \"\"\"Creates a heatmap of the density of dataSet.\n",
        "\n",
        "    Args:\n",
        "      accuracy: int, optional\n",
        "        Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "        of data points plotted has the dimension accuracy*accuracy.\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      verbose: 0 or 1, optional\n",
        "        Whether to plot the density map or not.\n",
        "      savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "      path: str, optional\n",
        "        Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "    Returns:\n",
        "      2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  density_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  printProgressBar(0, accuracy**2)\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      count = dataSet.loc[(dataSet['x_i1'] - j/accuracy)**2 + \n",
        "              (dataSet['x_i2'] - i/accuracy)**2 <= significance**2]\n",
        "      \n",
        "      density_map[i,j] = len(count)\n",
        "\n",
        "      printProgressBar(i*accuracy + j + 1, accuracy**2)\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(density_map),vmax=np.max(density_map))\n",
        "\n",
        "  # Plotting \n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if specific_color != None:\n",
        "      ax.set_title(f'Density of {COLORS[specific_color]} in {dataSet_char}')\n",
        "    else:\n",
        "      ax.set_title(f'Total density in {dataSet_char}')\n",
        "\n",
        "    plt.imshow(density_map, origin='lower', cmap='Spectral', norm=norm,\n",
        "               extent=[0, 1, 0, 1])\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}DensityMap_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}DensityMap_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "  return density_map\n",
        "  \n",
        "\n",
        "\n",
        "def plotDensity(dataSet=CURRENT_SET, significance=0.1,\n",
        "                cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Colorises and plots the points of dataSet according to their numbers\n",
        "    of neighbors.\n",
        "\n",
        "    Args:\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "      path: str, optional\n",
        "        Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \"\"\"\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  array = np.zeros((total_number_of_points, 3))\n",
        "\n",
        "  # Counting all neighbours within a radius of significance\n",
        "  for i in range(total_number_of_points):\n",
        "    count = dataSet.loc[(dataSet['x_i1'] - dataSet['x_i1'].loc[i])**2 +\n",
        "     (dataSet['x_i2'] - dataSet['x_i2'].loc[i])**2 <= significance**2]\n",
        "\n",
        "    array[i, 0] = dataSet['x_i1'].loc[i]\n",
        "    array[i, 1] = dataSet['x_i2'].loc[i]\n",
        "    array[i, 2] = len(count)\n",
        "\n",
        "    printProgressBar(i+1, total_number_of_points)\n",
        "\n",
        "  print(f'Max: {np.max(array[:,2])}')\n",
        "  print(f'Min: {np.min(array[:,2])}')\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(array[:,2]),vmax=np.max(array[:,2]))\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.scatter(array[:, 0], array[:, 1], c=array[:, 2], cmap=cmap, norm=norm)\n",
        " \n",
        "  ax.set_title(f'Density of dataset {dataSet_char}')\n",
        "  plt.axis('scaled')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show() \n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Density_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Density_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "def makeWeightedCertaintyMap(model, accuracy, significance=0.1,\n",
        "                          useThresholdPredict=False, referenceMethod='even',\n",
        "                          referenceValue=None, drawGrid=True, savePlot=False,\n",
        "                          path=''):\n",
        "  \"\"\"Plots the model's prediction certainty weighted with the density of points\n",
        "    given in the dataset.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's weighted prediction certainty is to be plotted.\n",
        "    accuracy: int\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False)\n",
        "      when calculating the model's prediction certainty.\n",
        "    referenceMethod: str: 'even', 'maxDensity', or 'customValue', optional\n",
        "      The method used to calculate the reference density used. 'even'\n",
        "      calculates the density if all points in dataset were evenly spaced. \n",
        "      'maxDensity' uses the maximum density from densityMap as the reference\n",
        "      ´density. 'customValue' uses a custom reference density.\n",
        "    referenceValue: float between 0 and 1, optional\n",
        "      Defines the custom reference density when using 'customValue' reference\n",
        "      method. Leave blank otherwise. \n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if invalid referenceMethod was given.\n",
        "\n",
        "  \"\"\"\n",
        "  if not referenceMethod in ['even', 'evenSqrt', 'evenLog', 'maxDensity',\n",
        "                             'customValue']:\n",
        "   raise TypeError(f'Invalid referenceMethod given. referenceMethod should be' +\n",
        "                   f' \"even\", \"evenSqrt\", \"evenLog\", \"maxDensity\", or ' +\n",
        "                   f'\"customValue\", but \"{referenceMethod}\" was given.')\n",
        "    \n",
        "  dataSet = getDataSet()\n",
        "\n",
        "  print(f'Calculating certainty map:')\n",
        "  certaintyMap = makeCertaintyMap(model, accuracy, None, useThresholdPredict,\n",
        "                                  verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  print(f'Calculating density map:')\n",
        "  densityMap = makeDensityMap(accuracy, significance=significance, verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  if referenceMethod == 'even':\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenSqrt':\n",
        "    densityMap = np.sqrt(densityMap)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenLog':\n",
        "    densityMap = np.log(densityMap+1)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'maxDensity':\n",
        "    maxDensity = np.max(densityMap)\n",
        "    densityMap = densityMap/maxDensity\n",
        "\n",
        "  elif referenceMethod == 'customValue':\n",
        "    densityMap = densityMap/referenceValue\n",
        "\n",
        "  weightedCertaintyMap = certaintyMap*densityMap\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  plt.imshow(weightedCertaintyMap, origin='lower', cmap='tab20b', vmin=0,\n",
        "             vmax=np.max(weightedCertaintyMap), extent=[0, 1, 0, 1])\n",
        "  \n",
        "  ax.set_title(f'Weighted certainty of datset {CURRENT_SET}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}WeightedCertaintyMap_{CURRENT_SET}.pdf')\n",
        "    fig.savefig(f'{path}WeightedCertaintyMap_{CURRENT_SET}.png', dpi=300)\n",
        "\n",
        "  return weightedCertaintyMap\n",
        "\n",
        "\n",
        "\n",
        "def makeDistributionMap(dataSet=CURRENT_SET, accuracy=10, colorbarLim=-1,\n",
        "                        drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Plots the distribution of dataSet.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'. Dataset who's distribution is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      The distribution map is split up into accuracy*accuracy many fields.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum\n",
        "      distribution percentage is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = dataSet\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  distribution_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    distribution_map[x_i2,x_i1] = distribution_map[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  distribution_map = distribution_map/len(dataSet)\n",
        "\n",
        "  # Plotting \n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_title(f'Distribution of datset {dataSet_char}')\n",
        "\n",
        "\n",
        "  if colorbarLim == -1:\n",
        "    colorbarLim = np.max(distribution_map)\n",
        "\n",
        "  plt.imshow(distribution_map, origin='lower', cmap='Spectral', vmin=0,\n",
        "             vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "  \n",
        "\n",
        "  plt.colorbar()\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}DistributionMap_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}DistributionMap_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "  return distribution_map\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htUB_naf6hZ1",
        "cellView": "form"
      },
      "source": [
        "#@title Penalty Effect\n",
        "\n",
        "def calculatePenaltyEffect(model, x, y, validation_data, interval=(0,1),\n",
        "                           accuracy=10, batch_size=32, epochs=200, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to penalty.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int, optional\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  penalties = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    penalty = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(penalty),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "    history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "    prediction = model.predict(validation_data[0])\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    penalties[i] = penalty\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, iterations+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(penalties, total_misclass_percentages, 'b', penalties, \n",
        "              red_misclass_percentages, 'r', penalties, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by penalty')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Penalty')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averagePenaltyEffect(model, n, valSet_size, path='', interval=(0,1),\n",
        "                         accuracy=10, batch_size=32, epochs=200, verbose=1,\n",
        "                         useBalanceDataset=False):\n",
        "  \"\"\"Plots average penalty effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the penalty effect is measured and averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    useBalanceDataset: boolean, optional\n",
        "      Whether to balance the dataset before training or not.\n",
        "    All others:\n",
        "      See calculatePenaltyEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(penalties), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    \n",
        "    if useBalanceDataset:\n",
        "      dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculatePenaltyEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "         green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotPenaltyEffect(model, data=result, interval=interval, accuracy=accuracy, n=n, \n",
        "                    valSet_size=valSet_size, batch_size=batch_size, epochs=epochs,\n",
        "                    path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}Penalty_Data_{CURRENT_SET}_{model.name}_' +\n",
        "                          f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=penalties).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, penalties,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{interval}', f'{accuracy}', f'{batch_size}', f'{epochs}',\n",
        "                    f'{useBalanceDataset}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','interval','accuracy',\n",
        "           'batch_size','epochs','useBalanceDataset']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotPenaltyEffect(model, data, interval, accuracy, n, valSet_size, batch_size, epochs,\n",
        "                      dataset=CURRENT_SET, ylim=[0,10], maj_yt_incr=1,\n",
        "                      min_yt_incr=0.1, figsize=(14,10), showParameters=True,\n",
        "                      resolution=300, path=''):\n",
        "  \"\"\"Plots average penalty effect given by 'data' and saves png and pdf of plot\n",
        "    to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs:\n",
        "      Parameters used for training and calculaing the average penalty effect.\n",
        "      Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the penalty effect was measured on. 'A', 'B' or 'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Penalties to be plotted on the x-axis\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(penalties, total_misclass_percentages_avg, 'b', penalties, \n",
        "            red_misclass_percentages_avg, 'r', penalties,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by penalty',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Penalty', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(penalties)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpYD2sOxkBjP",
        "cellView": "form"
      },
      "source": [
        "#@title k-Nearest-Neighbour\n",
        "\n",
        "def KNN(dataSet, point, k, significance=0.1, increment=0.05, show_plot=True):\n",
        "  \"\"\" K-nearest neighbor classifier.\n",
        "\n",
        "  Statistical classifier. Uses the k nearest neighbors to predict the color of a\n",
        "  given point by comparing the number of neighbours of each color and weigthing\n",
        "  them with their squared distance to the point.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset to be used for\n",
        "      calculation. Defaults to dataset selected by CURRENT_SET.\n",
        "    point: Array in the form of [x_i1, x_i2]\n",
        "    k: Positive int\n",
        "      Number of neighbours taken into account for classification\n",
        "    significance: float between 0 and 1, optional\n",
        "      Start search radius.\n",
        "    increment: float between 0 and 1, optional\n",
        "      Amount of increment of the search radius while gathering k neighbours.\n",
        "    show_plot: boolean, optional\n",
        "      If 'True' the function plots the dataset and the selected neighbours.\n",
        "\n",
        "  Returns:\n",
        "    A 2-tuple with the predictions for each class. \n",
        "    (prediction_green, prediction_red)\n",
        "  \"\"\"\n",
        "  # Gathering points until at least k neighbours are found \n",
        "  neighb = np.array([])\n",
        "  while significance <= 1 and neighb.shape[0] < k:\n",
        "      neighb = dataSet.loc[(dataSet['x_i1'] - point[0])**2 +\n",
        "                           (dataSet['x_i2'] -point[1])**2 <= significance**2]\n",
        "      significance += increment\n",
        "  \n",
        "  # Reindexing\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "  # Removing all overhang neighbours until there are only k\n",
        "  while neighb.shape[0] > k:\n",
        "    neighb = neighb.drop(np.argmax(dist))\n",
        "    dist[np.argmax(dist)] = -1\n",
        "\n",
        "  # Reindexing\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "\n",
        "  pred_g = 0\n",
        "  pred_r = 0\n",
        "\n",
        "  # Sum the neighbours of each color with the weight 1-dist^2 \n",
        "  for i in range(neighb.shape[0]):\n",
        "    if neighb['l_i'].loc[i] == 0:\n",
        "      pred_g += (1 - dist[i])\n",
        "    elif neighb['l_i'].loc[i] == 1:\n",
        "      pred_r += (1 - dist[i])\n",
        "\n",
        "  # Normalize\n",
        "  pred_g = pred_g / neighb.shape[0]\n",
        "  pred_r = pred_r / neighb.shape[0]\n",
        "\n",
        "  # Plot neighbours \n",
        "  if show_plot:\n",
        "    selected_neighb = [[neighb['x_i1'].loc[i], neighb['x_i2'].loc[i]]\n",
        "                       for i in range(neighb.shape[0])]\n",
        "    makePlot(dataSet, [point], selected_neighb)\n",
        "    print(f'Prediction for green: \\t{pred_g}')\n",
        "    print(f'Prediction for red: \\t{pred_r}')\n",
        "\n",
        "  return (pred_g, pred_r)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMapKNN(k, accuracy = 100, specific_color = None):\n",
        "  \"\"\"Visualizes the prediction certainty of K-nearest-neighbour algorithm for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    k: postive int\n",
        "      The number of neighbours specified for the KNN algorithm who's certainty\n",
        "      is to bevisualized.\n",
        "    accuracy: positive int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid \n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red. \n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy and k\n",
        "    is not an int.\n",
        "  \"\"\"\n",
        "  #Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "    \n",
        "  if not isinstance(k, int):\n",
        "    raise TypeError(f'Invalid type for k. Type is {type(k)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  # Init Data\n",
        "  dataSet = getDataSet()\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Main Loop\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      result = KNN(dataSet, [j/accuracy, i/accuracy], k, show_plot=False)\n",
        "\n",
        "      if specific_color != None:\n",
        "        # Saving the prediction for the specified color\n",
        "        accuracy_map[i,j] = result[specific_color]\n",
        "      else:\n",
        "        accuracy_map[i,j] = np.max(result)\n",
        "    \n",
        "      # Print current progress\n",
        "      printProgressBar((j+1) + i*accuracy, accuracy**2)\n",
        "\n",
        "  # Choosing headline\n",
        "  if specific_color != None:\n",
        "    plt.title(f'Certaintiy for {COLORS[specific_color]}')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1)\n",
        "\n",
        "  else:\n",
        "    plt.title(f'General Certainty')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1)\n",
        "\n",
        "  # Plot\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.xticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.yticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.show()\n",
        "  \n",
        "  return accuracy_map\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tepNATmlkBjy",
        "cellView": "form"
      },
      "source": [
        "#@title Epoch Batch Size\n",
        "\n",
        "def epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    saveAndPlot=True, path='', verbose=1,\n",
        "                    useBalanceDataset=False):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number for a random validation set on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which classifies the validation set.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation set.\n",
        "    batchRange: 2-tuple of ints\n",
        "      The range of batch sizes used. (x,y) where x is the smallest and y is the\n",
        "      largest batch size used. \n",
        "    batchIncrements: int\n",
        "      Increment in which the batch size is increased.\n",
        "    epochRange: 2-tuple of ints\n",
        "      The range of epochs used. (x,y) where x is the smallest and y is the\n",
        "      largest epoch number used.\n",
        "    epochIncrements: int\n",
        "      Increment in which the epoch number is increased.\n",
        "    epsilon: float, optional\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "    saveAndPlot: boolean, optional\n",
        "      Whever to save results to Excel and plot graphs or not. Set to false when\n",
        "      using averageEpochsBatchSize.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print a progress bar or not.\n",
        "    useBalanceDataset: boolean, optional\n",
        "      Whether to balance the dataset before training or not.\n",
        "\n",
        "  Returns:\n",
        "    6-tuple (epochs, batch_sizes, total_misclass_percentage,\n",
        "    red_misclass_percentage, green_misclass_percentage, valSet).\n",
        "    First 5 elements are lists, valSet is pd.DataFrame.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  if verbose > 0:\n",
        "    start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs = []\n",
        "  batch_sizes = []\n",
        "  total_misclass_percentage = []\n",
        "  red_misclass_percentage = []\n",
        "  green_misclass_percentage = []\n",
        "\n",
        "  # Defining iteration lists\n",
        "  batch_size_iter = np.arange(batchRange[0], batchRange[1]+1, batchIncrements)\n",
        "  epoch_iter = np.arange(epochRange[0], epochRange[1]+1, epochIncrements)\n",
        "\n",
        "  if batch_size_iter[0] == 0:\n",
        "    batch_size_iter[0] = 1\n",
        "\n",
        "  # Preparing data\n",
        "  dataSet = getDataSet()\n",
        "  dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "  # Choose random validation set\n",
        "  random.seed(time.time())\n",
        "  val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "  valSet_points, valSet_labels = separateValidationSet(dataSet,val_indices)\n",
        "  \n",
        "  if useBalanceDataset:\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "  training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "  training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "  number_of_points = len(valSet_labels)\n",
        "  red_points = len(np.where(valSet_labels==1)[0])\n",
        "  green_points = len(np.where(valSet_labels==0)[0])\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    num_training_points = training_labels.shape[0]\n",
        "    progress = 0\n",
        "    full = 0\n",
        "    # Calculate full progress\n",
        "    for ep in epoch_iter:\n",
        "      for ba in batch_size_iter:\n",
        "        full += ep*math.ceil(num_training_points/ba)\n",
        "    # Print bar\n",
        "    printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Epoch loop\n",
        "  for ep in epoch_iter:\n",
        "    # Batch size loop\n",
        "    for ba in batch_size_iter:\n",
        "      epochs.append(ep)\n",
        "      batch_sizes.append(ba)\n",
        "\n",
        "      # Prepare model for classification\n",
        "      model.set_weights(initialWeights)\n",
        "\n",
        "      history = model.fit(x=training_points, y=training_labels, batch_size=ba, \n",
        "                          epochs=ep, verbose=0)\n",
        "\n",
        "      # Classification and saving results\n",
        "      prediction = model.predict(valSet_points)\n",
        "\n",
        "      correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                 == True)\n",
        "      incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                   == False)\n",
        "\n",
        "      total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "      red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "      green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "      total_misclass_percentage.append((total_misclassifications/number_of_points)*100)\n",
        "      red_misclass_percentage.append((red_misclassifications/red_points)*100)\n",
        "      green_misclass_percentage.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "      # Update progress bar\n",
        "      if verbose > 0:\n",
        "        progress += ep*math.ceil(num_training_points/ba)\n",
        "        printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "  \n",
        "  # Validation set\n",
        "  valSet = pd.DataFrame.from_dict({'x_i1':valSet_points[:,0],'x_i2':valSet_points[:,1],\n",
        "                                  'l_i':valSet_labels})\n",
        "  \n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}',\n",
        "                    f'{useBalanceDataset}']}\n",
        "\n",
        "  index = ['model','dataset','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon',\n",
        "           'useBalanceDataset']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  if saveAndPlot==True:\n",
        "    today = date.today()\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}EBS_Data_{CURRENT_SET}_' + \n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "    \n",
        "    # Create multiindex for epochs and batch_sizes\n",
        "    arrays = [epochs,batch_sizes]\n",
        "\n",
        "    tuples = list(zip(*arrays))\n",
        "\n",
        "    multiindex = pd.MultiIndex.from_tuples(tuples,\n",
        "                                      names=[\"epoch\", \"batch_size\"])\n",
        "    \n",
        "    # All data\n",
        "    allData = pd.DataFrame({'total':total_misclass_percentage,\n",
        "                            'red':red_misclass_percentage,\n",
        "                            'green':green_misclass_percentage}, index=multiindex)\n",
        "    \n",
        "    allData.to_excel(writer, sheet_name='All Data')\n",
        "\n",
        "    # Optimum points\n",
        "    result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage)\n",
        "    optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "    optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "    # Parameters\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation set\n",
        "    valSet.to_excel(writer, sheet_name='Validation Set')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage, valSet)\n",
        "  \n",
        "  if saveAndPlot==True:\n",
        "    plotEpochsBatchSize(model, result, path=path)\n",
        "\n",
        "  return result\n",
        "  \n",
        "\n",
        "\n",
        "def calculateOptimumPoints(data, epsilon):\n",
        "  \"\"\"Calculates optimum points of epoch and batch size for total, red, and green\n",
        "    misclassification. \n",
        "\n",
        "  Args:\n",
        "    data: 5-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage) or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    epsilon: float\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "\n",
        "  Returns: pd.DataFrame\n",
        "    columns: [min_misclass, epsilon, opt_misclass, opt_epoch, opt_batch,\n",
        "             t_misclass_here, r_misclass here, g_misclass_here]\n",
        "    rows: [total, red, green]\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not in correct form.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The elements of the 5-tuple data must all have the' +\n",
        "                  f' same shape. The {i+1}. element has shape {data[i].shape}' +\n",
        "                  f' and the {i+2}. element has shape {data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The elements of the 5-tuple data must all be' + \n",
        "                      f' 1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass_percentage = data[2]\n",
        "    red_misclass_percentage = data[3]\n",
        "    green_misclass_percentage = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, 'All Data')\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "\n",
        "    epochs = data['epoch']\n",
        "    batch_sizes = data['batch_size']\n",
        "    total_misclass_percentage = data['total']\n",
        "    red_misclass_percentage = data['red']\n",
        "    green_misclass_percentage = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # CALCULATE OPTIMUM POINTS\n",
        "  # For total: Finds the configuration with total misclass within epsilon of\n",
        "  #   minimum total misclass which has the lowest red misclass.\n",
        "  # For green: Finds the configuration with green misclass within epsilon of\n",
        "  #   minimum green misclass which has the lowest red misclass.\n",
        "  # For red: Finds the configuration with red misclass within epsilon of\n",
        "  #   minimum red misclass which has the lowest total misclass.\n",
        "  columns = ['min_misclass', 'epsilon', 'opt_misclass', 'opt_epoch', 'opt_batch',\n",
        "             't_misclass_here', 'r_misclass here', 'g_misclass_here']\n",
        "  rows = ['total', 'red', 'green']\n",
        "  t_considerable_indices = []\n",
        "  r_considerable_indices = []\n",
        "  g_considerable_indices = []\n",
        "\n",
        "  #Total\n",
        "  t_min = np.min(total_misclass_percentage)\n",
        "  t_opt = np.argmin(total_misclass_percentage)  # Index of optimum point for t\n",
        "  for index in range(len(total_misclass_percentage)):\n",
        "    if total_misclass_percentage[index] <= (t_min+epsilon):\n",
        "      t_considerable_indices.append(index)\n",
        "  for index in t_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[t_opt]:\n",
        "      t_opt = index\n",
        "\n",
        "  #Green\n",
        "  g_min = np.min(green_misclass_percentage)\n",
        "  g_opt = np.argmin(green_misclass_percentage)  # Index of optimum point for g\n",
        "  for index in range(len(green_misclass_percentage)):\n",
        "    if green_misclass_percentage[index] <= (g_min+epsilon):\n",
        "      g_considerable_indices.append(index)\n",
        "  for index in g_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[g_opt]:\n",
        "      g_opt = index\n",
        "\n",
        "  #Red\n",
        "  r_min = np.min(red_misclass_percentage)\n",
        "  r_opt = np.argmin(red_misclass_percentage)  # Index of optimum point for r\n",
        "  for index in range(len(red_misclass_percentage)):\n",
        "    if red_misclass_percentage[index] <= (r_min+epsilon):\n",
        "      r_considerable_indices.append(index)\n",
        "  # Find point with lowest total misclass\n",
        "  # Only change r_opt if the improvement in total misclass is greater than the\n",
        "  #   loss in red misclass\n",
        "  for index in r_considerable_indices:  \n",
        "    if (total_misclass_percentage[index] < total_misclass_percentage[r_opt] and \n",
        "        (total_misclass_percentage[index]-total_misclass_percentage[r_opt] <\n",
        "         red_misclass_percentage[r_opt]-red_misclass_percentage[index])):\n",
        "      r_opt = index\n",
        "  \n",
        "  total_row = [t_min, epsilon, total_misclass_percentage[t_opt], epochs[t_opt],\n",
        "               batch_sizes[t_opt], total_misclass_percentage[t_opt],\n",
        "               red_misclass_percentage[t_opt], green_misclass_percentage[t_opt]]\n",
        "  red_row = [r_min, epsilon, red_misclass_percentage[r_opt], epochs[r_opt],\n",
        "               batch_sizes[r_opt], total_misclass_percentage[r_opt],\n",
        "               red_misclass_percentage[r_opt], green_misclass_percentage[r_opt]]\n",
        "  green_row = [g_min, epsilon, green_misclass_percentage[g_opt], epochs[g_opt],\n",
        "               batch_sizes[g_opt], total_misclass_percentage[g_opt],\n",
        "               red_misclass_percentage[g_opt], green_misclass_percentage[g_opt]]\n",
        "\n",
        "  return pd.DataFrame([total_row, red_row, green_row], index=rows, \n",
        "                      columns=columns)\n",
        "  \n",
        "\n",
        "def averageEpochsBatchSize(model, n, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    path='', verbose=1, useBalanceDataset=False):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number averaged over n validation sets on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    n: int\n",
        "      Number of iterations.\n",
        "    All others:\n",
        "      See epochsBatchSize.\n",
        "\n",
        "  Returns:\n",
        "    5 tuple of lists (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "    green_misclass_avg).\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs_collected = []\n",
        "  batch_sizes_collected = []\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "\n",
        "  # For saving in excel\n",
        "  validationSets = {}\n",
        "  misclassCollected = {}\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # Collecting misclassification percentages\n",
        "    data = epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                           batchIncrements, epochRange, epochIncrements,\n",
        "                           epsilon=0, saveAndPlot=False, verbose=0, \n",
        "                           useBalanceDataset=useBalanceDataset)\n",
        "\n",
        "    epochs_collected.append(data[0])\n",
        "    batch_sizes_collected.append(data[1])\n",
        "    total_misclass_collected.append(data[2])\n",
        "    red_misclass_collected.append(data[3])\n",
        "    green_misclass_collected.append(data[4])\n",
        "\n",
        "    # Adding validation set to dictionary for dataframe\n",
        "    validationSets[f'x_i1:{i}'] = data[5]['x_i1']\n",
        "    validationSets[f'x_i2:{i}'] = data[5]['x_i2']\n",
        "    validationSets[f'l_i:{i}'] = data[5]['l_i']\n",
        "\n",
        "    # Adding misclassification data to dictionary for dataframe\n",
        "    misclassCollected[f'total:{i}'] = data[2]\n",
        "    misclassCollected[f'red:{i}'] = data[3]\n",
        "    misclassCollected[f'green{i}'] = data[4]\n",
        "\n",
        "    # Update progress bar\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  epochs = np.average(epochs_collected, axis=0)\n",
        "  batch_sizes = np.average(batch_sizes_collected, axis=0)\n",
        "  total_misclass_avg = np.average(total_misclass_collected, axis=0)\n",
        "  red_misclass_avg = np.average(red_misclass_collected, axis=0)\n",
        "  green_misclass_avg = np.average(green_misclass_collected, axis=0)\n",
        "\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  today = date.today()\n",
        "\n",
        "  # Create multiindex for epochs and batch_sizes\n",
        "  arrays = [epochs,batch_sizes]\n",
        "\n",
        "  tuples = list(zip(*arrays))\n",
        "\n",
        "  multiindex = pd.MultiIndex.from_tuples(tuples, names=[\"epoch\", \"batch_size\"])\n",
        "\n",
        "  # Initialize writer\n",
        "  writer = pd.ExcelWriter(f'{path}Avg_EBS_Data_{CURRENT_SET}_' + \n",
        "                        f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "  # Average \n",
        "  average = pd.DataFrame({'total':total_misclass_avg,\n",
        "                          'red':red_misclass_avg,\n",
        "                          'green':green_misclass_avg}, index=multiindex)\n",
        "\n",
        "  average.to_excel(writer, sheet_name='Average')\n",
        "\n",
        "  # Collected\n",
        "  misclassCollected = pd.DataFrame(misclassCollected, index=multiindex)\n",
        "  misclassCollected.to_excel(writer, sheet_name='Collected')\n",
        "\n",
        "  # Optimum points\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "  optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}',\n",
        "                    f'{useBalanceDataset}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon',\n",
        "           'useBalaceDataset']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "  parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  validationSets = pd.DataFrame.from_dict(validationSets)\n",
        "  validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  \n",
        "  plotEpochsBatchSize(model, result, path=path, prefix='Avg_')\n",
        "  \n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotEpochsBatchSize(model, data, dataset=CURRENT_SET,\n",
        "                        misclass_range=(0,15), figsize=(14,10), resolution=300,\n",
        "                        cmap='viridis', path='', prefix=''):\n",
        "  \"\"\"Plots a 3D graph showing the relation between epoch number, batch size,\n",
        "    and percentage misclassification.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model which was used for training and classification.\n",
        "    data: 6-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage, valSet) or the name of an Excel\n",
        "      sheet present in the directory as a String (e.g. 'data.xlsx').\n",
        "    dataset: char, optional\n",
        "      The dataset used. 'A', 'B' or 'C'.\n",
        "    misclass_range: 2-tuple, optional\n",
        "      The range of misclassification percentages plotted (limits of the z-axis).\n",
        "    figsize: 2-tuple, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    cmap: Colormap, optional\n",
        "      A colormap for the surface patches.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    prefix: str, optional\n",
        "      appended to the front of the pnd and pdf file names\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is of invalid type or shape.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The first 5 elements of the tuple data must all ' +\n",
        "                  f'have the same shape. The {i+1}. element has shape ' +\n",
        "                  f'{data[i].shape} and the {i+2}. element has shape ' +\n",
        "                  f'{data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The first 5 elements of the tuple data must all be ' +\n",
        "                      f'1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass = data[2]\n",
        "    red_misclass = data[3]\n",
        "    green_misclass = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, sheet_name=0, index_col=[0,1])\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "    \n",
        "    index_list = list(data.index)\n",
        "    index_len = len(index_list)\n",
        "    \n",
        "    epochs = []\n",
        "    batch_sizes = []\n",
        "    for i in range(index_len):\n",
        "      epochs.append(index_list[i][0])\n",
        "      batch_sizes.append(index_list[i][1])\n",
        "\n",
        "    total_misclass = data['total']\n",
        "    red_misclass = data['red']\n",
        "    green_misclass = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # Plotting\n",
        "  fig = plt.figure(figsize=(figsize[0], figsize[1]*3))\n",
        "  # Total misclassification\n",
        "  ax_t = fig.add_subplot(3, 1, 1, projection='3d')\n",
        "  ax_t.plot_trisurf(epochs, batch_sizes, total_misclass, cmap=cmap)\n",
        "  ax_t.set_title(f'Dataset {dataset}: Total misclassification by epoch and batch size')\n",
        "  ax_t.set_xlabel('Epochs')\n",
        "  ax_t.set_ylabel('Batch size')\n",
        "  ax_t.set_zlabel('% misclassification')\n",
        "  ax_t.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Red misclassification\n",
        "  ax_r = fig.add_subplot(3, 1, 2, projection='3d')\n",
        "  ax_r.plot_trisurf(epochs, batch_sizes, red_misclass, cmap=cmap)\n",
        "  ax_r.set_title(f'Dataset {dataset}: Red misclassification by epoch and batch size')\n",
        "  ax_r.set_xlabel('Epochs')\n",
        "  ax_r.set_ylabel('Batch size')\n",
        "  ax_r.set_zlabel('% misclassification')\n",
        "  ax_r.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Green misclassification\n",
        "  ax_g = fig.add_subplot(3, 1, 3, projection='3d')\n",
        "  ax_g.plot_trisurf(epochs, batch_sizes, green_misclass, cmap=cmap)\n",
        "  ax_g.set_title(f'Dataset {dataset}: Green misclassification by epoch and batch size')\n",
        "  ax_g.set_xlabel('Epochs')\n",
        "  ax_g.set_ylabel('Batch size')\n",
        "  ax_g.set_zlabel('% misclassification')\n",
        "  ax_g.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdlLeaiYaqo",
        "cellView": "form"
      },
      "source": [
        "#@title Different Training Approaches\n",
        "def diffPenaltyAproach(n, val_size, model, penalty, epochs, batch_size, increment, epoch_end_of_inc, verbose=0, figsize=(14,10), path=''):\n",
        "  '''Plots the average misclassification of each class for penalty increasing, \n",
        "  consistent penalty and penalty decreasing fitting in a bar graph.\n",
        "\n",
        "  Args:\n",
        "    n: int\n",
        "      Number of cyclces for averaging\n",
        "    model: keras model\n",
        "      Model which classifies the validation set.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    penalty: float between 0 and 1\n",
        "      Penalty to be added to the loss of misclassified red points during fitting\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation set.\n",
        "    epochs: int\n",
        "      Number of epochs for each training cycle\n",
        "    batch_size: int\n",
        "      Batch size to be used for training\n",
        "    epoch_increment: int\n",
        "      Amount of epochs after the penalty should be incremented\n",
        "    epoch_end_of_inc: int\n",
        "      Defines the point at which the increasing or decreasing stops and the modell\n",
        "      will train with a consistent penalty value for the rest of the epochs\n",
        "    verbose: boolean, optional\n",
        "      Whether to print a progress bar or not.\n",
        "    figsize: 2-tuple of int, optional\n",
        "      Determines the size of the figure\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  '''\n",
        "  dataSet_original = getDataSet()\n",
        "  valSets = [getBalancedValSetIndices(dataSet_original, val_size, THRESHOLD_VAL) for i in range(n)]\n",
        "\n",
        "  history_1 = np.zeros((n,3))\n",
        "  history_2 = np.zeros((n,3))\n",
        "  history_3 = np.zeros((n,3))\n",
        "\n",
        "  printProgressBar(0, 3*n)\n",
        "\n",
        "  model.set_weights(initialWeights)\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels)\n",
        "    \n",
        "    history_1[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i+1, 3*n)\n",
        "\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    model.fit(training_points, training_labels, epochs=epochs,\n",
        "                                batch_size=batch_size, shuffle=True, verbose=0)\n",
        "\n",
        "    history_2[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + n+1, 3*n)\n",
        "  \n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=False)\n",
        "    \n",
        "    history_3[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + 2*n+1, 3*n)\n",
        "  \n",
        "  clear_output()\n",
        "\n",
        "  labels = ['total', 'red', 'green']\n",
        "  y_1 = [i/n for i in np.sum(history_1, axis=0)]\n",
        "  y_2 = [i/n for i in np.sum(history_2, axis=0)]\n",
        "  y_3 = [i/n for i in np.sum(history_3, axis=0)]\n",
        "\n",
        "  x = np.arange(len(labels))  # the label locations\n",
        "  width = 0.2  # the width of the bars\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  rects1 = ax.bar(x - width, y_1, width, label='With Increment')\n",
        "  rects2 = ax.bar(x, y_2, width, label='Normal')\n",
        "  rects3 = ax.bar(x + width, y_3, width, label='With Decrement')\n",
        "\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_ylabel('Misclassification in %')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "\n",
        "  fig.text(0,0, f'Dataset: {CURRENT_SET}, Epochs: {epochs}, Batch Size: {batch_size}, Epoch Increment: {increment}, Epoch end of Increment: {epoch_end_of_inc}')\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Comparison{CURRENT_SET}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bljczC5kBj7",
        "cellView": "form"
      },
      "source": [
        "#@title Custom Loss Function\n",
        "\n",
        "def construct_custom_penalty_loss(penalty,\n",
        "                                  lossFunction=keras.losses.sparse_categorical_crossentropy):\n",
        "  \"\"\"Constructs a loss function which penalizes 'red as green' misclassifications. \n",
        "\n",
        "  Args:\n",
        "    penalty: float between 0 and 1\n",
        "      Value added to the loss if a red point is misclassified as green. \n",
        "    lossFunction: loss function, optional\n",
        "      The loss function used after adapting the loss values.\n",
        "\n",
        "  Returns:\n",
        "    custom_penalty_loss function with specified penalty and loss function. Can be\n",
        "    used like a regular loss function. \n",
        "  \"\"\"\n",
        "\n",
        "  def custom_penalty_loss(y_true, y_pred):\n",
        "    length = tf.shape(y_true)[0]\n",
        "\n",
        "    #Creating a vector with all values set to the penalty: [0.3, 0.3, ... 0.3]\n",
        "    error = tf.multiply(tf.constant(penalty, tf.float32), tf.ones(length)) \n",
        "\n",
        "    #Setting every entry to 0 if the corresponding entry in y_true is 1\n",
        "    error = tf.where(tf.equal(y_true[:, 0], tf.zeros(length)), error, tf.zeros(length))\n",
        "\n",
        "    #Setting every entry to 0 if the algorithm predicted 0\n",
        "    error = tf.where(tf.greater(y_pred[:, 0], y_pred[:, 1]), tf.zeros(length), error)\n",
        "\n",
        "    #Transforms the vector from [0, 0, 0.3, ... 0,3] to [[0, -0], [0, -0], [0.3, -0.3], ... [0.3, -0.3]]\n",
        "    error = tf.stack([error, tf.multiply(tf.constant(-1, tf.float32), error)], 1)\n",
        "\n",
        "    #Adding the artificial loss\n",
        "    y_pred = y_pred + error\n",
        "\n",
        "    #Eliminating values > 1 or < 0\n",
        "    y_pred0 = tf.where(tf.greater(y_pred[:, 0], tf.ones(length)), tf.ones(length), y_pred[:, 0])\n",
        "    y_pred1 = tf.where(tf.greater(y_pred[:, 1], tf.zeros(length)), y_pred[:, 1], tf.zeros(length))\n",
        "    y_pred = tf.stack([y_pred0, y_pred1], axis=1)\n",
        "\n",
        "\n",
        "    loss = lossFunction(y_pred=y_pred, y_true=y_true)\n",
        "    return loss\n",
        "  \n",
        "  return custom_penalty_loss"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_01aKYUEy82",
        "cellView": "form"
      },
      "source": [
        "#@title Certainty Balance Effect\n",
        "\n",
        "def calculateBalanceEffect(model, dataset, validation_data, interval=(0.8,1),\n",
        "                             accuracy=10, batch_size=64, epochs=500, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to the\n",
        "    certainty threshold for predicting points as green.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    dataset: pandas.DataFrame\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset which the\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the threshold interval plotted. x is the lowest\n",
        "      threshold, y the highest.\n",
        "    accuracy: int, optional\n",
        "      Threshold interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  thresholds = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  # Initialize and fit model\n",
        "  model.set_weights(initialWeights)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "  \n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    threshold = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "\n",
        "    current_data = dataset.copy()\n",
        "    current_data = balanceDataset(current_data, threshold, verbose=0)\n",
        "\n",
        "    y = np.array(current_data['l_i']).astype('float')\n",
        "    x = np.array(current_data[['x_i1','x_i2']])\n",
        "\n",
        "    model.set_weights(initialWeights)\n",
        "    history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "    prediction = model.predict(validation_data[0])\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction,axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    thresholds[i] = threshold\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, accuracy+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(thresholds, total_misclass_percentages, 'b', thresholds, \n",
        "              red_misclass_percentages, 'r', thresholds, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by balance threshold')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Balance threshold')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averageBalanceEffect(model, n, valSet_size, path='', interval=(0.8,1),\n",
        "                         accuracy=10, batch_size=64, epochs=500, verbose=1):\n",
        "  \"\"\"Plots average balance effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the certainty threshold effect is measured and\n",
        "      averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    useBalanceDataset: boolean, optional\n",
        "      Whether to balance the dataset before training or not.\n",
        "    All others:\n",
        "      See calculateBalanceEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(thresholds), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = random.sample(range(SOURCE_SIZE[CURRENT_SET]), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculateBalanceEffect(model, dataSet, \n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "            green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotBalanceEffect(model, data=result, interval=interval, accuracy=accuracy,\n",
        "                      n=n, valSet_size=valSet_size, batch_size=batch_size,\n",
        "                      epochs=epochs, path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}BalanceThreshold_Data_{CURRENT_SET}_' +\n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=thresholds).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, thresholds,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{interval}', f'{accuracy}', f'{batch_size}',\n",
        "                    f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plotBalanceEffect(model, data, interval, accuracy, n, valSet_size,\n",
        "                      batch_size, epochs, penalty=PENALTY, dataset=CURRENT_SET,\n",
        "                      ylim=[0,10], maj_yt_incr=1, min_yt_incr=0.1,\n",
        "                      figsize=(14,10), showParameters=True, resolution=300,\n",
        "                      path=''):\n",
        "  \"\"\"Plots average certainty threshold effect given by 'data' and saves png and\n",
        "    pdf of plot to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the certainty threshold interval plotted. x is the\n",
        "      lowest penalty, y the highest.\n",
        "    accuracy: int\n",
        "      Certainty threshold interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs, penalty:\n",
        "      Parameters used for training and calculaing the average certainty\n",
        "      threshold effect. Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the certainty penalty effect was measured on. 'A', 'B' or\n",
        "      'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Thresholds to be plotted on the x-axis\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(thresholds, total_misclass_percentages_avg, 'b', thresholds, \n",
        "            red_misclass_percentages_avg, 'r', thresholds,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by balance threshold',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Balance threshold', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(thresholds)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}\\n' +\n",
        "                   f'Penalty: {penalty}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}BalanceEffect_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}BalanceEffect_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "12yBTNbsaoto"
      },
      "source": [
        "#@title Certainty Threshold Effect\n",
        "\n",
        "def calculateThresholdEffect(model, x, y, validation_data, interval=(0.8,1),\n",
        "                             accuracy=10, batch_size=64, epochs=500, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to the\n",
        "    certainty threshold for predicting points as green.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the threshold interval plotted. x is the lowest\n",
        "      threshold, y the highest.\n",
        "    accuracy: int, optional\n",
        "      Threshold interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  thresholds = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  # Initialize and fit model\n",
        "  model.set_weights(initialWeights)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "  history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    threshold = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "\n",
        "    prediction = thresholdPredict(validation_data[0], model, threshold)\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction,axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    thresholds[i] = threshold\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, accuracy+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(thresholds, total_misclass_percentages, 'b', thresholds, \n",
        "              red_misclass_percentages, 'r', thresholds, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by certainty threshold')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Certainty threshold')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averageThresholdEffect(model, n, valSet_size, path='', interval=(0.8,1),\n",
        "                         accuracy=10, batch_size=64, epochs=500, verbose=1,\n",
        "                         useBalanceDataset=False):\n",
        "  \"\"\"Plots average certainty threshold effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the certainty threshold effect is measured and\n",
        "      averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    useBalanceDataset: boolean, optional\n",
        "      Whether to balance the dataset before training or not.\n",
        "    All others:\n",
        "      See calculateThresholdEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(thresholds), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = random.sample(range(SOURCE_SIZE[CURRENT_SET]), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    \n",
        "    if useBalanceDataset:\n",
        "      dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculateThresholdEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "            green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotThresholdEffect(model, data=result, interval=interval, accuracy=accuracy,\n",
        "                      n=n, valSet_size=valSet_size, batch_size=batch_size,\n",
        "                      epochs=epochs, path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}CertaintyThreshold_Data_{CURRENT_SET}_' +\n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=thresholds).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, thresholds,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{interval}', f'{accuracy}', f'{batch_size}',\n",
        "                    f'{epochs}', f'{useBalanceDataset}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','interval','accuracy',\n",
        "           'batch_size','epochs','useBalanceDataset']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotThresholdEffect(model, data, interval, accuracy, n, valSet_size,\n",
        "                      batch_size, epochs, penalty=PENALTY, dataset=CURRENT_SET,\n",
        "                      ylim=[0,10], maj_yt_incr=1, min_yt_incr=0.1,\n",
        "                      figsize=(14,10), showParameters=True, resolution=300,\n",
        "                      path=''):\n",
        "  \"\"\"Plots average certainty threshold effect given by 'data' and saves png and\n",
        "    pdf of plot to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the certainty threshold interval plotted. x is the\n",
        "      lowest penalty, y the highest.\n",
        "    accuracy: int\n",
        "      Certainty threshold interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs, penalty:\n",
        "      Parameters used for training and calculaing the average certainty\n",
        "      threshold effect. Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the certainty penalty effect was measured on. 'A', 'B' or\n",
        "      'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Thresholds to be plotted on the x-axis\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(thresholds, total_misclass_percentages_avg, 'b', thresholds, \n",
        "            red_misclass_percentages_avg, 'r', thresholds,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by certainty threshold',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Certainty threshold', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(thresholds)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}\\n' +\n",
        "                   f'Penalty: {penalty}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVX3rT2aStCu",
        "cellView": "form"
      },
      "source": [
        "#@title Points Per Square\n",
        "def pointsPerSquare(dataSet=CURRENT_SET, accuracy=100):\n",
        "  \"\"\"Calculates the number of points from dataSet present in each square of an\n",
        "    accuracy x accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'.\n",
        "    accuracy: int, optional\n",
        "      The grid consists of accuracy x accuracy many squares.\n",
        "\n",
        "  Returns:\n",
        "    Three 2-D np.array of the shape (accuracy,accuracy): squares, red and green.\n",
        "    squares contains the number of total points present in each square, red\n",
        "    contains the number of red points present in each square, and green contains\n",
        "    the number of green points present in each square.\n",
        "  \"\"\"\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  squares = np.zeros((accuracy, accuracy))\n",
        "  red = np.zeros((accuracy, accuracy))\n",
        "  green = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    squares[x_i2,x_i1] = squares[x_i2,x_i1]+1\n",
        "\n",
        "    if (dataSet.loc[i]['l_i']) == 0:\n",
        "      green[x_i2,x_i1] = green[x_i2,x_i1]+1\n",
        "    else:\n",
        "      red[x_i2,x_i1] = red[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  return squares, red, green"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkIU-4-J-jw",
        "cellView": "form"
      },
      "source": [
        "#@title Misclassifications per square\n",
        "def misclassPerSquare(model, validationSet, accuracy=10, useThresholdPredict=False,\n",
        "                      drawGrid=True, verbose=0, savePlot=False, path='',\n",
        "                      colorbarLim=-1):\n",
        "  \"\"\"Calculates proportion of red, green, and total validaion points\n",
        "    misclassified per square in an accuracy*accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    validationSet: 2-tuple of np.arrays\n",
        "      2-tuple of the form (valSet_points, valSet_labels), where valSet_points is\n",
        "      a np.array of shape (x,2) and valSet_labels is a np.array of shape (x,1).\n",
        "    accuracy: int, optional\n",
        "      The dataset is split up into accuracy*accuracy many fields.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the misclassifications per square or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plot will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum misclass\n",
        "      proportion is used as the upper limit.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple 2-D np.arrays (total, red, green) of shape (accuracy,accuracy)\n",
        "    containing the proportions of total, red, and green misclassifications per\n",
        "    square as floats between 0 and 1.\n",
        "  \"\"\"\n",
        "  # Predicting the validation points\n",
        "  valSet_points = validationSet[0]\n",
        "  valSet_labels = validationSet[1]\n",
        "\n",
        "  # Preparing arrays\n",
        "  totalPoints = np.zeros((accuracy,accuracy))\n",
        "  totalMisclass = np.zeros((accuracy,accuracy))\n",
        "  redPoints = np.zeros((accuracy,accuracy))\n",
        "  redMisclass = np.zeros((accuracy,accuracy))\n",
        "  greenPoints = np.zeros((accuracy,accuracy))\n",
        "  greenMisclass = np.zeros((accuracy,accuracy))\n",
        "\n",
        "  # Predicting points\n",
        "  if useThresholdPredict:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identifying incorrectly classified points\n",
        "  incorrect_indices = np.where((valSet_labels != np.argmax(prediction, axis=1)))\n",
        "\n",
        "  # Multiplying all entries with accuracy to calculate which square each\n",
        "  # validation point falls into\n",
        "  valSet_points = valSet_points*accuracy\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, len(valSet_points))\n",
        "\n",
        "  for i in range(len(valSet_points)):\n",
        "    x_i1 = math.floor(valSet_points[i,0])\n",
        "    x_i2 = math.floor(valSet_points[i,1])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    # Total\n",
        "    totalPoints[x_i2,x_i1] += 1\n",
        "    if i in incorrect_indices[0]:\n",
        "      totalMisclass[x_i2,x_i1] += 1\n",
        "    \n",
        "    # Red\n",
        "    if valSet_labels[i] == 1:\n",
        "      redPoints[x_i2,x_i1] += 1\n",
        "\n",
        "      if i in incorrect_indices[0]:\n",
        "        redMisclass[x_i2,x_i1] += 1\n",
        "    \n",
        "    # Green\n",
        "    if valSet_labels[i] == 0:\n",
        "      greenPoints[x_i2,x_i1] += 1\n",
        "\n",
        "      if i in incorrect_indices[0]:\n",
        "        greenMisclass[x_i2,x_i1] += 1\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, len(valSet_points))\n",
        "\n",
        "  # Setting all 0 entries in xPoints to 1 to prevent div by zero\n",
        "  totalPoints[totalPoints == 0] = 1\n",
        "  redPoints[redPoints == 0] = 1\n",
        "  greenPoints[greenPoints == 0] = 1\n",
        "\n",
        "  # Calculating proportion of validation points misclassified\n",
        "  totalMisclassPerSquare = totalMisclass/totalPoints\n",
        "  redMisclassPerSquare = redMisclass/redPoints\n",
        "  greenMisclassPerSquare = greenMisclass/greenPoints\n",
        "\n",
        "  # PLOTTING\n",
        "  today = date.today()\n",
        "  # Total\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of val points misclassified in {CURRENT_SET}')\n",
        "   \n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(totalMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(totalMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving total plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Red\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of r val points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(redMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(redMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving red plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Green\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of g val points misclassified in {CURRENT_SET}')\n",
        "    \n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(greenMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(greenMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving green plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  return (totalMisclassPerSquare, redMisclassPerSquare, greenMisclassPerSquare)\n",
        "\n",
        "\n",
        "\n",
        "def avgMisclassPerSquare(model, initialWeights, n, valSet_size, batch_size,\n",
        "                         epochs, accuracy=10, useThresholdPredict=False,\n",
        "                         drawGrid=True, verbose=1, savePlot=False,\n",
        "                         saveToExcel=False, path='', colorbarLim=-1,\n",
        "                         useBalanceDataset=False):\n",
        "  \"\"\"Calculates average proportions of red, green, and total validaion points\n",
        "    misclassified per square in an accuracy*accuracy grid over n training rounds\n",
        "    and validation sets.\n",
        "    \n",
        "  Args:\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    n: int\n",
        "      The number of training/predition rounds to average over.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation sets.\n",
        "    batch_size: int\n",
        "      Batch size used for training the model.\n",
        "    epochs: int\n",
        "      Number of epochs used for training the model.\n",
        "    accuracy: int, optional\n",
        "      The dataset is split up into accuracy*accuracy many fields.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot results or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    saveToExcel: boolean, optional\n",
        "      Whether to save the results as an Excel document or not.\n",
        "    path: str, optional\n",
        "      Path to which the results will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum avg\n",
        "      misclass proportion is used as the upper limit.\n",
        "    useBalanceDataset: boolean, optional\n",
        "      Whether to balance the dataset before training or not.\n",
        "\n",
        "  Returns: 6-tuple (at, ar, ag, ct, cr, cg)\n",
        "    - (at, ar, ag): 2-D np.arrays (avgTotal, avgRed, avgGreen) of shape\n",
        "    (accuracy,accuracy) containing the avg proportions of total, red, and green\n",
        "    misclassifications per square as floats between 0 and 1.\n",
        "    - (ct, cr, cg): 1-D lists of shape (n,) of 2-D np.arrays of shape\n",
        "    (accuracy,accuracy) containing the proportions of total, red, and green\n",
        "    misclassifications per square of each run as floats between 0 and 1.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "  avgTotalMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  avgRedMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  avgGreenMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  validationSets = {}\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  for i in range(n):\n",
        "    # Preparing data\n",
        "    dataSet = getDataSet()\n",
        "    dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]\n",
        "\n",
        "    # Choose random validation set\n",
        "    random.seed(time.time())\n",
        "    val_indices = random.sample(range(len(dataSet)), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet, val_indices)\n",
        "\n",
        "    if useBalanceDataset:\n",
        "      dataSet = balanceDataset(dataSet, THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Classifying validation set\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    history = model.fit(x=training_points, y=training_labels,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "    \n",
        "    # Calculating misclassification per square\n",
        "    result = misclassPerSquare(model, (valSet_points,valSet_labels), accuracy,\n",
        "                               useThresholdPredict)\n",
        "    \n",
        "    total_misclass_collected.append(result[0])\n",
        "    red_misclass_collected.append(result[1])\n",
        "    green_misclass_collected.append(result[2])\n",
        "\n",
        "    # Saving validation set for Excel\n",
        "    if saveToExcel == True:\n",
        "      valSet_points = pd.DataFrame(valSet_points)\n",
        "      valSet_labels = pd.DataFrame(valSet_labels)\n",
        "\n",
        "      validationSets[f'x_i1:{i}'] = valSet_points[0]\n",
        "      validationSets[f'x_i2:{i}'] = valSet_points[1]\n",
        "      validationSets[f'l_i:{i}'] = valSet_labels[0]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  avgTotalMisclassPerSquare = np.average(total_misclass_collected, axis=0)\n",
        "  avgRedMisclassPerSquare = np.average(red_misclass_collected, axis=0)\n",
        "  avgGreenMisclassPerSquare = np.average(green_misclass_collected, axis=0)\n",
        "\n",
        "  # PLOTTING  \n",
        "  today = date.today()\n",
        "  # Total\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgTotalMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgTotalMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving total plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Total_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Red\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of r points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgRedMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgRedMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving red plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Red_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Green\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of g points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgGreenMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgGreenMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Saving green plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Green_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "    \n",
        "\n",
        "  # Saving to Excel \n",
        "  if saveToExcel == True:\n",
        "    # Averages\n",
        "    index = [i/accuracy for i in range(accuracy)]\n",
        "    index.reverse()  # Reverse index for correct orientation in Excel\n",
        "    columns = [i/accuracy for i in range(accuracy)]  \n",
        "    # Flip for correct orientation in Excel\n",
        "    totalFlipped = np.flipud(avgTotalMisclassPerSquare)\n",
        "    redFlipped = np.flipud(avgRedMisclassPerSquare)\n",
        "    greenFlipped = np.flipud(avgGreenMisclassPerSquare)\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}Avg_Misclass_Per_Square_' +\n",
        "                            f'{CURRENT_SET}_{model.name}_' +\n",
        "                            f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "    # Total \n",
        "    total = pd.DataFrame(totalFlipped, columns=columns,\n",
        "                         index=index)\n",
        "    \n",
        "    total.to_excel(writer, sheet_name='Total')\n",
        "\n",
        "    # Red\n",
        "    red = pd.DataFrame(redFlipped, columns=columns,\n",
        "                       index=index)\n",
        "    \n",
        "    red.to_excel(writer, sheet_name='Red')\n",
        "\n",
        "    # Green\n",
        "    green = pd.DataFrame(greenFlipped, columns=columns,\n",
        "                       index=index)\n",
        "    \n",
        "    green.to_excel(writer, sheet_name='Green')\n",
        "\n",
        "    # Parameters\n",
        "    data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{accuracy}',\n",
        "                      f'{valSet_size}', f'{batch_size}', f'{epochs}',\n",
        "                      f'{PENALTY}', f'{useThresholdPredict}',\n",
        "                      f'{MIN_GREEN_CERT}', f'{useBalanceDataset}']}\n",
        "\n",
        "    index = ['model','dataset','n','accuracy','valSet_size','batch_size',\n",
        "             'epochs','penalty','useThresholdPredict','min_green_cert',\n",
        "             'useBalanceDataset']\n",
        "\n",
        "    parameters = pd.DataFrame(data, index=index)\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation sets\n",
        "    validationSets = pd.DataFrame(validationSets)\n",
        "    validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "    # Collected\n",
        "    index = [i/accuracy for i in range(accuracy)]\n",
        "    index.reverse()  # Reverse index for correct orientation in Excel\n",
        "    columns = [i/accuracy for i in range(accuracy)]  \n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}Collected_Misclass_Per_Square_' +\n",
        "                            f'{CURRENT_SET}_{model.name}_' +\n",
        "                            f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "    \n",
        "    # Iterate over all training/validation runs\n",
        "    for i in range(n):\n",
        "      # Flip for correct orientation in Excel\n",
        "      totalCollectedFlipped = np.flipud(total_misclass_collected[i])\n",
        "      redCollectedFlipped = np.flipud(red_misclass_collected[i])\n",
        "      greenCollectedFlipped = np.flipud(green_misclass_collected[i])\n",
        "      \n",
        "      # Total \n",
        "      total = pd.DataFrame(totalCollectedFlipped, columns=columns,\n",
        "                          index=index)\n",
        "      \n",
        "      total.to_excel(writer, sheet_name=f'total_{i}')\n",
        "\n",
        "      # Red\n",
        "      red = pd.DataFrame(redCollectedFlipped, columns=columns,\n",
        "                        index=index)\n",
        "      \n",
        "      red.to_excel(writer, sheet_name=f'red_{i}')\n",
        "\n",
        "      # Green\n",
        "      green = pd.DataFrame(greenCollectedFlipped, columns=columns,\n",
        "                        index=index)\n",
        "      \n",
        "      green.to_excel(writer, sheet_name=f'green_{i}')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return (avgTotalMisclassPerSquare, avgRedMisclassPerSquare,\n",
        "          avgGreenMisclassPerSquare, total_misclass_collected,\n",
        "          red_misclass_collected, green_misclass_collected)\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy2HWZscTuIm",
        "cellView": "form"
      },
      "source": [
        "#@title Weighted Misclassification Probability\n",
        "def weightedMisclassProbability(distributionMap, misclassPerSquare, model,\n",
        "                                specific_color=None, verbose=1, colorbarLim=-1,\n",
        "                                drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Calculates the probability that the next point will be misclassified\n",
        "    by weighting the misclassification probaility of each square with the\n",
        "    probability distribution of the datset.\n",
        "  \n",
        "  Args:\n",
        "    distributionMap: 2-D np.array of shape (x,x)\n",
        "      The distribution map of the dataset.\n",
        "    misclassPerSquare: 6-tuple of 2-D np.array of shape (x,x) or str\n",
        "      (at, ar, ag, ct, cr, cg) misclassification probabilities per square as\n",
        "      floats between 0 and 1 or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    model: keras.model\n",
        "      The model used for calculating misclassPerSquare (only used for naming\n",
        "      files here).\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, calculates weighted green misclassification probability. If 1,\n",
        "      analogously for red. If none given, total misclassification probability is\n",
        "      calculated.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to show the plot or not.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum\n",
        "      misclassification probability is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    Float between 0 and 1. The total probability that the next point will be\n",
        "    misclassified.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if distributionMap.shape != misclassPerSquare.shape.\n",
        "    TypeError: if specific_color is not 0, 1, or None.\n",
        "  \"\"\"\n",
        "  if specific_color != None and specific_color != 0 and specific_color != 1:\n",
        "    raise TypeError(f'specific_color should be 0, 1, or None, but is ' +\n",
        "                    f'{specific_color}.')\n",
        "    \n",
        "  # Getting data from Excel sheet\n",
        "  if type(misclassPerSquare) == str:\n",
        "    data = pd.ExcelFile(misclassPerSquare)\n",
        "    if specific_color == None:\n",
        "      data = pd.read_excel(data, sheet_name='Total', index_col=0)\n",
        "    elif specific_color == 1:\n",
        "      data = pd.read_excel(data, sheet_name='Red', index_col=0)\n",
        "    elif specific_color == 0:\n",
        "      data = pd.read_excel(data, sheet_name='Green', index_col=0)\n",
        "    \n",
        "    misclassPerSquare = np.array(data)\n",
        "    misclassPerSquare = np.flipud(misclassPerSquare)\n",
        "  \n",
        "  # Getting selected color from tuple\n",
        "  if type(misclassPerSquare) == tuple:\n",
        "    if specific_color == None:\n",
        "      misclassPerSquare = misclassPerSquare[0]\n",
        "    elif specific_color == 1:\n",
        "      misclassPerSquare = misclassPerSquare[1]\n",
        "    elif specific_color == 0:\n",
        "      misclassPerSquare = misclassPerSquare[2]\n",
        "    \n",
        "  # Checking shapes\n",
        "  if distributionMap.shape != misclassPerSquare.shape:\n",
        "    raise TypeError(f'distributionMap.shape and misclassPerSquare.shape must ' + \n",
        "                    f'be equal. distributionMap.shape is ' +\n",
        "                    f'{distributionMap.shape} and misclassPerSquare.shape is ' +\n",
        "                    f'{misclassPerSquare.shape}.')\n",
        "\n",
        "  weightedMap = distributionMap*misclassPerSquare\n",
        "\n",
        "  result = np.sum(weightedMap)\n",
        "\n",
        "  if specific_color == None:\n",
        "    char = ''\n",
        "    color = 'total'\n",
        "    prefix = 'Total_'\n",
        "  elif specific_color == 0:\n",
        "    char = 'g '\n",
        "    color = 'green'\n",
        "    prefix = 'Green_'\n",
        "  elif specific_color == 1:\n",
        "    char = 'r '\n",
        "    color = 'red'\n",
        "    prefix = 'Red_'\n",
        "\n",
        "  # Plotting\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Weighted {char}misclass probability in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(weightedMap)\n",
        "\n",
        "    plt.imshow(weightedMap, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    config_info = (f'Total: {round(result*100,2)}%')\n",
        "    ax.text(1.05, 1.03, config_info, weight='bold')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  today = date.today()\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Weighted_{prefix}Misclass_Prob_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Weighted_{prefix}Misclass_Prob_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "  \n",
        "  print(f'Weighted {color} misclassification probability: {round(result*100,2)}%')\n",
        "\n",
        "  return result"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjTBuhEJ65e5",
        "cellView": "form"
      },
      "source": [
        "#@title Calculate Upper Bounds\n",
        "def calculateUpperBounds(distributionMap, misclassPerSquare, n, proportionOfRuns,\n",
        "                         model, verbose=1, saveToExcel=False, path=''):\n",
        "  \"\"\"Calculates the upper bounds for total, red, and green misclassification\n",
        "    probability fulfilled by a given proportion of runs. \n",
        "\n",
        "  Args:\n",
        "    distributionMap: 2-D np.array of shape (x,x)\n",
        "      The distribution map of the dataset.\n",
        "    misclassPerSquare: 6-tuple of 2-D np.array of shape (x,x) or str\n",
        "      (at, ar, ag, ct, cr, cg) misclassification probabilities per square as\n",
        "      floats between 0 and 1 or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    n: int\n",
        "      The number of training/prediction rounds. (The number of misclassPerSquare\n",
        "      maps collected in misclassPerSquare).\n",
        "    proportionOfRuns: float between 0 and 1\n",
        "      The proportion (as a float) of runs which must fulfill the upper bound.\n",
        "    model: keras.model\n",
        "      The model used for calculating misclassPerSquare (only used for naming\n",
        "      files here).\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to print the results or not.\n",
        "    saveToExcel: boolean, optional\n",
        "      Whether to save the calculated upper bounds to Excel or not.\n",
        "    path: str, optional\n",
        "      Path to which the sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of floats (t,r,g) containing the upper bounds for total, red, and\n",
        "    green misclassification probability fulfilled by the given proportion of\n",
        "    runs.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if distributionMap.shape is not equal to shape of each\n",
        "    misclassPerSquare array/sheet.\n",
        "  \"\"\"\n",
        "  # Preparing data arrays\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "  total_misclass_probs = []\n",
        "  red_misclass_probs = []\n",
        "  green_misclass_probs = []\n",
        "\n",
        "  # Getting data from Excel sheet\n",
        "  if type(misclassPerSquare) == str:\n",
        "    data = pd.ExcelFile(misclassPerSquare)\n",
        "    for i in range(n):\n",
        "      data_t = pd.read_excel(data, sheet_name=f'total_{i}', index_col=0)\n",
        "      data_t = np.array(data_t)\n",
        "      total_misclass_collected.append(np.flipud(data_t))\n",
        "      \n",
        "      data_r = pd.read_excel(data, sheet_name=f'red_{i}', index_col=0)\n",
        "      data_r = np.array(data_r)\n",
        "      red_misclass_collected.append(np.flipud(data_r))\n",
        "\n",
        "      data_g = pd.read_excel(data, sheet_name=f'green_{i}', index_col=0)\n",
        "      data_g = np.array(data_g)\n",
        "      green_misclass_collected.append(np.flipud(data_g))\n",
        "    \n",
        "  # Getting selected color from tuple\n",
        "  if type(misclassPerSquare) == tuple:\n",
        "      total_misclass_collected = misclassPerSquare[3]\n",
        "      red_misclass_collected = misclassPerSquare[4]\n",
        "      green_misclass_collected = misclassPerSquare[5]\n",
        "      \n",
        "  # Checking for correct shapes\n",
        "  if distributionMap.shape != total_misclass_collected[0].shape:\n",
        "    raise TypeError(f'distributionMap.shape and shape of misclassPerSquare ' +  \n",
        "                    f'arrays must be equal. distributionMap.shape is ' +\n",
        "                    f'{distributionMap.shape} and shape of misclassPerSquare ' +\n",
        "                    f'is {total_misclass_collected[0].shape}.')\n",
        "    \n",
        "  # Calculating misclassification probabilities for each run and color\n",
        "  for i in range(n):\n",
        "    t_prob = (np.sum(distributionMap*total_misclass_collected[i]))\n",
        "    total_misclass_probs.append(t_prob)\n",
        "\n",
        "    r_prob = (np.sum(distributionMap*red_misclass_collected[i]))\n",
        "    red_misclass_probs.append(r_prob)\n",
        "\n",
        "    g_prob = (np.sum(distributionMap*green_misclass_collected[i]))\n",
        "    green_misclass_probs.append(g_prob)\n",
        "\n",
        "  # Calculating upper bounds\n",
        "  total_misclass_probs = np.sort(total_misclass_probs)\n",
        "  red_misclass_probs = np.sort(red_misclass_probs)\n",
        "  green_misclass_probs = np.sort(green_misclass_probs)\n",
        "  \n",
        "  target = math.ceil(n*proportionOfRuns)\n",
        "\n",
        "  res_t = total_misclass_probs[target-1]\n",
        "  res_r = red_misclass_probs[target-1]\n",
        "  res_g = green_misclass_probs[target-1]\n",
        "\n",
        "  max_t = total_misclass_probs[-1]\n",
        "  max_r = red_misclass_probs[-1]\n",
        "  max_g = green_misclass_probs[-1]\n",
        "  \n",
        "  res = pd.DataFrame([[res_t, max_t],\n",
        "                      [res_r, max_r],\n",
        "                      [res_g, max_g]],\n",
        "                     index=['total','red','green'],\n",
        "                     columns=[f'Fulfilled by {round(proportionOfRuns*100,2)}%',\n",
        "                              f'Fulfilled by 100%'])\n",
        "  \n",
        "  # Save to Excel\n",
        "  today = date.today()\n",
        "  if saveToExcel == True:\n",
        "    res.to_excel(f'{path}Upper_Bounds_{CURRENT_SET}_{model.name}_' +\n",
        "                 f'{today.strftime(\"%d-%m-%Y\")}.xlsx', sheet_name='Upper bounds')\n",
        "  \n",
        "  if verbose > 0:\n",
        "    print(res)\n",
        "  \n",
        "  return (res_t, res_r, res_g)\n",
        "    "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3YU__k-8CfW"
      },
      "source": [
        "# Magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR2BPwsfkBj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff60019-770f-4ded-8d77-bef4ccc309ae"
      },
      "source": [
        "# Data Preparation\n",
        "dataSet = getDataSet()\n",
        "dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet, validationIndices=VAL_INDICES)\n",
        "\n",
        "dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA)\n",
        "\n",
        "#Creating tensors\n",
        "training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "training_points = np.array(dataSet[['x_i1','x_i2']])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificially exended by 12257 points\n",
            "Relation is now: 0.7 green : 0.3 red \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuey_TJrkBj8"
      },
      "source": [
        "# Configure and compile model\n",
        "initalizer = keras.initializers.GlorotNormal()\n",
        "\n",
        "model_0 = keras.Sequential([\n",
        "          keras.layers.Flatten(input_shape=(2,)),      #input layer: 2 neurons\n",
        "          keras.layers.Dense(100,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(70,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(50,activation='relu', kernel_initializer=initalizer),       \n",
        "          keras.layers.Dense(10,activation='relu', kernel_initializer=initalizer),\n",
        "          keras.layers.Dense(2,activation='softmax', kernel_initializer=initalizer)   #output layer: 2 neurons              \n",
        "          ], name=\"model_0\")\n",
        "\n",
        "model_0.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Save initial weights\n",
        "initialWeights = model_0.get_weights()\n",
        "\n",
        "# Fit model\n",
        "history = model_0.fit(training_points, training_labels, batch_size=1800, epochs=100,\n",
        "                    shuffle=True, validation_data=(valSet_points, valSet_labels))\n",
        "clear_output()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iluuGabGaWPj",
        "outputId": "9e6ee12e-649c-4c19-ffd5-2efc152e73a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "source": [
        "averageBalanceEffect(model_0, 1, 8000, interval=(0, 0.4), accuracy=4, batch_size=1800, epochs=2000)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken: 23.26 minutes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.1875, 1.4   , 1.05  , 1.075 , 1.025 ]),\n",
              " array([2.78514589, 8.62068966, 1.85676393, 4.37665782, 4.11140584]),\n",
              " array([1.02125311, 0.64863373, 0.96605023, 0.73143803, 0.7038366 ]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    }
  ]
}