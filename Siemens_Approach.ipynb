{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siemens_Approach.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericbill21/siemens/blob/master/Siemens_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppIOSk-I8wV2"
      },
      "source": [
        "# Imports, Config & GPU Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeg6hj5kBjI"
      },
      "source": [
        "# Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Arithmetic Operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Data visualization\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Progress calculation\n",
        "import sys\n",
        "import time\n",
        "from datetime import date\n",
        "\n",
        "# Time prediciton\n",
        "PREV_TIME = 0\n",
        "PB_START_TIME = 0\n",
        "\n",
        "# Mounting Google drive\n",
        "from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd5eTUqAkBjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4aeadce-5c06-48b9-c709-6528c0626351"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# ram_gb = virtual_memory().total / 1e9\n",
        "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "# if ram_gb < 20:\n",
        "#   print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "#   print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "#   print('re-execute this cell.')\n",
        "# else:\n",
        "#   print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr 23 11:55:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjZxd7_8MKI"
      },
      "source": [
        "# Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRjXSfIYkBjM"
      },
      "source": [
        "# Dictionaries\n",
        "COLORS = {0 : 'green', 1 : 'red', 'green' : 0, 'red' : 1}\n",
        "SOURCES = {'A' : 'https://drive.google.com/file/d/1hAzAKZNpmSclSI7HnV_cRjpMS4Kh5r1q/view?usp=sharing', 'B' : 'https://drive.google.com/file/d/12VlecL-5iYs-BFpnTOba1x65jWofBX1P/view?usp=sharing', 'C' : 'https://drive.google.com/file/d/1-Z0RuJIi1cZcqrrmV6TqT0O1PwI2OiBY/view?usp=sharing'}\n",
        "SOURCE_SIZE = {'A': 1000,'B' : 5000, 'C' : 50000}\n",
        "\n",
        "CURRENT_SET = 'C'\n",
        "\n",
        "# Balancing dataset to threshold\n",
        "THRESHOLD_DATA = 0.4\n",
        "\n",
        "# Threshold for balanced validation set\n",
        "THRESHOLD_VAL = 0.4\n",
        "\n",
        "# Minimum certainty required to predict green\n",
        "MIN_GREEN_CERT = 0.9\n",
        "\n",
        "# Random number seed\n",
        "random.seed(time.time())\n",
        "\n",
        "subsetA = random.sample(range(1000), 150)\n",
        "subsetB = random.sample(range(5000), 800)\n",
        "subsetC = random.sample(range(50000), 8000)\n",
        "\n",
        "VAL_INDICES = locals()['subset' + CURRENT_SET]\n",
        "\n",
        "# Penalty applied to false green classifications in custom loss function\n",
        "PENALTY = 0.2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVhVxxhc5T-a"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC174oUT5utW",
        "cellView": "form"
      },
      "source": [
        "#@title Data Operations\n",
        "\n",
        "def getDataSet(dataset=CURRENT_SET):\n",
        "  \"\"\"Returns pandas.DataFrame of dataset.\n",
        "  \n",
        "  Args:\n",
        "    dataset: char, optional\n",
        "      The dataset to return. 'A', 'B', or 'C'.\n",
        "  \"\"\"\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+SOURCES[dataset].split('/')[-2]\n",
        "  return pd.read_excel(path)\n",
        "\n",
        "def separateValidationSet(dataSet, validationIndices):\n",
        "  \"\"\"Separates a subset of points from dataSet as validation points.\n",
        "\n",
        "  Validation points are extracted and deleted from dataSet to be used for\n",
        "  validation later on.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset which the\n",
        "      validation points are extracted from.\n",
        "    validationIndices: 1-D list of ints\n",
        "      The elements corresponding to these indices are extracted from dataSet.\n",
        "\n",
        "  Returns:\n",
        "    2-tuple of the form (valSet_points, valSet_labels), where valSet_points\n",
        "    is a np.array of shape (x,2) and valSet_labels is a np.array of shape (x,1).\n",
        "  \"\"\"\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be \\\n",
        "      {pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if len(np.array(validationIndices).shape) != 1:\n",
        "    raise TypeError(f'The shape of the parameter validationIndices is: \\\n",
        "      {np.array(validationIndices).shape}, but it should be 1 dimensional')\n",
        "  \n",
        "  valSet_points = dataSet[['x_i1','x_i2']].loc[validationIndices]\n",
        "  valSet_labels = dataSet['l_i'].loc[validationIndices]\n",
        "  \n",
        "  # Saving the validation points\n",
        "  valSet_points = np.array(valSet_points)\n",
        "  valSet_labels = np.array(valSet_labels).astype('float')\n",
        "\n",
        "  # Removing the validation point\n",
        "  dataSet.drop(index=validationIndices, inplace=True)\n",
        "  dataSet.reset_index(inplace=True)\n",
        "\n",
        "  return (valSet_points, valSet_labels)\n",
        "\n",
        "def timeCalc():\n",
        "  \"\"\"Calculates time between previous call and current call.\n",
        "\n",
        "  Returns:\n",
        "    Time difference in minutes as float.\n",
        "  \"\"\"\n",
        "  global PREV_TIME\n",
        "  if PREV_TIME == 0:\n",
        "    PREV_TIME = time.time()\n",
        "    return 0\n",
        "  \n",
        "  res = (time.time() - PREV_TIME) / 60\n",
        "  PREV_TIME = time.time()\n",
        "  return res\n",
        "\n",
        "def balanceDataset(dataSet, threshold, verbose=1):\n",
        "  \"\"\"Artificially balances dataSet by duplicating red or green points.\n",
        "\n",
        "  Args: \n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. The datset to be balanced.\n",
        "    threshold: float between 0 and 0.5\n",
        "      The function duplicates red or green points until the fraction of points\n",
        "      of the less frequent color is at least equal to the threshold.\n",
        "\n",
        "  Returns:\n",
        "    pandas.DataFrame with columns 'x_i1', 'x_i2', 'l_i1'.\n",
        "  \"\"\"\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  amount = 0\n",
        "\n",
        "  if number_of_red_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_red_points) // (1 - threshold) )\n",
        "    red_points = dataSet.loc[dataSet['l_i'] == 1] #Getting all red points\n",
        "    chosen_points = red_points.sample(amount, replace=True) #Selecting a random subset of red points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending the subset\n",
        "\n",
        "  if number_of_green_points / total_number_of_points < threshold:\n",
        "    amount = int( (threshold * total_number_of_points - number_of_green_points) // (1 - threshold) )\n",
        "    green_points = dataSet.loc[dataSet['l_i'] == 0] #Getting all green points\n",
        "    chosen_points = green_points.sample(amount, replace=True) #Selecting a random subset of green points\n",
        "    dataSet = dataSet.append(chosen_points, ignore_index=True) #appending green subset\n",
        "\n",
        "  dataSet = dataSet[['x_i1','x_i2','l_i']]\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  number_of_green_points = dataSet.loc[dataSet[\"l_i\"] == 0].shape[0]\n",
        "  number_of_red_points = dataSet.loc[dataSet[\"l_i\"] == 1].shape[0]\n",
        "\n",
        "  if verbose > 0:\n",
        "    print(f'Artificially exended by {amount} points')\n",
        "    print(f'Relation is now: {round(number_of_green_points / total_number_of_points, 2)}',\n",
        "            f'green : {round(number_of_red_points / total_number_of_points, 2)} red ')\n",
        "  \n",
        "  return dataSet\n",
        "\n",
        "def getBalancedValSetIndices(dataSet, size, threshold):\n",
        "  \"\"\"Get indices of validation points such that neither color represents\n",
        "    less than (threshold*100)% of the validation set.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset from\n",
        "      which the validation points are to be chosen.\n",
        "    size: int\n",
        "      Size of the validation set.\n",
        "    threshold: float between 0 and 1\n",
        "      Fraction of validation points which each color must at least\n",
        "      represent.\n",
        "\n",
        "  Returns:\n",
        "    1-D array of ints (indices).\n",
        "  \"\"\"\n",
        "  random.seed(time.time())\n",
        "\n",
        "  # Amount of points for each color\n",
        "  amount_g = int(random.randint(size*threshold, size*(1-threshold)))\n",
        "  amount_r = size - amount_g\n",
        "\n",
        "  # Indices of each points with the specific color\n",
        "  indices_g = np.where(dataSet['l_i'] == 0)[0]\n",
        "  indices_r = np.where(dataSet['l_i'] == 1)[0]\n",
        "\n",
        "  # Check if possible \n",
        "  if indices_g.shape[0] + indices_r.shape[0] < size:\n",
        "    raise ValueError('The requested size of the validation set is not feasible')\n",
        "\n",
        "  if indices_r.shape[0] < amount_r:\n",
        "    indices_g += amount_r - indces_r.shape[0]\n",
        "\n",
        "  if indices_g.shape[0] < amount_g:\n",
        "    indices_r += amount_g - indces_g.shape[0]\n",
        "  \n",
        "  # Randomly selceting a subset for each color\n",
        "  indices_g = np.random.choice(indices_g, amount_g)\n",
        "  indices_r = np.random.choice(indices_r, amount_r)\n",
        "\n",
        "  # Concatenate and shuffle the chosen subsets\n",
        "  indices = np.concatenate([indices_g, indices_r])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  return indices\n",
        "\n",
        "def getProportionOfMisclassification(model, val_data):\n",
        "\n",
        "  # Creating Numpy arrays from tensors\n",
        "  points = val_data[0]\n",
        "  labels = val_data[1].astype('float')\n",
        "\n",
        "  # Counting number of points for each class\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  prediction = model.predict(val_data[0])\n",
        "\n",
        "  # Determining the incorrect predictions\n",
        "  incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  # Counting the number of misclassifications\n",
        "  total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  return ((total_misclassifications/number_of_points)*100,\n",
        "          (red_misclassifications/red_points)*100,\n",
        "          (green_misclassifications/green_points)*100)\n",
        "\n",
        "\n",
        "def penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=True, verbose=0):\n",
        "\n",
        "  if increasing:\n",
        "    array_penalties = np.linspace(0, penalty, (epochs - epoch_end_of_inc) // increment)\n",
        "  else:\n",
        "    array_penalties = np.linspace(penalty, 0, (epochs - epoch_end_of_inc) // increment)\n",
        "\n",
        "  for i in range((epochs - epoch_end_of_inc) // increment):\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(array_penalties[i]), metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(training_points, training_labels, batch_size=batch_size, epochs=increment,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "\n",
        "  model.fit(training_points, training_labels, batch_size=batch_size, epochs=epochs - epoch_end_of_inc,\n",
        "                  shuffle=True, verbose=verbose)\n",
        "  \n",
        "\n",
        "def thresholdPredict(data, model, threshold):\n",
        "  \"\"\"Generates output predictions for the input samples. Points are only\n",
        "    predicted as green if the model's certainty for green is > threshold. All \n",
        "    other points are predicted red.\n",
        "\n",
        "  Args:\n",
        "    data: array-like, tensors, tf.data dataset...\n",
        "      Input samples.\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    threshold: float between 0.5 and 1\n",
        "      The minimum certainty required for the network to predict a point as green.\n",
        "\n",
        "  Returns:\n",
        "    Numpy array(s) of predictions.\n",
        "  \"\"\"\n",
        "  prediction = model.predict(data)\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i,0] >= 0.5 and prediction[i,0] < threshold:\n",
        "      temp = prediction[i,0]\n",
        "      prediction[i,0] = prediction[i,1]\n",
        "      prediction[i,1] = temp\n",
        "\n",
        "  return prediction\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woke-yOw50gQ"
      },
      "source": [
        "#@title Visualisation\n",
        "\n",
        "def printProgressBar(iteration, total, prefix = '', suffix = '', decimals = 1,\n",
        "                     length = 100, fill = '█'):\n",
        "  \"\"\"Prints a progress bar.\n",
        "\n",
        "  Args:\n",
        "    iteration: int\n",
        "      Current progress step as. (iteration/total progress).\n",
        "    total: int\n",
        "      Total progress steps until completion.\n",
        "    prefix: str, optional\n",
        "      Printed infront of the progress bar.\n",
        "    suffix: str, optional\n",
        "      Printed behind ETA.\n",
        "    decimals: int, optional\n",
        "      Number of decimal places of percentage progress.\n",
        "    length: int, optional\n",
        "      Length of the progress bar in characters.\n",
        "    fill: char, optional\n",
        "      Filler of the progress bar.\n",
        "  \"\"\"\n",
        "  # Preparing strings\n",
        "  percentage_progress = (100*(iteration/float(total)))\n",
        "  percent = (\"{0:.\" + str(decimals) + \"f}\").format(percentage_progress)\n",
        "  filledLength = int(length * iteration // total)\n",
        "  bar = fill * filledLength + '-' * (length - filledLength)\n",
        "\n",
        "  # Bob's alternative time calculation\n",
        "  if iteration == 0:\n",
        "    global PB_START_TIME\n",
        "    PB_START_TIME = time.time()\n",
        "    time_so_far = 0\n",
        "    time_remaining = 0\n",
        "  else:\n",
        "    time_so_far = time.time() - PB_START_TIME\n",
        "    time_remaining = time_so_far/percentage_progress * (100-percentage_progress)\n",
        "\n",
        "  sys.stdout.write(f'\\r{prefix} |{bar}| {percent}% | ETA: {round((time_remaining/60), 2)} minutes | {suffix}')\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  # Erease progress bar on complete\n",
        "  if iteration == total:\n",
        "    global PREV_TIME\n",
        "    PREV_TIME = 0\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def makePlot(dataSet=CURRENT_SET, correct_pred_points = np.array([]),\n",
        "             incorrect_pred_points = np.array([]), drawGrid=True,\n",
        "             savePlot=False, path=''):\n",
        "  \"\"\"\"Plots green and red points and markers as scatter graph.\n",
        "  \n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame or char, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "      Dataset to be plotted.\n",
        "    correct_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing correctly predicted points. Marked as black\n",
        "      'x' on scatter graph.\n",
        "    incorrect_pred_points: 2-D list, optional \n",
        "      List of shape (x,2) containing incorrectly predicted points. Marked as\n",
        "      black '*' on scatter graph.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Raises:\n",
        "    TypeError: If dataSet is not an instance of pd.DataFrame or char or the\n",
        "    other parameters do not have the required shape.\n",
        "  \"\"\"\n",
        "  # Preparing optional parameters\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if isinstance(correct_pred_points, list):\n",
        "    correct_pred_points = np.array(correct_pred_points)\n",
        "  if isinstance(incorrect_pred_points, list):\n",
        "    incorrect_pred_points = np.array(incorrect_pred_points)\n",
        "\n",
        "  # Checking for the right type\n",
        "  if not isinstance(dataSet, pd.DataFrame):\n",
        "    raise TypeError(f'dataSet is of type: {type(dataSet)}, but should be ' +\n",
        "                    f'{pd.DataFrame}')\n",
        "\n",
        "  # Checking for the right shape \n",
        "  if (correct_pred_points.shape != (correct_pred_points.shape[0],2)\n",
        "      and np.array(correct_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter correct_pred_points is: \\\n",
        "      {np.array(correct_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  if (incorrect_pred_points.shape != (incorrect_pred_points.shape[0],2)\n",
        "      and np.array(incorrect_pred_points).shape != (0,)):\n",
        "    raise TypeError(f'The shape of the parameter incorrect_pred_points is: \\\n",
        "      {np.array(incorrect_pred_points).shape}, but it should be 2 dimensional')\n",
        "  \n",
        "  # Creating a subplot\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Scattering all points\n",
        "  x = dataSet['x_i1']\n",
        "  y = dataSet['x_i2']\n",
        "  c = [COLORS[i] for i in dataSet['l_i']] \n",
        "\n",
        "  ax.scatter(x, y, c=c)\n",
        "\n",
        "  # Adding markers to the specified points\n",
        "  if correct_pred_points.shape[0] > 0:\n",
        "    ax.scatter(correct_pred_points[:, 0], correct_pred_points[:, 1],\n",
        "              marker = \"x\", c = 'black', label='correct')\n",
        "  if incorrect_pred_points.shape[0] > 0:\n",
        "    ax.scatter(incorrect_pred_points[:, 0], incorrect_pred_points[:, 1],\n",
        "            marker = \"*\", c = 'black', label='incorrect')\n",
        "\n",
        "  if correct_pred_points.shape[0] > 0 or incorrect_pred_points.shape[0] > 0:\n",
        "    plt.legend()\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.axis('scaled')\n",
        "  ax.set_title(f'Dataset {dataSet_char}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Dataset_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMap(model, accuracy=100, specific_color=None,\n",
        "                     useThresholdPredict=False, drawGrid=True, verbose=1,\n",
        "                     savePlot=False, path=''):\n",
        "  \"\"\"Plots the prediction certainty of the model for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's preidction certainty is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the certainty map or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy is not\n",
        "      an int.\n",
        "  \"\"\"\n",
        "  # Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    array = np.array([[j/accuracy, i/accuracy] for j in range(accuracy)])\n",
        "    \n",
        "    # Predict points\n",
        "    if useThresholdPredict == True:\n",
        "      result = thresholdPredict(array, model, MIN_GREEN_CERT)\n",
        "    else:\n",
        "      result = model.predict(array)\n",
        "\n",
        "    if specific_color != None:\n",
        "      # Saving the prediction for the specified color\n",
        "      accuracy_map[i] = result[:, specific_color]\n",
        "    \n",
        "    else:\n",
        "      result = result.max(axis=1) # Getting each max value\n",
        "\n",
        "      # Normalize the values which are between 0.5 <-> 1 to 0 <-> 1\n",
        "      accuracy_map[i] = result\n",
        "  \n",
        "    # Print current progress\n",
        "    printProgressBar(i, accuracy-1)\n",
        "\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if specific_color != None:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      ax.set_title(f'Certainty for {COLORS[specific_color]} in {CURRENT_SET}')\n",
        "    else:\n",
        "      plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1,\n",
        "                 extent=[0, 1, 0, 1])\n",
        "      ax.set_title(f'Total certainty in {CURRENT_SET}')\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}CertaintyMap_{CURRENT_SET}.pdf')\n",
        "    fig.savefig(f'{path}CertaintyMap_{CURRENT_SET}.png', dpi=300)\n",
        "\n",
        "  return accuracy_map\n",
        "\n",
        "\n",
        "\n",
        "def plotLoss(history):\n",
        "  \"\"\"Plots training loss and validation loss with respect to training epochs.\n",
        "\n",
        "  Args:\n",
        "    history: keras History\n",
        "      history of keras model.\n",
        "  \"\"\"\n",
        "  if 'val_loss' in history.history:\n",
        "    plt.plot(history.history['val_loss'])\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def showPredictions(model, history, valSet_points, valSet_labels,\n",
        "                    useThresholdPredict=False, showCorrectPoints=False,\n",
        "                    drawGrid=True):\n",
        "  \"\"\"Plots the predictions for the validation points.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which performs the predictions.\n",
        "    valSet_points: 2-D array of shape (x,2)\n",
        "      Data points used for validation.\n",
        "    valSet_labels: 1-D array of shape (x,)\n",
        "      Ground truth labels of the validation points.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    showCorrectPoints: boolean, optional\n",
        "      Whether correctly classified points should be marked as black 'x' or not.\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "\n",
        "  Returns:\n",
        "    2-dimensional numpy array of shape (x,2) with the predictions for the\n",
        "    validation points.\n",
        "  \"\"\"\n",
        "  # Predict the validation points\n",
        "  if useThresholdPredict == True:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identifying correctly and incorrectly classified points\n",
        "  correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == True)\n",
        "  incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "  number_of_points = np.bincount(np.argmax(prediction, axis=1))\n",
        "\n",
        "  total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "  red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "  green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "  #Average misclassification certainty\n",
        "  misclass_certainties = []\n",
        "  for i in incorrect_indices[0]:\n",
        "    misclass_certainties.append(np.max(prediction[i]))\n",
        "  avg_misclass_certainty = sum(misclass_certainties)/total_misclassifications\n",
        "  \n",
        "  valAccuracy = 100 - (total_misclassifications/sum(number_of_points))*100\n",
        "\n",
        "  print('Validation accuracy: {:.2f}%'.format(valAccuracy))\n",
        "  print(f'Predictions for green: {number_of_points[0]} / {len(valSet_labels)}')\n",
        "  print(f'Predictions for red: {number_of_points[1]} / {len(valSet_labels)}')\n",
        "  print(f'Points misclassified: {total_misclassifications}')\n",
        "  print(f'Red points misclassified: {red_misclassifications}')\n",
        "  print(f'Green points misclassified: {green_misclassifications}')\n",
        "  print('Average misclassification certainty: {:.2f}'.format(avg_misclass_certainty))\n",
        "\n",
        "  if showCorrectPoints:\n",
        "    makePlot(correct_pred_points=valSet_points[correct_indices],\n",
        "           incorrect_pred_points=valSet_points[incorrect_indices], \n",
        "           drawGrid=drawGrid)\n",
        "  else:\n",
        "    makePlot(incorrect_pred_points=valSet_points[incorrect_indices], \n",
        "             drawGrid=drawGrid)\n",
        "    \n",
        "  # Make bar graph showing red and green misclassifications\n",
        "  bars = ('Red', 'Green')\n",
        "  height = [red_misclassifications, green_misclassifications]\n",
        "  x_pos = np.arange(len(bars))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.bar(x_pos, height, width=0.35, color=['red', 'green'])\n",
        "\n",
        "  ax.set_ylabel('Misclassifications')\n",
        "  ax.set_title('Misclassifications by color')\n",
        "  ax.set_xticks(x_pos)\n",
        "  ax.set_xticklabels(bars)\n",
        "\n",
        "  rects = ax.patches # Array of bars\n",
        "\n",
        "  labels = [red_misclassifications, green_misclassifications]\n",
        "\n",
        "  for rect, label in zip(rects, labels): # Add labels above bars\n",
        "      height = rect.get_height()\n",
        "      ax.text(rect.get_x() + rect.get_width() / 2, height, label,\n",
        "              ha='center', va='bottom')\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "def makeDensityMap(accuracy, dataSet=CURRENT_SET, significance=0.1,\n",
        "                   cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                   drawGrid=True, verbose=1, savePlot=False, path=''):\n",
        "  \"\"\"Creates a heatmap of the density of dataSet.\n",
        "\n",
        "    Args:\n",
        "      accuracy: int, optional\n",
        "        Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "        of data points plotted has the dimension accuracy*accuracy.\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      verbose: 0 or 1, optional\n",
        "        Whether to plot the density map or not.\n",
        "      savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "      path: str, optional\n",
        "        Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "    Returns:\n",
        "      2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet_char = dataSet\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  density_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  printProgressBar(0, accuracy**2)\n",
        "\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      count = dataSet.loc[(dataSet['x_i1'] - j/accuracy)**2 + \n",
        "              (dataSet['x_i2'] - i/accuracy)**2 <= significance**2]\n",
        "      \n",
        "      density_map[i,j] = len(count)\n",
        "\n",
        "      printProgressBar(i*accuracy + j + 1, accuracy**2)\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(density_map),vmax=np.max(density_map))\n",
        "\n",
        "  # Plotting \n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    if specific_color != None:\n",
        "      ax.set_title(f'Density of {COLORS[specific_color]} in {dataSet_char}')\n",
        "    else:\n",
        "      ax.set_title(f'Total density in {dataSet_char}')\n",
        "\n",
        "    plt.imshow(density_map, origin='lower', cmap='Spectral', norm=norm,\n",
        "               extent=[0, 1, 0, 1])\n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}DensityMap_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}DensityMap_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "  return density_map\n",
        "  \n",
        "\n",
        "\n",
        "def plotDensity(dataSet=CURRENT_SET, significance=0.1,\n",
        "                cmap=plt.cm.get_cmap('Spectral'), specific_color = None,\n",
        "                drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Colorises and plots the points of dataSet according to their numbers\n",
        "    of neighbors.\n",
        "\n",
        "    Args:\n",
        "      dataSet: pandas.DataFrame or char, optional\n",
        "        Dataframe with columns 'x_i1', 'x_i2', 'l_i1' or char 'A', 'B', or 'C'.\n",
        "        Dataset to be plotted.\n",
        "      signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "      cmap: matplotlib colormap, optional\n",
        "        Is used for color coding the density of the dataset at the end.\n",
        "      specific_color: 0 or 1, optional\n",
        "        If 0, a heatmap of only green points is computed. If 1, analogously for\n",
        "        red.\n",
        "      drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "      savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "      path: str, optional\n",
        "        Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \"\"\"\n",
        "  dataSet_char = None\n",
        "  if type(dataSet) == str:\n",
        "    dataSet = getDataSet(dataSet)\n",
        "\n",
        "  if dataSet_char == None:\n",
        "    dataSet_char = CURRENT_SET\n",
        "\n",
        "  if specific_color != None:\n",
        "    dataSet = dataSet.loc[dataSet['l_i'] == specific_color]\n",
        "    dataSet.reset_index(inplace=True)\n",
        "\n",
        "  total_number_of_points = dataSet.shape[0]\n",
        "  array = np.zeros((total_number_of_points, 3))\n",
        "\n",
        "  # Counting all neighbours within a radius of significance\n",
        "  for i in range(total_number_of_points):\n",
        "    count = dataSet.loc[(dataSet['x_i1'] - dataSet['x_i1'].loc[i])**2 +\n",
        "     (dataSet['x_i2'] - dataSet['x_i2'].loc[i])**2 <= significance**2]\n",
        "\n",
        "    array[i, 0] = dataSet['x_i1'].loc[i]\n",
        "    array[i, 1] = dataSet['x_i2'].loc[i]\n",
        "    array[i, 2] = len(count)\n",
        "\n",
        "    printProgressBar(i+1, total_number_of_points)\n",
        "\n",
        "  print(f'Max: {np.max(array[:,2])}')\n",
        "  print(f'Min: {np.min(array[:,2])}')\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Used for the normalization\n",
        "  norm = plt.Normalize(vmin=np.min(array[:,2]),vmax=np.max(array[:,2]))\n",
        "\n",
        "  # Setting parameters for ploting\n",
        "  plt.scatter(array[:, 0], array[:, 1], c=array[:, 2], cmap=cmap, norm=norm)\n",
        " \n",
        "  ax.set_title(f'Density of dataset {dataSet_char}')\n",
        "  plt.axis('scaled')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show() \n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Density_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}Density_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "\n",
        "def makeWeightedCertaintyMap(model, accuracy, significance=0.1,\n",
        "                          useThresholdPredict=False, referenceMethod='even',\n",
        "                          referenceValue=None, drawGrid=True, savePlot=False,\n",
        "                          path=''):\n",
        "  \"\"\"Plots the model's prediction certainty weighted with the density of points\n",
        "    given in the dataset.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model who's weighted prediction certainty is to be plotted.\n",
        "    accuracy: int\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid\n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    signifcance: float between 0 and 1, optional\n",
        "        Determines the radius in which neighbours are being counted for the \n",
        "        density of a particular point.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False)\n",
        "      when calculating the model's prediction certainty.\n",
        "    referenceMethod: str: 'even', 'maxDensity', or 'customValue', optional\n",
        "      The method used to calculate the reference density used. 'even'\n",
        "      calculates the density if all points in dataset were evenly spaced. \n",
        "      'maxDensity' uses the maximum density from densityMap as the reference\n",
        "      ´density. 'customValue' uses a custom reference density.\n",
        "    referenceValue: float between 0 and 1, optional\n",
        "      Defines the custom reference density when using 'customValue' reference\n",
        "      method. Leave blank otherwise. \n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if invalid referenceMethod was given.\n",
        "\n",
        "  \"\"\"\n",
        "  if not referenceMethod in ['even', 'evenSqrt', 'evenLog', 'maxDensity',\n",
        "                             'customValue']:\n",
        "   raise TypeError(f'Invalid referenceMethod given. referenceMethod should be' +\n",
        "                   f' \"even\", \"evenSqrt\", \"evenLog\", \"maxDensity\", or ' +\n",
        "                   f'\"customValue\", but \"{referenceMethod}\" was given.')\n",
        "    \n",
        "  dataSet = getDataSet()\n",
        "\n",
        "  print(f'Calculating certainty map:')\n",
        "  certaintyMap = makeCertaintyMap(model, accuracy, None, useThresholdPredict,\n",
        "                                  verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  print(f'Calculating density map:')\n",
        "  densityMap = makeDensityMap(accuracy, significance=significance, verbose=0)\n",
        "  clear_output()\n",
        "\n",
        "  if referenceMethod == 'even':\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenSqrt':\n",
        "    densityMap = np.sqrt(densityMap)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'evenLog':\n",
        "    densityMap = np.log(densityMap+1)\n",
        "\n",
        "    totalPoints = SOURCE_SIZE[CURRENT_SET]\n",
        "    neighborhoodArea = math.pi*(significance**2)\n",
        "    evenDensity = totalPoints*neighborhoodArea\n",
        "    densityMap = densityMap/evenDensity\n",
        "\n",
        "  elif referenceMethod == 'maxDensity':\n",
        "    maxDensity = np.max(densityMap)\n",
        "    densityMap = densityMap/maxDensity\n",
        "\n",
        "  elif referenceMethod == 'customValue':\n",
        "    densityMap = densityMap/referenceValue\n",
        "\n",
        "  weightedCertaintyMap = certaintyMap*densityMap\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  plt.imshow(weightedCertaintyMap, origin='lower', cmap='tab20b', vmin=0,\n",
        "             vmax=np.max(weightedCertaintyMap), extent=[0, 1, 0, 1])\n",
        "  \n",
        "  ax.set_title(f'Weighted certainty of datset {CURRENT_SET}')\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}WeightedCertaintyMap_{CURRENT_SET}.pdf')\n",
        "    fig.savefig(f'{path}WeightedCertaintyMap_{CURRENT_SET}.png', dpi=300)\n",
        "\n",
        "  return weightedCertaintyMap\n",
        "\n",
        "\n",
        "\n",
        "def makeDistributionMap(dataSet=CURRENT_SET, accuracy=10, colorbarLim=-1,\n",
        "                        drawGrid=True, savePlot=False, path=''):\n",
        "  \"\"\"Plots the distribution of dataSet.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'. Dataset who's distribution is to be plotted.\n",
        "    accuracy: int, optional\n",
        "      The distribution map is split up into accuracy*accuracy many fields.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum\n",
        "      distribution percentage is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "  \n",
        "  Returns:\n",
        "    2-D np.array of the shape (accuracy,accuracy).\n",
        "  \"\"\"\n",
        "  dataSet_char = dataSet\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  distribution_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    distribution_map[x_i2,x_i1] = distribution_map[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  distribution_map = distribution_map/len(dataSet)\n",
        "\n",
        "  # Plotting \n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_title(f'Distribution of datset {dataSet_char}')\n",
        "\n",
        "\n",
        "  if colorbarLim == -1:\n",
        "    colorbarLim = np.max(distribution_map)\n",
        "\n",
        "  plt.imshow(distribution_map, origin='lower', cmap='Spectral', vmin=0,\n",
        "             vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "  \n",
        "\n",
        "  plt.colorbar()\n",
        "  ax.set_xlabel('x_i1')\n",
        "  ax.set_ylabel('x_i2')\n",
        "  ax.set_xlim((0,1))\n",
        "  ax.set_ylim((0,1))\n",
        "  ax.set_xticks([i/10 for i in range(11)])\n",
        "  ax.set_yticks([i/10 for i in range(11)])\n",
        "  if drawGrid == True:\n",
        "    ax.grid(alpha=0.3, color='black')\n",
        "  plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}DistributionMap_{dataSet_char}.pdf')\n",
        "    fig.savefig(f'{path}DistributionMap_{dataSet_char}.png', dpi=300)\n",
        "\n",
        "  return distribution_map\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htUB_naf6hZ1"
      },
      "source": [
        "#@title Penalty Effect\n",
        "\n",
        "def calculatePenaltyEffect(model, x, y, validation_data, interval=(0,1), accuracy=10, \n",
        "                      batch_size=32, epochs=200, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to penalty.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int, optional\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  penalties = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    penalty = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=construct_custom_penalty_loss(penalty),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "    history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "    prediction = model.predict(validation_data[0])\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction, axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    penalties[i] = penalty\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, iterations+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(penalties, total_misclass_percentages, 'b', penalties, \n",
        "              red_misclass_percentages, 'r', penalties, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by penalty')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Penalty')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averagePenaltyEffect(model, n, valSet_size, path='', interval=(0,1),\n",
        "                         accuracy=10, batch_size=32, epochs=200, verbose=1):\n",
        "  \"\"\"Plots average penalty effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the penalty effect is measured and averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    All others:\n",
        "      See calculatePenaltyEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(penalties), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculatePenaltyEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "         green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotPenaltyEffect(model, data=result, interval=interval, accuracy=accuracy, n=n, \n",
        "                    valSet_size=valSet_size, batch_size=batch_size, epochs=epochs,\n",
        "                    path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}Penalty_Data_{CURRENT_SET}_{model.name}_' +\n",
        "                          f'{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=penalties).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, penalties,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{interval}', f'{accuracy}', f'{batch_size}', f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotPenaltyEffect(model, data, interval, accuracy, n, valSet_size, batch_size, epochs,\n",
        "                      dataset=CURRENT_SET, ylim=[0,10], maj_yt_incr=1,\n",
        "                      min_yt_incr=0.1, figsize=(14,10), showParameters=True,\n",
        "                      resolution=300, path=''):\n",
        "  \"\"\"Plots average penalty effect given by 'data' and saves png and pdf of plot\n",
        "    to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the penalty effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the penalty interval plotted. x is the lowest penalty,\n",
        "      y the highest.\n",
        "    accuracy: int\n",
        "      Penalty interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs:\n",
        "      Parameters used for training and calculaing the average penalty effect.\n",
        "      Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the penalty effect was measured on. 'A', 'B' or 'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Penalties to be plotted on the x-axis\n",
        "  penalties = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(penalties, total_misclass_percentages_avg, 'b', penalties, \n",
        "            red_misclass_percentages_avg, 'r', penalties,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by penalty',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Penalty', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(penalties)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}Penalty_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpYD2sOxkBjP",
        "cellView": "form"
      },
      "source": [
        "#@title k-Nearest-Neighbour\n",
        "\n",
        "def KNN(dataSet, point, k, significance=0.1, increment=0.05, show_plot=True):\n",
        "  \"\"\" K-nearest neighbor classifier.\n",
        "\n",
        "  Statistical classifier. Uses the k nearest neighbors to predict the color of a\n",
        "  given point by comparing the number of neighbours of each color and weigthing\n",
        "  them with their squared distance to the point.\n",
        "\n",
        "  Args:\n",
        "    dataSet: pandas.DataFrame, optional\n",
        "      Dataframe with columns 'x_i1', 'x_i2', 'l_i1'. Dataset to be used for\n",
        "      calculation. Defaults to dataset selected by CURRENT_SET.\n",
        "    point: Array in the form of [x_i1, x_i2]\n",
        "    k: Positive int\n",
        "      Number of neighbours taken into account for classification\n",
        "    significance: float between 0 and 1, optional\n",
        "      Start search radius.\n",
        "    increment: float between 0 and 1, optional\n",
        "      Amount of increment of the search radius while gathering k neighbours.\n",
        "    show_plot: boolean, optional\n",
        "      If 'True' the function plots the dataset and the selected neighbours.\n",
        "\n",
        "  Returns:\n",
        "    A 2-tuple with the predictions for each class. \n",
        "    (prediction_green, prediction_red)\n",
        "  \"\"\"\n",
        "  # Gathering points until at least k neighbours are found \n",
        "  neighb = np.array([])\n",
        "  while significance <= 1 and neighb.shape[0] < k:\n",
        "      neighb = dataSet.loc[(dataSet['x_i1'] - point[0])**2 +\n",
        "                           (dataSet['x_i2'] -point[1])**2 <= significance**2]\n",
        "      significance += increment\n",
        "  \n",
        "  # Reindexing\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "  # Removing all overhang neighbours until there are only k\n",
        "  while neighb.shape[0] > k:\n",
        "    neighb = neighb.drop(np.argmax(dist))\n",
        "    dist[np.argmax(dist)] = -1\n",
        "\n",
        "  # Reindexing\n",
        "  dist = np.zeros(neighb.shape[0])\n",
        "  neighb = neighb.reset_index()\n",
        "\n",
        "  # Calculating the distances of each neighbour to the target point\n",
        "  for i in range(neighb.shape[0]):\n",
        "    dist[i] = (neighb['x_i1'].loc[i] - point[0])**2 + (neighb['x_i2'].loc[i] -\n",
        "                                                       point[1])**2\n",
        "\n",
        "\n",
        "  pred_g = 0\n",
        "  pred_r = 0\n",
        "\n",
        "  # Sum the neighbours of each color with the weight 1-dist^2 \n",
        "  for i in range(neighb.shape[0]):\n",
        "    if neighb['l_i'].loc[i] == 0:\n",
        "      pred_g += (1 - dist[i])\n",
        "    elif neighb['l_i'].loc[i] == 1:\n",
        "      pred_r += (1 - dist[i])\n",
        "\n",
        "  # Normalize\n",
        "  pred_g = pred_g / neighb.shape[0]\n",
        "  pred_r = pred_r / neighb.shape[0]\n",
        "\n",
        "  # Plot neighbours \n",
        "  if show_plot:\n",
        "    selected_neighb = [[neighb['x_i1'].loc[i], neighb['x_i2'].loc[i]]\n",
        "                       for i in range(neighb.shape[0])]\n",
        "    makePlot(dataSet, [point], selected_neighb)\n",
        "    print(f'Prediction for green: \\t{pred_g}')\n",
        "    print(f'Prediction for red: \\t{pred_r}')\n",
        "\n",
        "  return (pred_g, pred_r)\n",
        "\n",
        "\n",
        "\n",
        "def makeCertaintyMapKNN(k, accuracy = 100, specific_color = None):\n",
        "  \"\"\"Visualizes the prediction certainty of K-nearest-neighbour algorithm for a grid of data points.\n",
        "\n",
        "  All data points have x and y values between 0 and 1. \n",
        "\n",
        "  Args:\n",
        "    k: postive int\n",
        "      The number of neighbours specified for the KNN algorithm who's certainty\n",
        "      is to bevisualized.\n",
        "    accuracy: positive int, optional\n",
        "      Data points are spaced 1/accuracy apart along the x and y axis. The grid \n",
        "      of data points plotted has the dimension accuracy*accuracy.\n",
        "    specific_color: 0 or 1, optional\n",
        "      If 0, plots the model's certainty that a data point is green for all\n",
        "      points in the grid. If 1, analogously for red. \n",
        "\n",
        "  Raises:\n",
        "    TypeError: If specific_color is not 'None', '0' or '1', or if accuracy and k\n",
        "    is not an int.\n",
        "  \"\"\"\n",
        "  #Exceptions\n",
        "  if specific_color != None:\n",
        "    if specific_color != 0 and specific_color != 1:\n",
        "      raise TypeError(f'Invalid value for specific_color. Value is {specific_color}, \\\n",
        "        but should be \"None\", \"0\" or \"1\".')\n",
        "\n",
        "  if not isinstance(accuracy, int):\n",
        "    raise TypeError(f'Invalid type for accuracy. Type is {type(accuracy)}, but \\\n",
        "      should be int.')\n",
        "    \n",
        "  if not isinstance(k, int):\n",
        "    raise TypeError(f'Invalid type for k. Type is {type(k)}, but \\\n",
        "      should be int.')\n",
        "\n",
        "  # Init Data\n",
        "  dataSet = getDataSet()\n",
        "  accuracy_map = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Main Loop\n",
        "  for i in range(accuracy):\n",
        "    for j in range(accuracy):\n",
        "      result = KNN(dataSet, [j/accuracy, i/accuracy], k, show_plot=False)\n",
        "\n",
        "      if specific_color != None:\n",
        "        # Saving the prediction for the specified color\n",
        "        accuracy_map[i,j] = result[specific_color]\n",
        "      else:\n",
        "        accuracy_map[i,j] = np.max(result)\n",
        "    \n",
        "      # Print current progress\n",
        "      printProgressBar((j+1) + i*accuracy, accuracy**2)\n",
        "\n",
        "  # Choosing headline\n",
        "  if specific_color != None:\n",
        "    plt.title(f'Certaintiy for {COLORS[specific_color]}')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0, vmax=1)\n",
        "\n",
        "  else:\n",
        "    plt.title(f'General Certainty')\n",
        "    plt.imshow(accuracy_map, origin='lower', cmap='tab20b', vmin=0.5, vmax=1)\n",
        "\n",
        "  # Plot\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('x_i1')\n",
        "  plt.ylabel('x_i2')\n",
        "  plt.xticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.yticks([i for i in range(0, accuracy+1, accuracy//10)], [i/accuracy for i in range(0, accuracy+1, accuracy//10)])\n",
        "  plt.show()\n",
        "  \n",
        "  return accuracy_map\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tepNATmlkBjy",
        "cellView": "form"
      },
      "source": [
        "#@title Epoch Batch Size\n",
        "\n",
        "def epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    saveAndPlot=True, path='', verbose=1):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number for a random validation set on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model which classifies the validation set.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation set.\n",
        "    batchRange: 2-tuple of ints\n",
        "      The range of batch sizes used. (x,y) where x is the smallest and y is the\n",
        "      largest batch size used. \n",
        "    batchIncrements: int\n",
        "      Increment in which the batch size is increased.\n",
        "    epochRange: 2-tuple of ints\n",
        "      The range of epochs used. (x,y) where x is the smallest and y is the\n",
        "      largest epoch number used.\n",
        "    epochIncrements: int\n",
        "      Increment in which the epoch number is increased.\n",
        "    epsilon: float, optional\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "    saveAndPlot: boolean, optional\n",
        "      Whever to save results to Excel and plot graphs or not. Set to false when\n",
        "      using averageEpochsBatchSize.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print a progress bar or not.\n",
        "\n",
        "  Returns:\n",
        "    6-tuple (epochs, batch_sizes, total_misclass_percentage,\n",
        "    red_misclass_percentage, green_misclass_percentage, valSet).\n",
        "    First 5 elements are lists, valSet is pd.DataFrame.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  if verbose > 0:\n",
        "    start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs = []\n",
        "  batch_sizes = []\n",
        "  total_misclass_percentage = []\n",
        "  red_misclass_percentage = []\n",
        "  green_misclass_percentage = []\n",
        "\n",
        "  # Defining iteration lists\n",
        "  batch_size_iter = np.arange(batchRange[0], batchRange[1]+1, batchIncrements)\n",
        "  epoch_iter = np.arange(epochRange[0], epochRange[1]+1, epochIncrements)\n",
        "\n",
        "  if batch_size_iter[0] == 0:\n",
        "    batch_size_iter[0] = 1\n",
        "\n",
        "  # Preparing data\n",
        "  dataSet = getDataSet()\n",
        "  dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "  # Choose random validation set\n",
        "  random.seed(time.time())\n",
        "  val_indices = getBalancedValSetIndices(dataSet, valSet_size, THRESHOLD_VAL)\n",
        "\n",
        "  valSet_points, valSet_labels = separateValidationSet(dataSet,val_indices)\n",
        "  \n",
        "  dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "  training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "  training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "  number_of_points = len(valSet_labels)\n",
        "  red_points = len(np.where(valSet_labels==1)[0])\n",
        "  green_points = len(np.where(valSet_labels==0)[0])\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    num_training_points = training_labels.shape[0]\n",
        "    progress = 0\n",
        "    full = 0\n",
        "    # Calculate full progress\n",
        "    for ep in epoch_iter:\n",
        "      for ba in batch_size_iter:\n",
        "        full += ep*math.ceil(num_training_points/ba)\n",
        "    # Print bar\n",
        "    printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Epoch loop\n",
        "  for ep in epoch_iter:\n",
        "    # Batch size loop\n",
        "    for ba in batch_size_iter:\n",
        "      epochs.append(ep)\n",
        "      batch_sizes.append(ba)\n",
        "\n",
        "      # Prepare model for classification\n",
        "      model.set_weights(initialWeights)\n",
        "\n",
        "      history = model.fit(x=training_points, y=training_labels, batch_size=ba, \n",
        "                          epochs=ep, verbose=0)\n",
        "\n",
        "      # Classification and saving results\n",
        "      prediction = model.predict(valSet_points)\n",
        "\n",
        "      correct_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                 == True)\n",
        "      incorrect_indices = np.where((valSet_labels == np.argmax(prediction, axis=1))\n",
        "                                   == False)\n",
        "\n",
        "      total_misclassifications = np.bincount(valSet_labels == np.argmax(prediction, axis=1))[0]\n",
        "      red_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 1)[0])\n",
        "      green_misclassifications = len(np.where(valSet_labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "      total_misclass_percentage.append((total_misclassifications/number_of_points)*100)\n",
        "      red_misclass_percentage.append((red_misclassifications/red_points)*100)\n",
        "      green_misclass_percentage.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "      # Update progress bar\n",
        "      if verbose > 0:\n",
        "        progress += ep*math.ceil(num_training_points/ba)\n",
        "        printProgressBar(progress, full, suffix=f'{progress}/{full} steps')\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "  \n",
        "  # Validation set\n",
        "  valSet = pd.DataFrame.from_dict({'x_i1':valSet_points[:,0],'x_i2':valSet_points[:,1],\n",
        "                                  'l_i':valSet_labels})\n",
        "  \n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}']}\n",
        "\n",
        "  index = ['model','dataset','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  if saveAndPlot==True:\n",
        "    today = date.today()\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}EBS_Data_{CURRENT_SET}_' + \n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "    \n",
        "    # Create multiindex for epochs and batch_sizes\n",
        "    arrays = [epochs,batch_sizes]\n",
        "\n",
        "    tuples = list(zip(*arrays))\n",
        "\n",
        "    multiindex = pd.MultiIndex.from_tuples(tuples,\n",
        "                                      names=[\"epoch\", \"batch_size\"])\n",
        "    \n",
        "    # All data\n",
        "    allData = pd.DataFrame({'total':total_misclass_percentage,\n",
        "                            'red':red_misclass_percentage,\n",
        "                            'green':green_misclass_percentage}, index=multiindex)\n",
        "    \n",
        "    allData.to_excel(writer, sheet_name='All Data')\n",
        "\n",
        "    # Optimum points\n",
        "    result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage)\n",
        "    optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "    optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "    # Parameters\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation set\n",
        "    valSet.to_excel(writer, sheet_name='Validation Set')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_percentage,\n",
        "            red_misclass_percentage, green_misclass_percentage, valSet)\n",
        "  \n",
        "  if saveAndPlot==True:\n",
        "    plotEpochsBatchSize(model, result, path=path)\n",
        "\n",
        "  return result\n",
        "  \n",
        "\n",
        "\n",
        "def calculateOptimumPoints(data, epsilon):\n",
        "  \"\"\"Calculates optimum points of epoch and batch size for total, red, and green\n",
        "    misclassification. \n",
        "\n",
        "  Args:\n",
        "    data: 5-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage) or the name of an Excel sheet present in the\n",
        "      directory as a String (e.g. 'data.xlsx').\n",
        "    epsilon: float\n",
        "      The allowed absolute percentage difference between the misclass percentage\n",
        "      of an optimum point and the minimum misclass percentage.\n",
        "\n",
        "  Returns: pd.DataFrame\n",
        "    columns: [min_misclass, epsilon, opt_misclass, opt_epoch, opt_batch,\n",
        "             t_misclass_here, r_misclass here, g_misclass_here]\n",
        "    rows: [total, red, green]\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not in correct form.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The elements of the 5-tuple data must all have the' +\n",
        "                  f' same shape. The {i+1}. element has shape {data[i].shape}' +\n",
        "                  f' and the {i+2}. element has shape {data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The elements of the 5-tuple data must all be' + \n",
        "                      f' 1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass_percentage = data[2]\n",
        "    red_misclass_percentage = data[3]\n",
        "    green_misclass_percentage = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, 'All Data')\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "\n",
        "    epochs = data['epoch']\n",
        "    batch_sizes = data['batch_size']\n",
        "    total_misclass_percentage = data['total']\n",
        "    red_misclass_percentage = data['red']\n",
        "    green_misclass_percentage = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # CALCULATE OPTIMUM POINTS\n",
        "  # For total: Finds the configuration with total misclass within epsilon of\n",
        "  #   minimum total misclass which has the lowest red misclass.\n",
        "  # For green: Finds the configuration with green misclass within epsilon of\n",
        "  #   minimum green misclass which has the lowest red misclass.\n",
        "  # For red: Finds the configuration with red misclass within epsilon of\n",
        "  #   minimum red misclass which has the lowest total misclass.\n",
        "  columns = ['min_misclass', 'epsilon', 'opt_misclass', 'opt_epoch', 'opt_batch',\n",
        "             't_misclass_here', 'r_misclass here', 'g_misclass_here']\n",
        "  rows = ['total', 'red', 'green']\n",
        "  t_considerable_indices = []\n",
        "  r_considerable_indices = []\n",
        "  g_considerable_indices = []\n",
        "\n",
        "  #Total\n",
        "  t_min = np.min(total_misclass_percentage)\n",
        "  t_opt = np.argmin(total_misclass_percentage)  # Index of optimum point for t\n",
        "  for index in range(len(total_misclass_percentage)):\n",
        "    if total_misclass_percentage[index] <= (t_min+epsilon):\n",
        "      t_considerable_indices.append(index)\n",
        "  for index in t_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[t_opt]:\n",
        "      t_opt = index\n",
        "\n",
        "  #Green\n",
        "  g_min = np.min(green_misclass_percentage)\n",
        "  g_opt = np.argmin(green_misclass_percentage)  # Index of optimum point for g\n",
        "  for index in range(len(green_misclass_percentage)):\n",
        "    if green_misclass_percentage[index] <= (g_min+epsilon):\n",
        "      g_considerable_indices.append(index)\n",
        "  for index in g_considerable_indices:  # Find point with lowest red misclass\n",
        "    if red_misclass_percentage[index] < red_misclass_percentage[g_opt]:\n",
        "      g_opt = index\n",
        "\n",
        "  #Red\n",
        "  r_min = np.min(red_misclass_percentage)\n",
        "  r_opt = np.argmin(red_misclass_percentage)  # Index of optimum point for r\n",
        "  for index in range(len(red_misclass_percentage)):\n",
        "    if red_misclass_percentage[index] <= (r_min+epsilon):\n",
        "      r_considerable_indices.append(index)\n",
        "  # Find point with lowest total misclass\n",
        "  # Only change r_opt if the improvement in total misclass is greater than the\n",
        "  #   loss in red misclass\n",
        "  for index in r_considerable_indices:  \n",
        "    if (total_misclass_percentage[index] < total_misclass_percentage[r_opt] and \n",
        "        (total_misclass_percentage[index]-total_misclass_percentage[r_opt] <\n",
        "         red_misclass_percentage[r_opt]-red_misclass_percentage[index])):\n",
        "      r_opt = index\n",
        "  \n",
        "  total_row = [t_min, epsilon, total_misclass_percentage[t_opt], epochs[t_opt],\n",
        "               batch_sizes[t_opt], total_misclass_percentage[t_opt],\n",
        "               red_misclass_percentage[t_opt], green_misclass_percentage[t_opt]]\n",
        "  red_row = [r_min, epsilon, red_misclass_percentage[r_opt], epochs[r_opt],\n",
        "               batch_sizes[r_opt], total_misclass_percentage[r_opt],\n",
        "               red_misclass_percentage[r_opt], green_misclass_percentage[r_opt]]\n",
        "  green_row = [g_min, epsilon, green_misclass_percentage[g_opt], epochs[g_opt],\n",
        "               batch_sizes[g_opt], total_misclass_percentage[g_opt],\n",
        "               red_misclass_percentage[g_opt], green_misclass_percentage[g_opt]]\n",
        "\n",
        "  return pd.DataFrame([total_row, red_row, green_row], index=rows, \n",
        "                      columns=columns)\n",
        "  \n",
        "\n",
        "def averageEpochsBatchSize(model, n, initialWeights, valSet_size, batchRange,\n",
        "                    batchIncrements, epochRange, epochIncrements, epsilon=0,\n",
        "                    path='', verbose=1):\n",
        "  \"\"\"Calculates total, red, and green % misclassification in relation to batch\n",
        "    size and epoch number averaged over n validation sets on CURRENT_SET.\n",
        "\n",
        "  Args:\n",
        "    n: int\n",
        "      Number of iterations.\n",
        "    All others:\n",
        "      See epochsBatchSize.\n",
        "\n",
        "  Returns:\n",
        "    5 tuple of lists (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "    green_misclass_avg).\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  epochs_collected = []\n",
        "  batch_sizes_collected = []\n",
        "  total_misclass_collected = []\n",
        "  red_misclass_collected = []\n",
        "  green_misclass_collected = []\n",
        "\n",
        "  # For saving in excel\n",
        "  validationSets = {}\n",
        "  misclassCollected = {}\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # Collecting misclassification percentages\n",
        "    data = epochsBatchSize(model, initialWeights, valSet_size, batchRange,\n",
        "                           batchIncrements, epochRange, epochIncrements,\n",
        "                           epsilon=0, saveAndPlot=False, verbose=0)\n",
        "\n",
        "    epochs_collected.append(data[0])\n",
        "    batch_sizes_collected.append(data[1])\n",
        "    total_misclass_collected.append(data[2])\n",
        "    red_misclass_collected.append(data[3])\n",
        "    green_misclass_collected.append(data[4])\n",
        "\n",
        "    # Adding validation set to dictionary for dataframe\n",
        "    validationSets[f'x_i1:{i}'] = data[5]['x_i1']\n",
        "    validationSets[f'x_i2:{i}'] = data[5]['x_i2']\n",
        "    validationSets[f'l_i:{i}'] = data[5]['l_i']\n",
        "\n",
        "    # Adding misclassification data to dictionary for dataframe\n",
        "    misclassCollected[f'total:{i}'] = data[2]\n",
        "    misclassCollected[f'red:{i}'] = data[3]\n",
        "    misclassCollected[f'green{i}'] = data[4]\n",
        "\n",
        "    # Update progress bar\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  epochs = np.average(epochs_collected, axis=0)\n",
        "  batch_sizes = np.average(batch_sizes_collected, axis=0)\n",
        "  total_misclass_avg = np.average(total_misclass_collected, axis=0)\n",
        "  red_misclass_avg = np.average(red_misclass_collected, axis=0)\n",
        "  green_misclass_avg = np.average(green_misclass_collected, axis=0)\n",
        "\n",
        "\n",
        "  # SAVE RESULTS TO EXCEL\n",
        "  today = date.today()\n",
        "\n",
        "  # Create multiindex for epochs and batch_sizes\n",
        "  arrays = [epochs,batch_sizes]\n",
        "\n",
        "  tuples = list(zip(*arrays))\n",
        "\n",
        "  multiindex = pd.MultiIndex.from_tuples(tuples, names=[\"epoch\", \"batch_size\"])\n",
        "\n",
        "  # Initialize writer\n",
        "  writer = pd.ExcelWriter(f'{path}Avg_EBS_Data_{CURRENT_SET}_' + \n",
        "                        f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "  # Average \n",
        "  average = pd.DataFrame({'total':total_misclass_avg,\n",
        "                          'red':red_misclass_avg,\n",
        "                          'green':green_misclass_avg}, index=multiindex)\n",
        "\n",
        "  average.to_excel(writer, sheet_name='Average')\n",
        "\n",
        "  # Collected\n",
        "  misclassCollected = pd.DataFrame(misclassCollected, index=multiindex)\n",
        "  misclassCollected.to_excel(writer, sheet_name='Collected')\n",
        "\n",
        "  # Optimum points\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  optimumPoints = calculateOptimumPoints(result, epsilon)\n",
        "  optimumPoints.to_excel(writer, sheet_name='Optimum Points')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{batchRange}', f'{batchIncrements}',\n",
        "                    f'{epochRange}', f'{epochIncrements}', f'{epsilon}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','batchRange',\n",
        "           'batchIncrements','epochRange','epochIncrements','epsilon']\n",
        "\n",
        "  parameters = pd.DataFrame(data, index=index)\n",
        "  parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  validationSets = pd.DataFrame.from_dict(validationSets)\n",
        "  validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "\n",
        "  # Plot and return results\n",
        "  result = (epochs, batch_sizes, total_misclass_avg, red_misclass_avg,\n",
        "            green_misclass_avg)\n",
        "  \n",
        "  plotEpochsBatchSize(model, result, path=path, prefix='Avg_')\n",
        "  \n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotEpochsBatchSize(model, data, dataset=CURRENT_SET,\n",
        "                        misclass_range=(0,15), figsize=(14,10), resolution=300,\n",
        "                        cmap='viridis', path='', prefix=''):\n",
        "  \"\"\"Plots a 3D graph showing the relation between epoch number, batch size,\n",
        "    and percentage misclassification.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      The model which was used for training and classification.\n",
        "    data: 6-tuple or str\n",
        "      (epochs, batch_sizes, total_misclass_percentage, red_misclass_percentage,\n",
        "      green_misclass_percentage, valSet) or the name of an Excel\n",
        "      sheet present in the directory as a String (e.g. 'data.xlsx').\n",
        "    dataset: char, optional\n",
        "      The dataset used. 'A', 'B' or 'C'.\n",
        "    misclass_range: 2-tuple, optional\n",
        "      The range of misclassification percentages plotted (limits of the z-axis).\n",
        "    figsize: 2-tuple, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    cmap: Colormap, optional\n",
        "      A colormap for the surface patches.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    prefix: str, optional\n",
        "      appended to the front of the pnd and pdf file names\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is of invalid type or shape.\n",
        "  \"\"\"\n",
        "  # DATA PREPARATION\n",
        "  if isinstance(data, tuple):\n",
        "    # Converting data to list of np arrays\n",
        "    data = list(data)\n",
        "    for i in range(5):\n",
        "      if isinstance(data[i], list):\n",
        "        data[i] = np.array(data[i])\n",
        "\n",
        "    # Checking list shapes\n",
        "    for i in range(4):\n",
        "      if data[i].shape != data[i+1].shape:\n",
        "        raise TypeError(f'The first 5 elements of the tuple data must all ' +\n",
        "                  f'have the same shape. The {i+1}. element has shape ' +\n",
        "                  f'{data[i].shape} and the {i+2}. element has shape ' +\n",
        "                  f'{data[i+1].shape}.')\n",
        "        \n",
        "    if len(data[0].shape) != 1:\n",
        "      raise TypeError(f'The first 5 elements of the tuple data must all be ' +\n",
        "                      f'1-dimensional.')\n",
        "      \n",
        "    epochs = data[0]\n",
        "    batch_sizes = data[1]\n",
        "    total_misclass = data[2]\n",
        "    red_misclass = data[3]\n",
        "    green_misclass = data[4]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    data = pd.read_excel(data, sheet_name=0, index_col=[0,1])\n",
        "\n",
        "    # Check columns for equal length\n",
        "    for col in list(data.columns):\n",
        "      if data[col].isnull().values.any():\n",
        "        raise TypeError(f'The columns of the excel sheet data are not of ' +\n",
        "                        f'equal length. Column {col} contains NAN.')\n",
        "    \n",
        "    index_list = list(data.index)\n",
        "    index_len = len(index_list)\n",
        "    \n",
        "    epochs = []\n",
        "    batch_sizes = []\n",
        "    for i in range(index_len):\n",
        "      epochs.append(index_list[i][0])\n",
        "      batch_sizes.append(index_list[i][1])\n",
        "\n",
        "    total_misclass = data['total']\n",
        "    red_misclass = data['red']\n",
        "    green_misclass = data['green']\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'data should be of type tuple or str, but is of type' + \n",
        "                    f' {type(data)}.')\n",
        "\n",
        "  # Plotting\n",
        "  fig = plt.figure(figsize=(figsize[0], figsize[1]*3))\n",
        "  # Total misclassification\n",
        "  ax_t = fig.add_subplot(3, 1, 1, projection='3d')\n",
        "  ax_t.plot_trisurf(epochs, batch_sizes, total_misclass, cmap=cmap)\n",
        "  ax_t.set_title(f'Dataset {dataset}: Total misclassification by epoch and batch size')\n",
        "  ax_t.set_xlabel('Epochs')\n",
        "  ax_t.set_ylabel('Batch size')\n",
        "  ax_t.set_zlabel('% misclassification')\n",
        "  ax_t.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Red misclassification\n",
        "  ax_r = fig.add_subplot(3, 1, 2, projection='3d')\n",
        "  ax_r.plot_trisurf(epochs, batch_sizes, red_misclass, cmap=cmap)\n",
        "  ax_r.set_title(f'Dataset {dataset}: Red misclassification by epoch and batch size')\n",
        "  ax_r.set_xlabel('Epochs')\n",
        "  ax_r.set_ylabel('Batch size')\n",
        "  ax_r.set_zlabel('% misclassification')\n",
        "  ax_r.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "  # Green misclassification\n",
        "  ax_g = fig.add_subplot(3, 1, 3, projection='3d')\n",
        "  ax_g.plot_trisurf(epochs, batch_sizes, green_misclass, cmap=cmap)\n",
        "  ax_g.set_title(f'Dataset {dataset}: Green misclassification by epoch and batch size')\n",
        "  ax_g.set_xlabel('Epochs')\n",
        "  ax_g.set_ylabel('Batch size')\n",
        "  ax_g.set_zlabel('% misclassification')\n",
        "  ax_g.set_zlim3d(misclass_range[0], misclass_range[1])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}{prefix}EBS_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdlLeaiYaqo",
        "cellView": "form"
      },
      "source": [
        "#@title Different Training Approaches\n",
        "def diffPenaltyAproach(n, val_size, model, penalty, epochs, batch_size, increment, epoch_end_of_inc, verbose=0, figsize=(14,10), path=''):\n",
        "\n",
        "  dataSet_original = getDataSet()\n",
        "  valSets = [getBalancedValSetIndices(dataSet_original, val_size, THRESHOLD_VAL) for i in range(n)]\n",
        "\n",
        "  history_1 = np.zeros((n,3))\n",
        "  history_2 = np.zeros((n,3))\n",
        "  history_3 = np.zeros((n,3))\n",
        "\n",
        "  printProgressBar(0, 3*n)\n",
        "\n",
        "  model.set_weights(initialWeights)\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels)\n",
        "    \n",
        "    history_1[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i+1, 3*n)\n",
        "\n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    model.fit(training_points, training_labels, epochs=epochs,\n",
        "                                batch_size=batch_size, shuffle=True, verbose=0)\n",
        "\n",
        "    history_2[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + n+1, 3*n)\n",
        "  \n",
        "  for i in range(n):\n",
        "    dataSet = dataSet_original.copy()\n",
        "    model.set_weights(initialWeights)\n",
        "    \n",
        "    val_data = separateValidationSet(dataSet, valSets[i])\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype(float)\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "  \n",
        "    penaltyIncreasingTraining(model, penalty, epochs, batch_size, increment, epoch_end_of_inc, training_points, training_labels, increasing=False)\n",
        "    \n",
        "    history_3[i] = getProportionOfMisclassification(model, val_data)\n",
        "    printProgressBar(i + 2*n+1, 3*n)\n",
        "  \n",
        "  clear_output()\n",
        "\n",
        "  labels = ['total', 'red', 'green']\n",
        "  y_1 = [i/n for i in np.sum(history_1, axis=0)]\n",
        "  y_2 = [i/n for i in np.sum(history_2, axis=0)]\n",
        "  y_3 = [i/n for i in np.sum(history_3, axis=0)]\n",
        "\n",
        "  x = np.arange(len(labels))  # the label locations\n",
        "  width = 0.2  # the width of the bars\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  rects1 = ax.bar(x - width, y_1, width, label='With Increment')\n",
        "  rects2 = ax.bar(x, y_2, width, label='Normal')\n",
        "  rects3 = ax.bar(x + width, y_3, width, label='With Decrement')\n",
        "\n",
        "  # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "  ax.set_ylabel('Misclassification in %')\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.legend()\n",
        "\n",
        "  fig.text(0,0, f'Dataset: {CURRENT_SET}, Epochs: {epochs}, Batch Size: {batch_size}, Epoch Increment: {increment}, Epoch end of Increment: {epoch_end_of_inc}')\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Saving\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}Comparison{CURRENT_SET}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bljczC5kBj7",
        "cellView": "form"
      },
      "source": [
        "#@title Custom Loss Function\n",
        "\n",
        "def construct_custom_penalty_loss(penalty,\n",
        "                                  lossFunction=keras.losses.sparse_categorical_crossentropy):\n",
        "  \"\"\"Constructs a loss function which penalizes 'red as green' misclassifications. \n",
        "\n",
        "  Args:\n",
        "    penalty: float between 0 and 1\n",
        "      Value added to the loss if a red point is misclassified as green. \n",
        "    lossFunction: loss function, optional\n",
        "      The loss function used after adapting the loss values.\n",
        "\n",
        "  Returns:\n",
        "    custom_penalty_loss function with specified penalty and loss function. Can be\n",
        "    used like a regular loss function. \n",
        "  \"\"\"\n",
        "\n",
        "  def custom_penalty_loss(y_true, y_pred):\n",
        "    length = tf.shape(y_true)[0]\n",
        "\n",
        "    #Creating a vector with all values set to the penalty: [0.3, 0.3, ... 0.3]\n",
        "    error = tf.multiply(tf.constant(penalty, tf.float32), tf.ones(length)) \n",
        "\n",
        "    #Setting every entry to 0 if the corresponding entry in y_true is 1\n",
        "    error = tf.where(tf.equal(y_true[:, 0], tf.zeros(length)), error, tf.zeros(length))\n",
        "\n",
        "    #Setting every entry to 0 if the algorithm predicted 0\n",
        "    error = tf.where(tf.greater(y_pred[:, 0], y_pred[:, 1]), tf.zeros(length), error)\n",
        "\n",
        "    #Transforms the vector from [0, 0, 0.3, ... 0,3] to [[0, -0], [0, -0], [0.3, -0.3], ... [0.3, -0.3]]\n",
        "    error = tf.stack([error, tf.multiply(tf.constant(-1, tf.float32), error)], 1)\n",
        "\n",
        "    #Adding the artificial loss\n",
        "    y_pred = y_pred + error\n",
        "\n",
        "    #Eliminating values > 1 or < 0\n",
        "    y_pred0 = tf.where(tf.greater(y_pred[:, 0], tf.ones(length)), tf.ones(length), y_pred[:, 0])\n",
        "    y_pred1 = tf.where(tf.greater(y_pred[:, 1], tf.zeros(length)), y_pred[:, 1], tf.zeros(length))\n",
        "    y_pred = tf.stack([y_pred0, y_pred1], axis=1)\n",
        "\n",
        "\n",
        "    loss = lossFunction(y_pred=y_pred, y_true=y_true)\n",
        "    return loss\n",
        "  \n",
        "  return custom_penalty_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_01aKYUEy82",
        "cellView": "form"
      },
      "source": [
        "#@title Certainty Threshold Effect\n",
        "\n",
        "def calculateThresholdEffect(model, x, y, validation_data, interval=(0.8,1),\n",
        "                             accuracy=10, batch_size=64, epochs=500, verbose=0):\n",
        "  \"\"\"Calculates red, green, and total misclassifications in relation to the\n",
        "    certainty threshold for predicting points as green.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    x: 2-D array of shape (x,2)\n",
        "      Training points.\n",
        "    y: 1-D array of shape (x,)\n",
        "      Training labels.\n",
        "    validation_data: 2-tuple\n",
        "      (valSet_points, valSet_labels) where valSet_points is a 2-D array of shape\n",
        "      (x,2) and valSet_labels a 1-D array of shape (x,). Validation points and\n",
        "      labels.\n",
        "    interval: 2-tuple, optional\n",
        "      (x,y) which defines the threshold interval plotted. x is the lowest\n",
        "      threshold, y the highest.\n",
        "    accuracy: int, optional\n",
        "      Threshold interval is evenly split into 'accuracy' many points.\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar and plot results or not. \n",
        "    All others: optional\n",
        "      See tf.keras.Model.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of int lists (total_misclass_percentage, red_misclass_percentage, \n",
        "    green_misclass_percentage).\n",
        "  \"\"\"\n",
        "  total_misclass_percentages = []\n",
        "  red_misclass_percentages = []\n",
        "  green_misclass_percentages = []\n",
        "  thresholds = np.zeros(accuracy + 1)\n",
        "  increments = (interval[1]-interval[0])/accuracy\n",
        "\n",
        "  points = validation_data[0]\n",
        "  labels = validation_data[1].astype(int)\n",
        "\n",
        "  number_of_points = len(labels)\n",
        "  red_points = len(np.where(labels==1)[0])\n",
        "  green_points = len(np.where(labels==0)[0])\n",
        "\n",
        "  # Initialize and fit model\n",
        "  model.set_weights(initialWeights)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                  metrics=['accuracy']) # Compile model with penalty\n",
        "\n",
        "  history = model.fit(x, y, batch_size, epochs, verbose=0,\n",
        "                        validation_data=validation_data)\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, accuracy+1)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(accuracy+1):\n",
        "    threshold = interval[0] + (interval[1]-interval[0])*(i/accuracy)\n",
        "\n",
        "    prediction = thresholdPredict(validation_data[0], model, threshold)\n",
        "\n",
        "    correct_indices = np.where((labels == np.argmax(prediction, axis=1)) == True)\n",
        "    incorrect_indices = np.where((labels == np.argmax(prediction, axis=1)) == False)\n",
        "\n",
        "    total_misclassifications = np.bincount(labels == np.argmax(prediction,axis=1))[0]\n",
        "    red_misclassifications = len(np.where(labels[incorrect_indices] == 1)[0])\n",
        "    green_misclassifications = len(np.where(labels[incorrect_indices] == 0)[0])\n",
        "\n",
        "    total_misclass_percentages.append((total_misclassifications/number_of_points)*100)\n",
        "    red_misclass_percentages.append((red_misclassifications/red_points)*100)\n",
        "    green_misclass_percentages.append((green_misclassifications/green_points)*100)\n",
        "\n",
        "    thresholds[i] = threshold\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, accuracy+1)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  if verbose > 0:\n",
        "    plt.figure(figsize=(20,15))\n",
        "    plt.plot(thresholds, total_misclass_percentages, 'b', thresholds, \n",
        "              red_misclass_percentages, 'r', thresholds, green_misclass_percentages,\n",
        "              'g')\n",
        "    plt.title(f'Dataset {CURRENT_SET}: Misclassification by certainty threshold')\n",
        "    plt.ylabel('% misclassified')\n",
        "    plt.xlabel('Certainty threshold')\n",
        "    plt.xticks(np.arange(interval[0], interval[1]+increments, increments))\n",
        "    plt.legend(['total', 'red', 'green'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  return (total_misclass_percentages, red_misclass_percentages,\n",
        "         green_misclass_percentages)\n",
        "  \n",
        "\n",
        "def averageThresholdEffect(model, n, valSet_size, path='', interval=(0.8,1),\n",
        "                         accuracy=10, batch_size=64, epochs=500, verbose=1):\n",
        "  \"\"\"Plots average certainty threshold effect over n iterations.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    n: int\n",
        "      Number of iterations the certainty threshold effect is measured and\n",
        "      averaged over.\n",
        "    valSet_size: int\n",
        "      Size of the validation set.\n",
        "    path: str, optional\n",
        "      Path to which the excel sheet will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    verbose: boolean, optional\n",
        "      Whether to print progress bar or not.\n",
        "    All others:\n",
        "      See calculateThresholdEffect.\n",
        "\n",
        "  Returns:\n",
        "    3-tuple of np arrays (total_misclass_percentages_avg,\n",
        "    red_misclass_percentages_avg, green_misclass_percentages_avg).\n",
        "  \"\"\"\n",
        "  #Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                        (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # INITIALIZATION OF DATA COLLECTION OBJECTS\n",
        "  # For averaging\n",
        "  total_misclass_percentages_collected = []\n",
        "  red_misclass_percentages_collected = []\n",
        "  green_misclass_percentages_collected = []\n",
        "  # For saving in excel\n",
        "  validation_points_collected = np.zeros((valSet_size, 3*n))\n",
        "  misclassification_matrix = np.zeros((len(thresholds), 3*n))\n",
        "  # Column names\n",
        "  val_columns = []\n",
        "  coll_columns = []\n",
        "\n",
        "  # Initialize progress bar\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  # MAIN LOOP\n",
        "  for i in range(n):\n",
        "    # PREPARING DATA\n",
        "    dataSet = getDataSet()\n",
        "    dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "    # Choose random validation set\n",
        "    val_indices = random.sample(range(SOURCE_SIZE[CURRENT_SET]), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet,\n",
        "                                            validationIndices=val_indices)\n",
        "    dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA, verbose=0)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Collecting misclassification percentages\n",
        "    allPercentages = calculateThresholdEffect(model, training_points, training_labels,\n",
        "                                            (valSet_points, valSet_labels),\n",
        "                                            interval=interval, accuracy=accuracy,\n",
        "                                            batch_size=batch_size, epochs=epochs, \n",
        "                                            verbose=0)\n",
        "\n",
        "    total_misclass_percentages_collected.append(allPercentages[0])\n",
        "    red_misclass_percentages_collected.append(allPercentages[1])\n",
        "    green_misclass_percentages_collected.append(allPercentages[2])\n",
        "\n",
        "    # Creating separate columns for validation set\n",
        "    val_columns.append(f'x_i1:{i}')\n",
        "    val_columns.append(f'x_i2:{i}')\n",
        "    val_columns.append(f'l_i:{i}')\n",
        "\n",
        "    for j in range(valSet_size):\n",
        "      validation_points_collected[j,3*i + 0] = valSet_points[j, 0]\n",
        "      validation_points_collected[j,3*i + 1] = valSet_points[j, 1] \n",
        "      validation_points_collected[j,3*i + 2] = valSet_labels[j] \n",
        "\n",
        "    # Creating seperarte columns for current misclassification\n",
        "    coll_columns.append(f'total:{i}')\n",
        "    coll_columns.append(f'red:{i}')\n",
        "    coll_columns.append(f'green:{i}')\n",
        "\n",
        "    misclassification_matrix[:, 3*i + 0] = allPercentages[0]\n",
        "    misclassification_matrix[:, 3*i + 1] = allPercentages[1]\n",
        "    misclassification_matrix[:, 3*i + 2] = allPercentages[2]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "  # Averaging\n",
        "  total_misclass_percentages_avg = np.average(total_misclass_percentages_collected, axis=0)\n",
        "  red_misclass_percentages_avg = np.average(red_misclass_percentages_collected, axis=0)\n",
        "  green_misclass_percentages_avg = np.average(green_misclass_percentages_collected, axis=0)\n",
        "\n",
        "  result = (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "            green_misclass_percentages_avg)\n",
        "\n",
        "  # PLOTTING RESULTS\n",
        "  plotThresholdEffect(model, data=result, interval=interval, accuracy=accuracy,\n",
        "                      n=n, valSet_size=valSet_size, batch_size=batch_size,\n",
        "                      epochs=epochs, path=path)\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  end_time = time.time()\n",
        "  total_time = (end_time-start_time)/60\n",
        "  print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  # Save results to excel\n",
        "  today = date.today()\n",
        "\n",
        "  writer = pd.ExcelWriter(f'{path}CertaintyThreshold_Data_{CURRENT_SET}_' +\n",
        "                          f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "  \n",
        "  # Average misclass percentages\n",
        "  pd.DataFrame([total_misclass_percentages_avg, red_misclass_percentages_avg, \n",
        "                green_misclass_percentages_avg], ['total','red','green'], \n",
        "               columns=thresholds).to_excel(writer, sheet_name=f'Average')\n",
        "\n",
        "  # Misclass percentages collected\n",
        "  pd.DataFrame(misclassification_matrix, thresholds,\n",
        "               columns=coll_columns).to_excel(writer, sheet_name=f'Collected')\n",
        "\n",
        "  # Parameters\n",
        "  data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{valSet_size}',\n",
        "                    f'{PENALTY}', f'{interval}', f'{accuracy}', f'{batch_size}',\n",
        "                    f'{epochs}']}\n",
        "\n",
        "  index = ['model','dataset','n','valSet_size','penalty','interval','accuracy',\n",
        "           'batch_size','epochs']\n",
        "\n",
        "  pd.DataFrame(data, index=index).to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "  # Validation sets\n",
        "  pd.DataFrame(validation_points_collected,\n",
        "               columns=val_columns).to_excel(writer, sheet_name=f'Validation Sets')\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def plotThresholdEffect(model, data, interval, accuracy, n, valSet_size,\n",
        "                      batch_size, epochs, penalty=PENALTY, dataset=CURRENT_SET,\n",
        "                      ylim=[0,10], maj_yt_incr=1, min_yt_incr=0.1,\n",
        "                      figsize=(14,10), showParameters=True, resolution=300,\n",
        "                      path=''):\n",
        "  \"\"\"Plots average certainty threshold effect given by 'data' and saves png and\n",
        "    pdf of plot to the directory.\n",
        "\n",
        "  Args:\n",
        "    model: keras model\n",
        "      Model for which the certainty threshold effect is measured.\n",
        "    data: 3-tuple of np arrays, or str\n",
        "      (total_misclass_percentages_avg, red_misclass_percentages_avg,\n",
        "      green_misclass_percentages_avg) or the name of an Excel sheet present in\n",
        "      the directory as a String (e.g. 'data.xlsx').\n",
        "    interval: 2-tuple\n",
        "      (x,y) which defines the certainty threshold interval plotted. x is the\n",
        "      lowest penalty, y the highest.\n",
        "    accuracy: int\n",
        "      Certainty threshold interval is evenly split into 'accuracy' many points.\n",
        "    n, valSet_size, batch_size, epochs, penalty:\n",
        "      Parameters used for training and calculaing the average certainty\n",
        "      threshold effect. Shown in configurations text in plot.\n",
        "    dataset: char, optional\n",
        "      Dataset which the certainty penalty effect was measured on. 'A', 'B' or\n",
        "      'C'.\n",
        "    ylim: 1D list of floats or ints, optional\n",
        "      [x,y] which defines the range of % misclassification shown on the y-axis.\n",
        "    maj_yt_incr: float, optional\n",
        "      The increments in which major y-ticks are plotted on the y-axis.\n",
        "    min_yt_incr: float, optional\n",
        "      The increments in which minor y-ticks are plotted on the y-axis.\n",
        "    figsize: 2-tuple of floats, optional\n",
        "      (x,y) where x is the width of the plot and y is the height of the plot.\n",
        "    showParameters: boolean, optional\n",
        "      Whether to include a configuratiuon text in the plot or not. \n",
        "    resolution: int, optional\n",
        "      Resolution of the plot png in dpi.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if data is not of type String or 3-tuple of np arrays.\n",
        "  \"\"\"\n",
        "  # Thresholds to be plotted on the x-axis\n",
        "  thresholds = np.arange(interval[0], interval[1]+(interval[1]-interval[0])/accuracy,\n",
        "                          (interval[1]-interval[0])/accuracy)\n",
        "\n",
        "  # DATA PREPARATION\n",
        "  if (isinstance(data, tuple) and isinstance(data[0], np.ndarray) and \n",
        "      isinstance(data[1], np.ndarray) and isinstance(data[2], np.ndarray) and\n",
        "      len(data)==3):\n",
        "    total_misclass_percentages_avg = data[0]\n",
        "    red_misclass_percentages_avg = data[1]\n",
        "    green_misclass_percentages_avg = data [2]\n",
        "\n",
        "  elif isinstance(data, str):\n",
        "    data = pd.ExcelFile(data)\n",
        "    avg_data = pd.read_excel(data, 'Average')\n",
        "\n",
        "    total = pd.DataFrame(avg_data.loc[0])\n",
        "    total = total.drop('Unnamed: 0')\n",
        "    total_misclass_percentages_avg = total[0]\n",
        "\n",
        "    red = pd.DataFrame(avg_data.loc[1])\n",
        "    red = red.drop('Unnamed: 0')\n",
        "    red_misclass_percentages_avg = red[1]\n",
        "\n",
        "    green = pd.DataFrame(avg_data.loc[2])\n",
        "    green = green.drop('Unnamed: 0')\n",
        "    green_misclass_percentages_avg = green[2]\n",
        "\n",
        "  else:\n",
        "    raise TypeError(f'Invalid type of data. data should be of type String or '\n",
        "                    + f'a 3-tuple of np arrays, but data is of type {type(data)}.')\n",
        "\n",
        "  # Define yticks\n",
        "  major_yticks = np.arange(0, ylim[1]+maj_yt_incr, maj_yt_incr)\n",
        "  minor_yticks = np.arange(0, ylim[1]+min_yt_incr, min_yt_incr)\n",
        "\n",
        "  # Create subplot\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  ax.plot(thresholds, total_misclass_percentages_avg, 'b', thresholds, \n",
        "            red_misclass_percentages_avg, 'r', thresholds,\n",
        "            green_misclass_percentages_avg, 'g')\n",
        "\n",
        "  ax.set_title(f'Dataset {dataset}: Average misclassification by certainty threshold',\n",
        "               fontsize='x-large')\n",
        "  ax.set_ylabel('% misclassified', fontsize='large')\n",
        "  ax.set_xlabel('Certainty threshold', fontsize='large')\n",
        "\n",
        "  # Ranges of x and y-axis\n",
        "  ax.set_xlim(list(interval))\n",
        "  ax.set_ylim(ylim)\n",
        "\n",
        "  # Set ticks\n",
        "  ax.set_xticks(thresholds)\n",
        "  ax.set_yticks(major_yticks)\n",
        "  ax.set_yticks(minor_yticks, minor=True)\n",
        "\n",
        "  # Color and grid\n",
        "  ax.set_facecolor('white')\n",
        "  ax.grid(which='minor', alpha=0.2, color='black')\n",
        "  ax.grid(which='major', alpha=0.5, color='black')\n",
        "\n",
        "  # Show configuration information on plot\n",
        "  if showParameters==True:\n",
        "    config_info = (f'{model.name}\\nn: {n}\\nVal. set size: {valSet_size}\\n' + \n",
        "                   f'Batch size: {batch_size}\\nEpochs: {epochs}\\n' +\n",
        "                   f'Penalty: {penalty}')\n",
        "    ax.text(interval[1]+(interval[1]/(8*figsize[0])), ylim[1]-(ylim[1]/figsize[1]),\n",
        "            config_info)\n",
        "\n",
        "  plt.legend(['total', 'red', 'green'], loc='upper left', fontsize='medium')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Get current date\n",
        "  today = date.today()\n",
        "\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.png', dpi=resolution)\n",
        "  fig.savefig(f'{path}CertaintyThreshold_Plt_{dataset}_{model.name}_' +\n",
        "              f'{today.strftime(\"%d-%m-%Y\")}.pdf')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVX3rT2aStCu",
        "cellView": "form"
      },
      "source": [
        "#@title Points Per Square\n",
        "def pointsPerSquare(dataSet=CURRENT_SET, accuracy=100):\n",
        "  \"\"\"Calculates the number of points from dataSet present in each square of an\n",
        "    accuracy x accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    dataSet: char, optional\n",
        "      'A', 'B', or 'C'.\n",
        "    accuracy: int, optional\n",
        "      The grid consists of accuracy x accuracy many squares.\n",
        "\n",
        "  Returns:\n",
        "    Three 2-D np.array of the shape (accuracy,accuracy): squares, red and green.\n",
        "    squares contains the number of total points present in each square, red\n",
        "    contains the number of red points present in each square, and green contains\n",
        "    the number of green points present in each square.\n",
        "  \"\"\"\n",
        "  dataSet = getDataSet(dataSet)\n",
        "\n",
        "  squares = np.zeros((accuracy, accuracy))\n",
        "  red = np.zeros((accuracy, accuracy))\n",
        "  green = np.zeros((accuracy, accuracy))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which\n",
        "  # square each point falls into\n",
        "  dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]*accuracy\n",
        "\n",
        "  printProgressBar(0, len(dataSet))\n",
        "\n",
        "  for i in range(len(dataSet)):\n",
        "    x_i1 = math.floor(dataSet.loc[i]['x_i1'])\n",
        "    x_i2 = math.floor(dataSet.loc[i]['x_i2'])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    squares[x_i2,x_i1] = squares[x_i2,x_i1]+1\n",
        "\n",
        "    if (dataSet.loc[i]['l_i']) == 0:\n",
        "      green[x_i2,x_i1] = green[x_i2,x_i1]+1\n",
        "    else:\n",
        "      red[x_i2,x_i1] = red[x_i2,x_i1]+1\n",
        "\n",
        "    printProgressBar(i+1, len(dataSet))\n",
        "\n",
        "  return squares, red, green"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkIU-4-J-jw",
        "cellView": "form"
      },
      "source": [
        "#@title Misclassifications per square\n",
        "def misclassPerSquare(model, validationSet, accuracy=10, useThresholdPredict=False,\n",
        "                      drawGrid=True, verbose=0, savePlot=False, path='',\n",
        "                      colorbarLim=-1):\n",
        "  \"\"\"Calculates proportion of validaion points misclassified per square in an\n",
        "    accuracy*accuracy grid.\n",
        "\n",
        "  Args:\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    validationSet: 2-tuple of np.arrays\n",
        "      2-tuple of the form (valSet_points, valSet_labels), where valSet_points is\n",
        "      a np.array of shape (x,2) and valSet_labels is a np.array of shape (x,1).\n",
        "    accuracy: int, optional\n",
        "      The dataset is split up into accuracy*accuracy many fields.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot the misclassifications per square or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plot will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum misclass\n",
        "      proportion is used as the upper limit.\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of shape (accuracy,accuracy) containing the proportions of\n",
        "    misclassifications per square as floats between 0 and 1.\n",
        "  \"\"\"\n",
        "  # Predict the validation points\n",
        "  valSet_points = validationSet[0]\n",
        "  valSet_labels = validationSet[1]\n",
        "\n",
        "  # Prepare arrays\n",
        "  valPointsPerSquare = np.zeros((accuracy,accuracy))\n",
        "  misclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "\n",
        "  # Predict points\n",
        "  if useThresholdPredict:\n",
        "    prediction = thresholdPredict(valSet_points, model, MIN_GREEN_CERT)\n",
        "  else:\n",
        "    prediction = model.predict(valSet_points)\n",
        "\n",
        "  # Identify incorrectly classified points\n",
        "  incorrect_indices = np.where((valSet_labels != np.argmax(prediction, axis=1)))\n",
        "\n",
        "  # Multiply all entries with accuracy to calculate which square each\n",
        "  # validation point falls into\n",
        "  valSet_points = valSet_points*accuracy\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, len(valSet_points))\n",
        "\n",
        "  for i in range(len(valSet_points)):\n",
        "    x_i1 = math.floor(valSet_points[i,0])\n",
        "    x_i2 = math.floor(valSet_points[i,1])\n",
        "\n",
        "    # If x_i1 or x_i2 coordinate is 1.0, reduce by 1 to prevent index out of\n",
        "    # bounds\n",
        "    if x_i1 == accuracy:\n",
        "      x_i1 = accuracy-1\n",
        "    if x_i2 == accuracy:\n",
        "      x_i2 = accuracy-1\n",
        "\n",
        "    valPointsPerSquare[x_i2,x_i1] += 1\n",
        "\n",
        "    if i in incorrect_indices[0]:\n",
        "      misclassPerSquare[x_i2,x_i1] += 1\n",
        "    \n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, len(valSet_points))\n",
        "\n",
        "  # Setting all 0 entries in valPointsPerSquare to 1 to prevent div by zero\n",
        "  valPointsPerSquare[valPointsPerSquare == 0] = 1\n",
        "\n",
        "  # Calculating proportion of validation points misclassified\n",
        "  misclassPerSquare = misclassPerSquare/valPointsPerSquare\n",
        "\n",
        "  if verbose > 0:\n",
        "    # Plotting \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Prop. of val points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(misclassPerSquare)\n",
        "\n",
        "    plt.imshow(misclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  return misclassPerSquare\n",
        "\n",
        "\n",
        "def avgMisclassPerSquare(model, initialWeights, n, valSet_size, batch_size,\n",
        "                         epochs, accuracy=10, useThresholdPredict=False,\n",
        "                         drawGrid=True, verbose=1, savePlot=False,\n",
        "                         saveToExcel=False, path='', colorbarLim=-1):\n",
        "  \"\"\"CURRENTLY WITHOUT BALANCING DATASET BEFORE TRAINING.\n",
        "    Calculates average proportion of validaion points misclassified per square\n",
        "    in an accuracy*accuracy grid over n training rounds and validation sets.\n",
        "    \n",
        "\n",
        "  Args:\n",
        "    model: keras.model\n",
        "      The model to perform the predictions.\n",
        "    initialWeights: array-like\n",
        "      Initial weights of model.\n",
        "    n: int\n",
        "      The number of training and predition rounds to average over.\n",
        "    valSet_size: int\n",
        "      Size of the randomly chosen validation sets.\n",
        "    batch_size: int\n",
        "      Batch size used for training the model.\n",
        "    epochs: int\n",
        "      Number of epochs used for training the model.\n",
        "    accuracy: int, optional\n",
        "      The dataset is split up into accuracy*accuracy many fields.\n",
        "    useThresholdPredict: boolean, optional\n",
        "      Whether to use thresholdPredict (True) or regular model.predict (False).\n",
        "    drawGrid: boolean, optional\n",
        "      Whether to draw a grid on the plot or not.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to plot results or not.\n",
        "    savePlot: boolean, optional\n",
        "      Whether to save the plot or not.\n",
        "    saveToExcel: boolean, optional\n",
        "      Whether to save the results as an Excel document or not.\n",
        "    path: str, optional\n",
        "      Path to which the plot will be saved. e.g. '/content/drive/MyDrive/'\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum avg\n",
        "      misclass proportion is used as the upper limit.\n",
        "\n",
        "  Returns:\n",
        "    2-D np.array of shape (accuracy,accuracy) containing the average proportions\n",
        "    of misclassifications per square as floats between 0 and 1.\n",
        "  \"\"\"\n",
        "  # Start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Preparing data collection lists\n",
        "  misclass_collected = []\n",
        "  avgMisclassPerSquare = np.zeros((accuracy,accuracy))\n",
        "  validationSets = {}\n",
        "\n",
        "  if verbose > 0:\n",
        "    printProgressBar(0, n)\n",
        "\n",
        "  OG_dataSet = getDataSet()\n",
        "\n",
        "  for i in range(n):\n",
        "    # Preparing data\n",
        "    dataSet = OG_dataSet.copy()\n",
        "    dataSet = dataSet[['x_i1', 'x_i2', 'l_i']]\n",
        "\n",
        "    # Choose random validation set\n",
        "    random.seed(time.time())\n",
        "    val_indices = random.sample(range(len(dataSet)), valSet_size)\n",
        "\n",
        "    valSet_points, valSet_labels = separateValidationSet(dataSet, val_indices)\n",
        "\n",
        "    training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "    training_points = np.array(dataSet[['x_i1','x_i2']])\n",
        "\n",
        "    # Classifying validation set\n",
        "    model.set_weights(initialWeights)\n",
        "\n",
        "    history = model.fit(x=training_points, y=training_labels,\n",
        "                        batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "    \n",
        "    # Calculating misclassification per square\n",
        "    result = misclassPerSquare(model, (valSet_points,valSet_labels), accuracy,\n",
        "                               useThresholdPredict)\n",
        "    \n",
        "    misclass_collected.append(result)\n",
        "\n",
        "    # Saving validation set for Excel\n",
        "    if saveToExcel == True:\n",
        "      valSet_points = pd.DataFrame(valSet_points)\n",
        "      valSet_labels = pd.DataFrame(valSet_labels)\n",
        "\n",
        "      validationSets[f'x_i1:{i}'] = valSet_points[0]\n",
        "      validationSets[f'x_i2:{i}'] = valSet_points[1]\n",
        "      validationSets[f'l_i:{i}'] = valSet_labels[0]\n",
        "\n",
        "    if verbose > 0:\n",
        "      printProgressBar(i+1, n)\n",
        "\n",
        "\n",
        "  # Averaging\n",
        "  avgMisclassPerSquare = np.average(misclass_collected, axis=0)\n",
        "\n",
        "  today = date.today()\n",
        "\n",
        "  # Plotting\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Avg prop. of val points misclassified in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(avgMisclassPerSquare)\n",
        "\n",
        "    plt.imshow(avgMisclassPerSquare, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Avg_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Avg_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "\n",
        "  # Saving to Excel \n",
        "  if saveToExcel == True:\n",
        "    index = [i/accuracy for i in range(accuracy)]\n",
        "    index.reverse()  # Reverse index for correct orientation in Excel\n",
        "    columns = [i/accuracy for i in range(accuracy)]  \n",
        "    # Flip for correct orientation in Excel\n",
        "    avgMisclassPerSquareFlipped = np.flipud(avgMisclassPerSquare)\n",
        "\n",
        "    # Initialize writer\n",
        "    writer = pd.ExcelWriter(f'{path}Avg_Misclass_Per_Square_{CURRENT_SET}_' + \n",
        "                            f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.xlsx')\n",
        "\n",
        "    # Average \n",
        "    average = pd.DataFrame(avgMisclassPerSquareFlipped, columns=columns,\n",
        "                           index=index)\n",
        "\n",
        "    average.to_excel(writer, sheet_name='Average')\n",
        "\n",
        "    # Parameters\n",
        "    data = {'Values':[f'{model.name}', f'{CURRENT_SET}', f'{n}', f'{accuracy}',\n",
        "                      f'{valSet_size}', f'{batch_size}', f'{epochs}',\n",
        "                      f'{useThresholdPredict}']}\n",
        "\n",
        "    index = ['model','dataset','n','accuracy','valSet_size','batch_size',\n",
        "             'epochs','useThresholdPredict']\n",
        "\n",
        "    parameters = pd.DataFrame(data, index=index)\n",
        "    parameters.to_excel(writer, sheet_name='Parameters')\n",
        "\n",
        "    # Validation sets\n",
        "    validationSets = pd.DataFrame(validationSets)\n",
        "    validationSets.to_excel(writer, sheet_name='Validation Sets')\n",
        "\n",
        "    writer.save()\n",
        "\n",
        "  # Print time taken for calculation\n",
        "  if verbose > 0:  \n",
        "    end_time = time.time()\n",
        "    total_time = (end_time-start_time)/60\n",
        "    print(f'Time taken: {round(total_time, 2)} minutes.')\n",
        "\n",
        "  return avgMisclassPerSquare\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Oy2HWZscTuIm"
      },
      "source": [
        "#@title Weighted Misclassification Probability\n",
        "def weightedMisclassProbability(distributionMap, misclassPerSquare, model,\n",
        "                                verbose=1, colorbarLim=-1, drawGrid=True,\n",
        "                                savePlot=False, path=''):\n",
        "  \"\"\"Calculates the total probability that the next point will be misclassified\n",
        "    by weighting the misclassification probaility of each square with the\n",
        "    probability distribution of the datset.\n",
        "  \n",
        "  Args:\n",
        "    distributionMap: 2-D np.array of shape (x,x)\n",
        "      The distribution map of the dataset.\n",
        "    misclassPerSquare: 2-D np.array of shape (x,x)\n",
        "      Misclassification probabilities per square as floats between 0 and 1.\n",
        "    model: keras.model\n",
        "      The model used for calculating misclassPerSquare.\n",
        "    verbose: 0 or 1, optional\n",
        "      Whether to show the plot or not.\n",
        "    colorbarLim: float between 0 and 1, optional\n",
        "      Upper limit for the colorbar. Defaults to -1 where the maximum\n",
        "      misclassification probability is used as the upper limit.\n",
        "    drawGrid: boolean, optional\n",
        "        Whether to draw a grid on the plot or not.\n",
        "    savePlot: boolean, optional\n",
        "        Whether to save the plot or not.\n",
        "    path: str, optional\n",
        "      Path to which the plots will be saved. e.g. '/content/drive/MyDrive/'\n",
        "\n",
        "  Returns:\n",
        "    Float between 0 and 1. The total probability that the next point will be\n",
        "    misclassified.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: if distributionMap.shape != misclassPerSquare.shape.\n",
        "  \"\"\"\n",
        "  if distributionMap.shape != misclassPerSquare.shape:\n",
        "    raise TypeError(f'distributionMap.shape and misclassPerSquare.shape must ' + \n",
        "                    f'be equal. distributionMap.shape is ' +\n",
        "                    f'{distributionMap.shape} and misclassPerSquare.shape is ' +\n",
        "                    f'{misclassPerSquare.shape}.')\n",
        "\n",
        "  weightedMap = distributionMap*misclassPerSquare\n",
        "\n",
        "  result = np.sum(weightedMap)\n",
        "\n",
        "  # Plotting\n",
        "  if verbose > 0:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_title(f'Weighted misclass probability in {CURRENT_SET}')\n",
        "\n",
        "    if colorbarLim == -1:\n",
        "      colorbarLim = np.max(weightedMap)\n",
        "\n",
        "    plt.imshow(weightedMap, origin='lower', cmap='Spectral', vmin=0,\n",
        "              vmax=colorbarLim, extent=[0, 1, 0, 1])\n",
        "    \n",
        "    plt.colorbar()\n",
        "    ax.set_xlabel('x_i1')\n",
        "    ax.set_ylabel('x_i2')\n",
        "    ax.set_xlim((0,1))\n",
        "    ax.set_ylim((0,1))\n",
        "    ax.set_xticks([i/10 for i in range(11)])\n",
        "    ax.set_yticks([i/10 for i in range(11)])\n",
        "    if drawGrid == True:\n",
        "      ax.grid(alpha=0.3, color='black')\n",
        "    config_info = (f'Total: {round(result*100,2)}%')\n",
        "    ax.text(1.05, 1.03, config_info, weight='bold')\n",
        "    plt.show()\n",
        "\n",
        "  # Save plot\n",
        "  today = date.today()\n",
        "  if savePlot == True:\n",
        "    fig.savefig(f'{path}Weighted_Misclass_Prob_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.pdf')\n",
        "    fig.savefig(f'{path}Weighted_Misclass_Prob_{CURRENT_SET}_' + \n",
        "                f'{model.name}_{today.strftime(\"%d-%m-%Y\")}.png', dpi=300)\n",
        "  \n",
        "  print(f'Total misclassification probability: {round(result*100,2)}%')\n",
        "\n",
        "  return result"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3YU__k-8CfW"
      },
      "source": [
        "# Magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR2BPwsfkBj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03287655-90d2-44fb-dd17-4898381cdbdc"
      },
      "source": [
        "# Data Preparation\n",
        "dataSet = getDataSet()\n",
        "dataSet.pop('Unnamed: 0') #Removing unnessary column\n",
        "\n",
        "valSet_points, valSet_labels = separateValidationSet(dataSet=dataSet, validationIndices=VAL_INDICES)\n",
        "\n",
        "dataSet = balanceDataset(dataSet, threshold=THRESHOLD_DATA)\n",
        "\n",
        "#Creating tensors\n",
        "training_labels = np.array(dataSet['l_i']).astype('float')\n",
        "training_points = np.array(dataSet[['x_i1','x_i2']])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artificially exended by 21291 points\n",
            "Relation is now: 0.6 green : 0.4 red \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuey_TJrkBj8"
      },
      "source": [
        "# Configure and compile model\n",
        "initalizer = keras.initializers.GlorotNormal()\n",
        "\n",
        "model_0 = keras.Sequential([\n",
        "          keras.layers.Flatten(input_shape=(2,)),      #input layer: 2 neurons\n",
        "          keras.layers.Dense(100,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(70,activation='relu', kernel_initializer=initalizer), \n",
        "          keras.layers.Dense(50,activation='relu', kernel_initializer=initalizer),       \n",
        "          keras.layers.Dense(10,activation='relu', kernel_initializer=initalizer),\n",
        "          keras.layers.Dense(2,activation='softmax', kernel_initializer=initalizer)   #output layer: 2 neurons              \n",
        "          ], name=\"model_0\")\n",
        "\n",
        "model_0.compile(optimizer='adam', loss=construct_custom_penalty_loss(PENALTY),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Save initial weights\n",
        "initialWeights = model_0.get_weights()\n",
        "\n",
        "# Fit model\n",
        "history = model_0.fit(training_points, training_labels, batch_size=1800, epochs=100,\n",
        "                    shuffle=True, validation_data=(valSet_points, valSet_labels))\n",
        "clear_output()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljZI45mP1KLr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "3b23a89a-b059-4f82-ef6b-d6190fe2d92a"
      },
      "source": [
        "misclass = avgMisclassPerSquare(model_0, initialWeights, 20, 8000, 1800, 2000,\n",
        "                                accuracy=50)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken: 37.39 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF9kppXod0ox",
        "outputId": "704faede-2bc9-4a0d-c633-a3d183ac43a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "dist = makeDistributionMap(accuracy=50)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zodycZcZiZQX",
        "outputId": "19bfbc4b-6475-4f50-fd6f-db7e577aa962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "x = weightedMisclassProbability(dist, misclass, model_0, savePlot=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total misclassification probability: 1.48%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}